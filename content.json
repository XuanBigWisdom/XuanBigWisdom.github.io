{"meta":{"title":"萱仔的学习小屋","subtitle":"学习学习学习哦耶","description":"","author":"xuanzaismart","url":"http://example.com","root":"/"},"pages":[{"title":"书单","date":"2024-09-04T01:51:59.344Z","updated":"2024-09-02T02:10:25.437Z","comments":false,"path":"books/index.html","permalink":"http://example.com/books/index.html","excerpt":"","text":""},{"title":"about","date":"2024-09-02T09:53:06.000Z","updated":"2024-09-02T09:55:24.218Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":""},{"title":"repository","date":"2024-09-03T12:22:55.000Z","updated":"2024-09-03T12:22:55.974Z","comments":true,"path":"repository/index.html","permalink":"http://example.com/repository/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2024-09-03T12:14:27.261Z","updated":"2024-09-02T02:10:25.438Z","comments":true,"path":"links/index.html","permalink":"http://example.com/links/index.html","excerpt":"","text":""},{"title":"","date":"2024-09-04T01:59:26.048Z","updated":"2024-09-04T01:59:16.093Z","comments":true,"path":"books/books1.html","permalink":"http://example.com/books/books1.html","excerpt":"","text":""},{"title":"categories","date":"2024-09-02T09:25:14.000Z","updated":"2024-09-02T09:55:07.768Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2024-09-02T09:24:06.000Z","updated":"2024-09-02T09:57:56.089Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"旧代码学习上传记录-天池-零基础入门NLP_-_新闻文本分类","slug":"旧代码学习上传记录-天池-零基础入门NLP_-_新闻文本分类","date":"2024-09-05T06:00:00.000Z","updated":"2024-09-08T15:04:13.789Z","comments":true,"path":"2024/09/05/旧代码学习上传记录-天池-零基础入门NLP_-_新闻文本分类/","permalink":"http://example.com/2024/09/05/%E6%97%A7%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E4%B8%8A%E4%BC%A0%E8%AE%B0%E5%BD%95-%E5%A4%A9%E6%B1%A0-%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP_-_%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/","excerpt":"","text":"搜寻到以前做过的旧代码操作记录，零基础入门nlp，用了机器学习的算法进行预测，本次给出的数据集格式是csv格式，读取文件的代码是 12df = pd.read_csv(file_path, sep=&#x27;\\t&#x27;) 包含了label和text，其中官网给出的数据为了避免被人为标记已经做了处理，其中数据提取出来如下所示 然后本次算法是选取了随机森林算法等多种算法进行训练预测，由于数据集涉及到20w条，本电脑内存不足，先把训练集分割成10个，取其中一个进行训练， 以下是拆分的代码 123456789101112131415import pandas as pddef split_csv_file(filename, num_files): df = pd.read_csv(filename) rows_per_file = len(df) if len(df) % num_files != 0: rows_per_file += 1 for i in range(num_files): start_row = i * rows_per_file end_row = min(start_row + rows_per_file, len(df)) df_subset = df.iloc[start_row:end_row] output_filename = f&#x27;split_&#123;i + 1&#125;.csv&#x27; df_subset.to_csv(output_filename, index=False) print(f&#x27;Saved &#123;output_filename&#125;&#x27;)split_csv_file(&#x27;./train_set.csv&#x27;, 10) 随机森林算法 12345678910111213141516171819202122import pandas as pdimport torchfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.metrics import accuracy_scorefrom torch.utils.data import Dataset, DataLoaderfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import LabelEncoderimport pandas as pddf = pd.read_csv(&#x27;./split_1.csv&#x27;, sep=&#x27;\\t&#x27;)print(df.head(3))df = pd.DataFrame(df)X_train, X_test, y_train, y_test = train_test_split(df[&#x27;text&#x27;], df[&#x27;label&#x27;], test_size=0.3, random_state=42)vectorizer = CountVectorizer()X_train = vectorizer.fit_transform(X_train)X_test = vectorizer.transform(X_test)classifier = RandomForestClassifier()classifier.fit(X_train, y_train)y_pred = classifier.predict(X_test)accuracy = accuracy_score(y_test, y_pred)print(f&#x27;Accuracy: &#123;accuracy:.2f&#125;&#x27;)","categories":[{"name":"新闻文本分类项目","slug":"新闻文本分类项目","permalink":"http://example.com/categories/%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"技术, nlp, python, bert, transform","slug":"技术-nlp-python-bert-transform","permalink":"http://example.com/tags/%E6%8A%80%E6%9C%AF-nlp-python-bert-transform/"}]},{"title":"萱仔求职系列——1.1_机器学习基础知识复习","slug":"萱仔求职系列——1.1_机器学习基础知识复习","date":"2024-09-05T06:00:00.000Z","updated":"2024-09-09T03:01:25.280Z","comments":true,"path":"2024/09/05/萱仔求职系列——1.1_机器学习基础知识复习/","permalink":"http://example.com/2024/09/05/%E8%90%B1%E4%BB%94%E6%B1%82%E8%81%8C%E7%B3%BB%E5%88%97%E2%80%94%E2%80%941.1_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%A4%8D%E4%B9%A0/","excerpt":"","text":"由于我最近拿到offer还是想再找找更好的机会，目前有很多的面试，面试的时候很多面试官会问一些机器学习的基础知识，由于我上一段实习的时候主要是机器学习和部分深度学习的内容，为了避免在面试的时候想不起来自己学习的内容，我还是决定边复习边学习，新开一个自我复习系列，巩固已经学到的知识。 -------------------------------------------------------------------------------------------------------------- 机器学习（准确的预测和分类等，牺牲部分可解释性获得强大的预测能力）&lt;——&gt;数理统计（推断特征之间的关系）机器学习——&gt;有监督，无监督，强化学习 有监督：数据有标签，——&gt;回归（标签连续）、分类（标签离散） 无监督：没有标签——&gt;聚类（识别数据的组）、降维（从高维数据检测识别低维的数据结构） 强化学习：让他自己学习 机器学习的分类主要包括有监督学习、无监督学习和强化学习。每个类别都有其独特的特点和应用场景。以下是对这三种主要类型的详细介绍： 1. 有监督学习 (Supervised Learning) 训练数据集中的每个样本都有对应的标签（或目标值），模型学习输入到输出的映射关系。模型的目标是根据输入预测输出，常用于分类和回归任务。 常用算法及其原理回归算法 线性回归 (Linear Regression) ： + **原理** ：找到一个线性函数，使得预测值与实际值之间的误差最小。 + **应用** ： **房价预测** ：根据特征（如房屋面积、房间数量、地理位置等）预测房价。 **股票价格预测** ：根据历史价格和其他相关指标预测股票的未来价格。 **销售预测** ：根据历史销售数据和市场因素预测未来销售额。 + 线性回归是一种基本的回归分析方法，用于建立特征（自变量）和目标变量（因变量）之间的线性关系。它通过最小化预测值与实际值之间的误差来找到最佳的回归函数。 线性回归模型假设目标变量 y与特征变量 X之间的关系可以用一个线性函数表示： 找到回归系数 β 使得 预测值 与 实际值 之间的误差最小。误差通常用均方误差（Mean Squared Error, MSE）来度量： 最小二乘法是最常用的方法，通过最小化误差平方和来估计回归系数。 岭回归 (Ridge Regression) ： + **原理** ：在线性回归的基础上加入L2正则化项，防止过拟合。 + **应用** ：高维数据的回归分析。 **高维数据的回归分析** ：如基因数据分析、文本数据处理等，特征维度远大于样本数的场景。 **解决多重共线性问题** ：当特征之间存在高度相关性时，岭回归能提供更稳定的回归系数估计。 Lasso 回归 (Lasso Regression) ： + **原理** ：是在线性回归的基础上加入 L1 正则化项，通过引入绝对值的惩罚项来约束回归系数，使得一些回归系数收缩到零。这种特性使得 Lasso 回归不仅可以用于预测，还可以进行特征选择。 + **应用** ：特征选择和预测。 Lasso 回归 vs. 岭回归 ： Lasso 回归 ：使用 L1 正则化项，进行特征选择，将不重要的特征的系数缩减为零。 岭回归 ：使用 L2 正则化项，不会将系数缩减为零，但可以减少系数的大小，从而减少过拟合。 多项式回归 (Polynomial Regression) ： 原理 ：将特征进行多项式扩展，捕捉非线性关系。 应用 ：非线性数据的回归分析。 分类算法 逻辑回归 (Logistic Regression) ： + **原理** ：使用逻辑函数将线性组合映射到0到1之间，表示事件发生的概率。 + **应用** ：二分类问题，如垃圾邮件分类、疾病预测。 **二分类问题** ：逻辑回归广泛应用于各种二分类问题，如垃圾邮件分类、疾病预测、客户流失预测等。 **概率预测** ：除了分类，逻辑回归还可以输出事件发生的概率。 对比其他算法 1. 逻辑回归 vs. 线性回归 ： * 逻辑回归 ：用于分类问题，输出的是概率值。 * 线性回归 ：用于回归问题，输出的是连续值。 2. 逻辑回归 vs. 决策树 ： * 逻辑回归 ：基于线性模型，适用于线性可分数据，易于解释。 * 决策树 ：基于树状结构，可以捕捉复杂的非线性关系，但容易过拟合。 3. 逻辑回归 vs. 支持向量机（SVM） ： * 逻辑回归 ：输出概率，易于解释和计算效率高。 * SVM ：通过最大化间隔实现分类，适用于高维数据，但计算复杂度高，参数调优复杂。 4. 逻辑回归 vs. 朴素贝叶斯 ： * 逻辑回归 ：不假设特征之间的独立性，适用于特征之间相关性较强的数据。 * 朴素贝叶斯 ：假设特征之间相互独立，计算效率高，适用于高维数据。 支持向量机 (SVM) ： 原理 ： 找到最大化分类边界的超平面。 支持向量机（Support Vector Machine, SVM）是一种监督学习模型，用于分类和回归分析。其主要思想是找到一个能够最大化分类边界的超平面，以此将不同类别的样本分开。在二分类问题中，SVM 通过选择支持向量（位于分类边界上的样本点）来确定最优的分类超平面。对于线性可分数据，SVM 的目标是找到一个决策超平面，使得两个类别之间的间隔最大化。这个间隔被称为“最大间隔”（Maximum Margin）。对于线性不可分的数据，SVM 通过引入核函数（Kernel Function）将数据映射到高维空间，使得在高维空间中可以找到线性可分的超平面。 应用 ：文本分类、图像识别。 对比其他算法 1. SVM vs. 逻辑回归 ： * SVM ：最大化分类间隔，适用于高维数据，支持非线性分类。 * 逻辑回归 ：线性模型，适用于线性可分数据，输出分类概率，计算效率高。 2. SVM vs. 决策树 ： * SVM ：基于超平面分类，适用于高维数据，易于处理非线性问题。 * 决策树 ：基于树状结构分类，易于解释，适用于处理缺失数据和类别不均衡的数据。 3. SVM vs. KNN（K近邻） ： * SVM ：训练时间较长，但预测时间较短，适用于高维数据。 * KNN ：训练时间较短，但预测时间较长，对大规模数据和高维数据不友好。 4. SVM vs. 神经网络 ： * SVM ：模型较为简单，适用于中小规模数据，参数调优相对复杂。 * 神经网络 ：适用于大规模数据和复杂非线性问题，具有更高的灵活性，但需要更多的计算资源和更长的训练时间。 决策树 (Decision Tree) ： + **原理** ： **递归分割特征空间，构建树结构。** 决策树是一种用于分类和回归任务的树形模型，通过递归地分割特征空间来构建树结构。在每个节点上，决策树选择一个特征及其阈值，以最佳方式将数据分割为不同的子集，从而使得每个子集的纯度最大化。这个过程一直递归进行，直到达到停止条件（如节点中的样本数小于某个阈值，或达到最大深度）。 + **应用** ：分类和回归任务。 信息增益 （Information Gain）： 信息增益是通过衡量分割前后信息熵的减少量来选择分割特征。信息熵（Entropy）的计算公式为： 基尼指数 （Gini Index）： 基尼指数用于衡量数据集的不纯度，其计算公式为： 卡方统计量 （Chi-square）： 卡方统计量用于衡量特征与目标变量之间的独立性，其计算公式为： 随机森林 (Random Forest) ： 原理 ：集成多棵决策树，通过多数投票或平均结果进行预测。决策树通过递归地分割特征空间来构建树结构，每个节点代表一个特征的测试条件，每个分支代表测试结果，每个叶节点代表一个类别或回归值。决策树的构建过程包括选择最佳分割特征和分割点，以最大化节点纯度。 应用 ：分类、回归任务。 梯度提升树 (Gradient Boosting Tree, GBT) 原理 梯度提升树通过逐步构建决策树，每棵树都在前一棵树的基础上进行优化，以最小化损失函数。每次迭代中，新的树拟合当前模型的残差。 极端梯度提升 (XGBoost) 原理 ：XGBoost 是梯度提升的优化实现，具有更高的效率和灵活性，通过正则化和并行计算加速训练过程，并能处理缺失值。 应用 ： LightGBM 原理 ：LightGBM 是一种基于直方的梯度提升框架，采用基于叶节点的生长策略，提高了计算效率。它通过优化数据存储和分裂策略，实现更快的训练速度和更低的内存消耗。 应用 ： 决策树及其变种（如随机森林、梯度提升树、XGBoost、LightGBM、CatBoost）在分类和回归任务中具有广泛应用。决策树通过递归分割特征空间构建树结构，而其变种通过集成学习和优化技术提高模型的性能和泛化能力。理解这些算法的原理和公式，有助于在实际应用中选择和优化模型。 随机森林与XGBoost的区别 随机森林 (Random Forest) 1. 原理 ： * 随机森林是一种集成学习方法，由多个决策树组成。每棵决策树都是通过对训练数据的随机抽样生成的。最终的预测结果通过对所有决策树的预测结果进行投票（分类任务）或平均（回归任务）得到。 * 在随机森林中，Bootstrap 采样（有放回抽样）的过程使得每个数据点在每棵树的训练数据集中被重复采样的概率可以通过以下公式计算： 1. 随机性 ： * 数据随机性 ：训练每棵树时，使用的是数据的随机子集（有放回抽样，即 Bootstrap 采样）。即每棵树的训练数据集是从原始数据集中随机抽取的。由于是有放回采样，每个样本可能在不同的树中出现多次。 * 特征随机性 ：在构建每棵树时，对每个节点的分裂特征进行随机选择。即在每个节点分裂时，只考虑随机选择的部分特征而不是全部特征，从而增加树之间的多样性。 2. 并行化 ： * 随机森林的每棵树是独立构建的，因此可以并行训练多棵树。这使得随机森林在训练时能够利用多核处理器的并行计算能力，提高计算效率。 3. 优缺点 ： * 优点 ：减少了过拟合，具有较好的泛化能力，适用于大规模数据集。 * 缺点 ：模型较为复杂，解释性较差，预测时需要对所有树的结果进行投票或平均，计算成本较高。 XGBoost (Extreme Gradient Boosting) 1. 原理 ： * XGBoost 是一种梯度提升树方法，通过逐步构建决策树，每棵树都在前一棵树的基础上进行优化，以最小化损失函数。它采用了贪心算法来优化模型，加入了正则化项来减少过拟合。 2. 并行化 ： * 特征并行 ：XGBoost 对特征进行并行处理。树的分裂是在每个节点的所有特征上并行计算的。使用直方图算法（Histogram-Based Method），将连续特征离散化为直方图，然后并行地计算每个特征的分裂增益。 * 样本并行 ：在训练过程中，XGBoost 使用了块分裂方法，将数据分成多个块，并在不同的块上并行训练。每个块包含了数据的不同部分，这些块可以同时进行计算。 * 树的并行 ：XGBoost 在构建树的过程中，可以并行构建不同的树（在某些实现中）。 3. 优缺点 ： * 优点 ：高效、准确，支持并行计算和分布式计算，对缺失值处理较好，适用于大规模数据集。 * 缺点 ：参数较多，需要调优，模型复杂性较高，对数据预处理要求较高。 XGBoost 的并行实现 XGBoost 的并行实现主要体现在以下几个方面： 1. 特征并行 ： * 在每次分裂时，XGBoost 会遍历所有特征，计算每个特征的分裂增益。通过将特征分配到不同的线程中，XGBoost 能够并行计算各个特征的分裂增益，从而加速分裂过程。 2. 样本并行 ： * 数据集被划分为多个块（或子样本），每个块在不同的线程中进行计算。这种并行化使得模型训练过程能够利用多核处理器的计算能力。 3. 直方图算法 ： * XGBoost 使用直方图算法来加速分裂点的搜索。特征值被离散化为直方图桶，然后并行计算每个桶的增益。这样可以大大减少计算量，提高训练速度。 4. 块分裂方法 ： * XGBoost 通过将数据划分为块，并在块之间并行计算，进一步提高了训练速度。每个块包含数据的一部分，这些块可以同时处理，并在最终合并结果。 随机森林的随机性 1. 数据随机性 ： * 随机森林通过 Bootstrap 采样（有放回抽样）来生成每棵决策树的训练数据集。即每棵决策树训练时使用的数据集是从原始数据集中随机抽取的样本，有可能重复抽取相同的数据点。 2. 特征随机性 ： * 在每个节点分裂时，只随机选择部分特征进行考虑，而不是使用所有特征。这样可以增加树的多样性，减少过拟合。 k近邻 (k-NN) ： 原理 ：通过计算与训练样本的距离，选择最近的k个邻居进行预测。 应用 ：图像识别、推荐系统。 朴素贝叶斯 (Naive Bayes) ： + **原理** ：基于贝叶斯定理和特征条件独立假设进行分类。朴素贝叶斯分类器是一种基于贝叶斯定理的简单且高效的分类算法。它广泛应用于文本分类、垃圾邮件过滤、情感分析等领域。尽管它的假设条件比较强（特征独立性假设），但在许多实际问题中表现良好。 + **应用** ：文本分类、情感分析。 + ![](https://i-blog.csdnimg.cn/direct/89c098bdaaf5462ba1273049bd177b6e.png) + #### 类型 - **高斯朴素贝叶斯** ： * 适用于特征值符合高斯分布（连续特征）的情况。 * 特征的条件概率密度函数假设为高斯分布。 - **多项式朴素贝叶斯** ： * 适用于特征值为计数或频率的情况（例如文本数据的词频）。 * 特征的条件概率假设为多项分布。 - **伯努利朴素贝叶斯** ： * 适用于特征为二值特征的情况（例如是否出现某个词）。 * 特征的条件概率假设为伯努利分布。 神经网络 (Neural Networks) ： 原理 ：通过多个层的神经元连接进行特征提取和映射。 应用 ：图像识别、语音识别、自然语言处理。 2. 无监督学习 (Unsupervised Learning)特点 数据无标签 ：训练数据集中的样本没有标签，模型需自行发现数据的结构和模式。 目标探索 ：模型的目标是探索数据的内在结构，常用于聚类和降维任务。 常用算法及其原理聚类算法 k-means 聚类 (k-means Clustering) ： 原理 ：通过最小化样本到最近质心的距离进行聚类。K-means 聚类是一种基于划分的方法，用于将数据集分成 kkk 个不同的簇或群组。每个簇由一个质心（centroid）表示，质心是簇中所有点的均值。 应用 ：客户分群、图像分割。 K-means 聚类 (K-means Clustering) 原理 * 任务 ：无监督学习的聚类算法。 * 目标 ：将数据集划分成 k 个簇，使得每个簇内的样本尽可能相似，而簇与簇之间的样本尽可能不同。 * 方法 ： 1. 初始化 ：随机选择 k 个初始质心。 2. 分配阶段 ：将每个数据点分配到最近的质心所在的簇。 3. 更新阶段 ：计算每个簇的质心，并更新质心的位置。 4. 迭代 ：重复分配和更新阶段，直到质心不再变化（或变化很小）。 主要特点 * 无监督 ：不需要标签信息，主要用于数据的分组和模式发现。 * 输出 ：簇的划分和每个簇的质心。 * 依赖 ：簇的数量 kkk 需要预先指定。 * 应用 ：客户分群、图像分割、数据预处理等。 K-最近邻 (KNN) 原理 * 任务 ：有监督学习的分类或回归算法。 * 目标 ：根据已标记的训练数据对新样本进行分类或回归。 * 方法 ： 1. 选择 k ：选择一个正整数 k。 2. 计算距离 ：对于每个待分类样本，计算它与训练集中所有样本的距离（如欧几里得距离）。 3. 选择最近邻 ：选择距离最近的 k 个样本。 4. 投票&#x2F;平均 ： + 分类 ：对这 k 个邻居的标签进行投票，选择最多的类别作为新样本的预测类别。 + 回归 ：对这 k 个邻居的目标值取平均，作为新样本的预测值。 主要特点 * 有监督 ：需要已标记的训练数据，用于分类或回归任务。 * 输出 ：分类结果或回归值。 * 依赖 ：选择的 k 值会影响分类结果或回归预测。 * 应用 ：文本分类、图像分类、推荐系统等。 区别总结 1. 任务类型 ： * K-means ：无监督学习（聚类）。 * KNN ：有监督学习（分类或回归）。 2. 输入 ： * K-means ：只需要数据，不需要标签。 * KNN ：需要带有标签的训练数据。 3. 输出 ： * K-means ：每个样本的簇分配和质心。 * KNN ：新样本的预测标签或值。 4. 算法目标 ： * K-means ：寻找数据的自然簇，最小化簇内的样本到质心的距离。 * KNN ：基于已知样本的标签或值，对新样本进行预测。 5. 适用场景 ： * K-means ：用于发现数据的自然分组或模式，适用于数据探索和预处理。 * KNN ：用于对新样本进行预测，适用于分类和回归任务。 层次聚类 (Hierarchical Clustering) ： 原理 ：通过合并或分裂簇生成层次结构。 应用 ：基因表达分析、文档聚类。 DBSCAN (Density-Based Spatial Clustering of Applications with Noise) ： 原理 ：基于密度连接样本，形成簇。 应用 ：地理数据分析、异常检测。 降维算法 主成分分析 (PCA, Principal Component Analysis) ： 原理 ：通过线性变换将数据投影到最大方差方向。主成分分析（PCA）是一种降维技术，其核心思想是通过线性变换将数据投影到一个新的坐标系中，使得数据在新的坐标系下的方差最大。具体步骤如下： 1.数据中心化 ： 将数据的每一维减去其均值，使数据的均值为零。这是为了使数据的协方差矩阵不受偏移的影响。 2.计算协方差矩阵 ： 协方差矩阵描述了数据各特征之间的相关性。对于 n维数据，协方差矩阵是n×n 的矩阵，矩阵中的每个元素表示两个特征之间的协方差。 3.计算特征值和特征向量 ： 计算协方差矩阵的特征值和特征向量。特征值表示数据沿着特征向量方向的方差大小。 4.选择主成分 ： 将特征向量按特征值的大小排序，选择前 k个特征向量（主成分），这些主成分对应最大方差的方向。选择的数量 k是降维的维度。 5.投影数据 ： 将数据投影到选定的主成分上，得到降维后的数据。这个过程通过特征向量矩阵的乘积完成。 应用 ：数据预处理、可视化。 t-SNE (t-Distributed Stochastic Neighbor Embedding) ： 原理 ：通过最小化高维空间和低维空间的分布差异进行降维。 应用 ：高维数据的可视化。 线性判别分析 (LDA, Linear Discriminant Analysis) ： 原理 ：通过最大化类间方差和类内方差比值进行降维。 应用 ：分类任务的降维。 3. 强化学习 (Reinforcement Learning)特点 让模型自行学习 ：通过与环境的交互，自行探索最优策略。 目标长期收益 ：模型的目标是最大化累计奖励，常用于动态和连续决策问题。 常用算法及其原理 马尔可夫决策过程 (MDP, Markov Decision Process) ： 原理 ：基于状态、动作和奖励的动态规划。 应用 ：描述强化学习环境。 深度Q网络 (DQN, Deep Q-Network) ： 原理 ：使用神经网络近似Q值函数。 应用 ：复杂环境下的强化学习，如游戏AI。 4. 特点对比 有监督学习 vs. 无监督学习 ： 有监督学习有明确的目标，模型根据已知标签进行训练，适用于分类和回归问题。 无监督学习没有标签，模型需自行发现数据的结构，适用于聚类和降维问题。 有监督学习 vs. 强化学习 ： 有监督学习使用静态数据集进行训练，目标是根据输入预测输出。 强化学习通过与环境的动态交互进行学习，目标是最大化累计奖励。 无监督学习 vs. 强化学习 ： 无监督学习探索数据的内在结构，没有明确的目标标签。 强化学习通过试错过程与环境交互，自行探索最优策略。 特化与泛化：泛化能力——&gt;验证一个模型是否强，得到的机器学习模型对新东西的预测能力 过拟合、欠拟合解决方案：1、拓展数据集（数据增强：①有监督②无监督：例如生成对抗网络，随机生成一些新的数据） 2、正则化：在损失函数上加正则化项，①L1②L2：岭回归 3、减少特征的选择①dropout ②集成学习：例如决策树变成随机森林 ③早停：提早结束对神经网络迭代 过拟合与欠拟合过拟合 （Overfitting）和 欠拟合 （Underfitting）是机器学习中常见的两个问题。 过拟合 ： 定义 ：模型在训练数据上表现很好，但在未见过的数据上表现较差。模型过于复杂，捕捉了训练数据中的噪声或异常值，导致泛化能力差。 表现 ：训练误差很低，但验证或测试误差较高。 欠拟合 ： 定义 ：模型过于简单，无法捕捉数据中的潜在模式，无论是在训练数据还是未见过的数据上表现都不好。 表现 ：训练误差和验证或测试误差都较高。 过拟合与欠拟合——解决方案1. 拓展数据集 数据增强 ： 有监督数据增强 ：通过对训练数据进行变换（例如旋转、缩放、裁剪等），增加数据多样性。例如，图像分类中的数据增强技术。 无监督数据增强 ： 生成对抗网络（GANs） ：通过生成对抗网络生成新的样本数据，增加数据集的多样性。 随机生成 ：例如随机生成数据点或特征组合，来扩展数据集。 2. 正则化 L1 正则化 （Lasso）： 定义 ：在损失函数中加入特征系数绝对值的和作为正则化项，具有特征选择的效果，可以使某些特征的权重变为零。 公式 ： 应用 ：适用于需要特征选择的场景。 L2 正则化 （Ridge）： 定义 ：在损失函数中加入特征系数的平方和作为正则化项，可以防止模型权重过大，使得模型更加平滑。 公式 ： 应用 ：适用于模型权重的控制，防止过拟合。 3. 减少特征的选择 Dropout ： 定义 ：在训练过程中随机丢弃神经网络中的一部分节点，防止节点之间的复杂共适应关系，从而减轻过拟合。 应用 ：常用于深度神经网络的训练中。 集成学习 ： 随机森林 ：通过多个决策树的组合来提高模型的稳定性和泛化能力，减少单棵树的过拟合问题。 Bagging 和 Boosting ：其他集成学习方法，通过组合多个模型来提高性能和稳定性。 早停 ： 定义 ：在训练过程中监控验证集的表现，如果验证集误差开始增加，则停止训练。这有助于防止过拟合。 应用 ：适用于深度学习模型的训练过程中，帮助选择最佳的训练轮次。 评价指标：分类 错误率e=1&#x2F;m*100%，acc=1-e，查准率P：预测的有多少是真的，召回率R：所有为1的样本中，预测对了多少 P-R曲线、ROC曲线（曲线下面积——AUC）、ks曲线、 回归 r^2，均方误差mse","categories":[{"name":"萱仔求职系列","slug":"萱仔求职系列","permalink":"http://example.com/categories/%E8%90%B1%E4%BB%94%E6%B1%82%E8%81%8C%E7%B3%BB%E5%88%97/"}],"tags":[{"name":"技术, 教程","slug":"技术-教程","permalink":"http://example.com/tags/%E6%8A%80%E6%9C%AF-%E6%95%99%E7%A8%8B/"}]},{"title":"萱仔求职系列——1.2_机器学习基础知识复习+部分代码实战","slug":"萱仔求职系列——1.2_机器学习基础知识复习+部分代码实战","date":"2024-09-05T06:00:00.000Z","updated":"2024-09-09T03:01:16.632Z","comments":true,"path":"2024/09/05/萱仔求职系列——1.2_机器学习基础知识复习+部分代码实战/","permalink":"http://example.com/2024/09/05/%E8%90%B1%E4%BB%94%E6%B1%82%E8%81%8C%E7%B3%BB%E5%88%97%E2%80%94%E2%80%941.2_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%A4%8D%E4%B9%A0+%E9%83%A8%E5%88%86%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98/","excerpt":"","text":"实战篇： 1、对用户需求分析 2、数据挖掘 3、数据预处理 4、特征工程——&gt; ①向量化（把数据转化成向量才能机器学习） ②特征选择（WOE、IV） ③分类特征：映射编码，二进制编码、独热编码（数据的维度变得很大） ④文本特征：TF-TDF ⑤图像特征：像素点 ⑥衍生特征：进入的数据已经不是最初的特征 5、选择模型——&gt;参数调优 ① 线性回归：用最小二乘法和梯度下降法拟合直线，用损失函数：残差平方和 多重共线性：使用的这些特征是否存在很强的 相关矩阵：相关系数，vif方差膨胀检验 12345678910111213141516171819202122232425262728293031323334import numpy as npimport matplotlib.pyplot as plt# 生成样本数据np.random.seed(0)X = 2 * np.random.rand(100, 1)y = 4 + 3 * X + np.random.randn(100, 1)# 线性回归模型class LinearRegression: def fit(self, X, y): X_b = np.c_[np.ones((len(X), 1)), X] self.theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y) def predict(self, X): X_b = np.c_[np.ones((len(X), 1)), X] return X_b.dot(self.theta)# 创建模型并训练model = LinearRegression()model.fit(X, y)# 预测X_new = np.array([[0], [2]])y_predict = model.predict(X_new)# 可视化plt.plot(X_new, y_predict, &quot;r-&quot;)plt.plot(X, y, &quot;b.&quot;)plt.xlabel(&quot;$x_1$&quot;)plt.ylabel(&quot;$y$&quot;)plt.show() ② 逻辑斯蒂回归：线性回归+sigmiod ，预测的是概率，而不是目标函数的值，用最小二乘法和梯度下降法拟合参数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import numpy as npfrom sklearn.datasets import load_irisimport matplotlib.pyplot as plt# 生成样本数据iris = load_iris()X = iris[&quot;data&quot;][:, 3:] # 花瓣宽度y = (iris[&quot;target&quot;] == 2).astype(np.int) # 逻辑回归模型class LogisticRegression: def __init__(self, learning_rate=0.1, n_iter=1000): self.learning_rate = learning_rate self.n_iter = n_iter def sigmoid(self, z): return 1 / (1 + np.exp(-z)) def fit(self, X, y): m, n = X.shape self.theta = np.random.randn(n + 1) X_b = np.c_[np.ones((m, 1)), X] for _ in range(self.n_iter): gradients = X_b.T.dot(self.sigmoid(X_b.dot(self.theta)) - y) / m self.theta -= self.learning_rate * gradients def predict_proba(self, X): X_b = np.c_[np.ones((len(X), 1)), X] return self.sigmoid(X_b.dot(self.theta)) def predict(self, X): return (self.predict_proba(X) &gt;= 0.5).astype(np.int)# 创建模型并训练model = LogisticRegression()model.fit(X, y)# 预测X_new = np.linspace(0, 3, 1000).reshape(-1, 1)y_proba = model.predict_proba(X_new)# 可视化plt.plot(X_new, y_proba, &quot;g-&quot;, label=&quot;Virginica probability&quot;)plt.plot(X[y == 0], y[y == 0], &quot;bs&quot;, label=&quot;Not Virginica&quot;)plt.plot(X[y == 1], y[y == 1], &quot;g^&quot;, label=&quot;Virginica&quot;)plt.xlabel(&quot;Petal width (cm)&quot;)plt.ylabel(&quot;Probability&quot;)plt.legend()plt.show() ③决策树（集成学习的基础）节点，分支，打标签：（建树依据，剪枝） 计算方法： 基尼系数：向着基尼系数变小的方向优化决策树 信息熵：信息增益增加的方向优化决策树 12345678910111213141516171819202122232425262728293031import numpy as npfrom sklearn.datasets import load_irisimport matplotlib.pyplot as plt# 生成样本数据iris = load_iris()X = iris.data[:, 2:] # 花瓣长度和宽度y = iris.target# 决策树模型class DecisionTree: def fit(self, X, y): from sklearn.tree import DecisionTreeClassifier self.tree = DecisionTreeClassifier(max_depth=2) self.tree.fit(X, y) def predict(self, X): return self.tree.predict(X)# 创建模型并训练model = DecisionTree()model.fit(X, y)# 可视化from sklearn.tree import plot_treeplt.figure(figsize=(12, 8))plot_tree(model.tree, filled=True, feature_names=iris.feature_names[2:], class_names=iris.target_names)plt.show() ④朴素贝叶斯分类器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import numpy as npclass NaiveBayes: def fit(self, X, y): self.classes = np.unique(y) self.mean = np.zeros((len(self.classes), X.shape[1])) self.var = np.zeros((len(self.classes), X.shape[1])) self.priors = np.zeros(len(self.classes)) for idx, c in enumerate(self.classes): X_c = X[y == c] self.mean[idx, :] = X_c.mean(axis=0) self.var[idx, :] = X_c.var(axis=0) self.priors[idx] = X_c.shape[0] / X.shape[0] def _gaussian_density(self, class_idx, x): mean = self.mean[class_idx] var = self.var[class_idx] numerator = np.exp(- (x - mean) ** 2 / (2 * var)) denominator = np.sqrt(2 * np.pi * var) return numerator / denominator def _predict_single(self, x): posteriors = [] for idx, c in enumerate(self.classes): prior = np.log(self.priors[idx]) class_conditional = np.sum(np.log(self._gaussian_density(idx, x))) posterior = prior + class_conditional posteriors.append(posterior) return self.classes[np.argmax(posteriors)] def predict(self, X): return np.array([self._predict_single(x) for x in X])# 生成样本数据from sklearn.datasets import load_irisiris = load_iris()X, y = iris.data, iris.targetmodel = NaiveBayes()model.fit(X, y)# 预测y_pred = model.predict(X)# 计算准确率accuracy = np.mean(y_pred == y)print(f&#x27;Accuracy: &#123;accuracy * 100:.2f&#125;%&#x27;) 6、结果可视化","categories":[{"name":"萱仔求职系列","slug":"萱仔求职系列","permalink":"http://example.com/categories/%E8%90%B1%E4%BB%94%E6%B1%82%E8%81%8C%E7%B3%BB%E5%88%97/"}],"tags":[{"name":"技术, 教程","slug":"技术-教程","permalink":"http://example.com/tags/%E6%8A%80%E6%9C%AF-%E6%95%99%E7%A8%8B/"}]},{"title":"萱仔求职系列——2.1_python基础知识复习——数据类型+数据结构","slug":"萱仔求职系列——2.1_python基础知识复习——数据类型+数据结构","date":"2024-09-05T06:00:00.000Z","updated":"2024-09-09T03:00:48.695Z","comments":true,"path":"2024/09/05/萱仔求职系列——2.1_python基础知识复习——数据类型+数据结构/","permalink":"http://example.com/2024/09/05/%E8%90%B1%E4%BB%94%E6%B1%82%E8%81%8C%E7%B3%BB%E5%88%97%E2%80%94%E2%80%942.1_python%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%A4%8D%E4%B9%A0%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B+%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","excerpt":"","text":"这几天疯狂面试，每天都接到新的面试，是时候复习一下我的python基础了，虽然平时做项目的时候用的都是一些库，加上自己时常不注重基础，以至于面试的时候有的非常详细的基础部分反倒扯后腿，被某宇宙大厂面试官建议虽然可能进公司之后不需要这么细节，但是面试的时候还是应该更熟悉基础。（心痛 所以我在这里总结一些常用的基础，和一些非常常用的数据结构算法代码，自己复习的同时也记录一下。 以下基础用一些题目进行辅助理解 面试经典 150 题 - 学习计划 - 力扣（LeetCode）全球极客挚爱的技术成长平台 1.python数据类型 在 Python 中，数据类型可以分为基本数据类型和容器数据类型。以下是对 Python 中主要 9 种数据类型的详细讲解： 1. Int（整型） 描述 ：整型（ int ）用于表示整数，支持正数、负数和零。Python 中的整型可以表示任意大小的整数，内存大小仅受限于机器的可用内存。 操作 ： 加法、减法、乘法、除法 幂运算： ** 整数除法： // 取模： % 2. Float（浮点型） 描述 ：浮点型（ float ）用于表示带有小数部分的数字，符合 IEEE 754 标准的双精度浮点数。浮点型通常用于需要精度较高的数值运算。 符号位（Sign Bit） ：用于表示数值的正负。 0 表示正数， 1 表示负数。 指数部分（Exponent） ：表示浮点数的数量级。它的值通过偏移量（bias）来存储，通常是 127 对于 32 位浮点数（单精度）和 1023 对于 64 位浮点数（双精度）。 尾数部分（Mantissa&#x2F;Significand） ：表示有效数字部分，通常是一个二进制小数。尾数部分的表示是归一化的，即最前面为 1 的二进制数默认为 1，所以通常不需要显式存储。 操作 ： 加法、减法、乘法、除法 幂运算： ** 科学计数法表示： 1.23e4 表示 1.23 × 10^4 3. Bool（布尔型） 描述 ：布尔型（ bool ）用于表示真值（True 和 False）。它是整型的子类，其中 True 等价于 1 ， False 等价于 0 。 操作 ： 逻辑运算： and , or , not 比较运算： == , != , &gt; , &lt; , &gt;= , &lt;= 4. Str（字符串） 描述 ：字符串（ str ）用于表示文本数据。字符串是不可变的数据类型，定义时可以使用单引号、双引号或三引号（用于多行字符串）。 操作 ： 字符串拼接： + 字符串重复： * 索引和切片： name[0] , name[1:4] 字符串方法： lower() , upper() , replace() , find() 示例字符串s &#x3D; “Hello, World! Welcome to Python.” 1. 获取字符串长度length &#x3D; len(s) 2. 字符串拼接s1 &#x3D; “Hello”s2 &#x3D; “World”s3 &#x3D; s1 + “, “ + s2 + “!” 3. 字符串重复repeated &#x3D; s1 * 3 4. 字符串切片sub1 &#x3D; s[0:5]sub2 &#x3D; s[7:]sub3 &#x3D; s[::2] 5. 字符串查找position &#x3D; s.find(“World”)position_index &#x3D; s.index(“World”) 6. 字符串替换new_s &#x3D; s.replace(“World”, “Python”) 7. 字符串拆分words &#x3D; s.split()comma_split &#x3D; s.split(“,”) 8. 字符串连接joined_s &#x3D; “ “.join(words) 9. 字符串大小写转换upper_s &#x3D; s.upper()lower_s &#x3D; s.lower()capitalized_s &#x3D; s.capitalize()title_s &#x3D; s.title()swapped_s &#x3D; s.swapcase() 10. 去除空白字符stripped_s &#x3D; s.strip()lstripped_s &#x3D; s.lstrip()rstripped_s &#x3D; s.rstrip() 11. 字符串判断starts_with_hello &#x3D; s.startswith(“Hello”)ends_with_python &#x3D; s.endswith(“Python”)is_alnum &#x3D; s1.isalnum()is_alpha &#x3D; s1.isalpha()is_digit &#x3D; s1.isdigit() 12. 字符串格式化formatted_s &#x3D; “My name is {} and I am {} years old.”.format(“Alice”, 25)formatted_f_string &#x3D; f”My name is {‘Alice’} and I am {25} years old.” 打印所有结果print(f”Length of the string: {length}”)print(f”Concatenation: {s3}”)print(f”Repeated string: {repeated}”)print(f”Substring [0:5]: {sub1}”)print(f”Substring [7:]: {sub2}”)print(f”Substring with step 2: {sub3}”)print(f”Position of ‘World’: {position}”)print(f”Replaced string: {new_s}”)print(f”Split by space: {words}”)print(f”Split by comma: {comma_split}”)print(f”Joined string: {joined_s}”)print(f”Upper case: {upper_s}”)print(f”Lower case: {lower_s}”)print(f”Capitalized: {capitalized_s}”)print(f”Title case: {title_s}”)print(f”Swapped case: {swapped_s}”)print(f”Stripped string: {stripped_s}”)print(f”Left stripped string: {lstripped_s}”)print(f”Right stripped string: {rstripped_s}”)print(f”Starts with ‘Hello’: {starts_with_hello}”)print(f”Ends with ‘Python’: {ends_with_python}”)print(f”Is alphanumeric: {is_alnum}”)print(f”Is alphabetic: {is_alpha}”)print(f”Is digit: {is_digit}”)print(f”Formatted string: {formatted_s}”)print(f”Formatted f-string: {formatted_f_string}”) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104### 5\\. **None（空值）*** **描述** ：空值（ `None` ）表示没有值或空对象，是一种特殊的常量。在函数中经常用作返回值，表示没有计算结果。* **用途** ： + 作为默认函数返回值 + 用于初始化变量以表示尚未赋值### 6\\. **List（列表）*** **描述** ：列表（ `list` ）是有序的、可变的序列，可以包含任意类型的元素。列表可以嵌套，即列表中的元素也可以是列表。* **操作** ： + 索引和切片： `numbers[0]` , `numbers[1:3]` + 添加元素： `append()` , `insert()` + 删除元素： `remove()` , `pop()` , `del` + 列表方法： `sort()` , `reverse()`### 7\\. **Tuple（元组）*** **描述** ：元组（ `tuple` ）是有序的、不可变的序列，可以包含任意类型的元素。元组通常用于需要保护数据不被修改的场景。* **操作** ： + 索引和切片： `coordinates[0]` , `mixed[1:3]` + 解包（将元组中的元素赋值给多个变量）： `x, y = coordinates` + 方法：元组没有像列表那样丰富的方法，但支持基本操作如遍历。### **8\\. Dict（字典）*** **描述** ：字典（ `dict` ）是键值对的集合，键是唯一的。字典是无序的（Python 3\\.7\\+ 版本中为插入顺序）。字典适用于映射关系，如查找表。* **操作** ： + 访问元素： `person[&quot;name&quot;]` + 添加/修改元素： `person[&quot;age&quot;] = 30` + 删除元素： `del person[&quot;city&quot;]` + 字典方法： `keys()` , `values()` , `items()`* | **操作** | **描述** | **示例** || --- | --- | --- || **创建字典** | 使用花括号 &#123;&#125; 或 dict() 函数创建字典 | my\\_dict \\= &#123;&quot;name&quot;: &quot;Alice&quot;, &quot;age&quot;: 25&#125; || **访问值** | 使用键来访问字典中的值 | name \\= my\\_dict\\[&quot;name&quot;] || **添加/更新项** | 使用键值对来添加新项或更新现有项 | my\\_dict\\[&quot;age&quot;] \\= 26 || **删除项** | 使用 del 语句删除特定键及其对应的值 | del my\\_dict\\[&quot;age&quot;] || **删除并返回项** | 使用 pop() 方法删除并返回指定键的值 | value \\= my\\_dict.pop(&quot;name&quot;) || **删除并返回项** | 使用 popitem() 方法删除并返回最后插入的键值对 | key, value \\= my\\_dict.popitem() || **获取值** | 使用 get() 方法获取指定键的值（键不存在时返回默认值） | age \\= my\\_dict.get(&quot;age&quot;, &quot;N/A&quot;) || **清空字典** | 使用 clear() 方法清空字典 | my\\_dict.clear() || **键的视图** | 使用 keys() 方法获取字典的所有键的视图 | keys \\= my\\_dict.keys() || **值的视图** | 使用 values() 方法获取字典的所有值的视图 | values \\= my\\_dict.values() || **项的视图** | 使用 items() 方法获取字典的所有键值对的视图 | items \\= my\\_dict.items() || **更新字典** | 使用 update() 方法将另一个字典的键值对合并到当前字典中 | my\\_dict.update(&#123;&quot;city&quot;: &quot;New York&quot;&#125;) || **检查键是否存在** | 使用 in 运算符检查键是否在字典中 | &quot;name&quot; in my\\_dict || **获取键的数量** | 使用 len() 函数获取字典中键的数量 | num\\_items \\= len(my\\_dict) | 创建字典my_dict &#x3D; {“name”: “Alice”, “age”: 25} 访问值print(my_dict[“name”]) # 输出: Alice 添加&#x2F;更新项my_dict[“age”] &#x3D; 26my_dict[“city”] &#x3D; “New York” 删除项del my_dict[“city”] 删除并返回项age &#x3D; my_dict.pop(“age”)print(age) # 输出: 26 删除并返回项key, value &#x3D; my_dict.popitem()print(key, value) # 输出: name Alice 获取值age &#x3D; my_dict.get(“age”, “N&#x2F;A”)print(age) # 输出: N&#x2F;A 清空字典my_dict.clear() 键的视图my_dict &#x3D; {“name”: “Alice”, “age”: 25}keys &#x3D; my_dict.keys()print(keys) # 输出: dict_keys([‘name’, ‘age’]) 值的视图values &#x3D; my_dict.values()print(values) # 输出: dict_values([‘Alice’, 25]) 项的视图items &#x3D; my_dict.items()print(items) # 输出: dict_items([(‘name’, ‘Alice’), (‘age’, 25)]) 更新字典my_dict.update({“city”: “New York”, “country”: “USA”}) 检查键是否存在print(“name” in my_dict) # 输出: True 获取键的数量num_items &#x3D; len(my_dict)print(num_items) # 输出: 3 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253### 9\\. **Set（集合）*** **描述** ：集合（ `set` ）是无序的、元素唯一的数据结构。集合用于存储不重复的元素，并支持集合运算（如并集、交集）。* **操作** ： + 添加元素： `add()` + 删除元素： `remove()` , `discard()` + 集合运算：交集（ `&amp;` ），并集（ `|` ），差集（ `-` ）* 常见的算法题用处： + 力扣面试150\\-88题，去重，（当然这道题还是双指针的方法最好了，因为是要原地处理有序数组，保证旁边都是一样的数，这里举个例子如何用集合去重） + ``` def remove_duplicates(nums): # 创建一个空集合来保存唯一元素 unique_set = set() # 定义一个索引指针，用于将唯一元素放置在 nums 中 index = 0 # 遍历 nums 数组 for num in nums: # 如果 num 不在集合中，说明它是一个新的唯一元素 if num not in unique_set: # 将 num 添加到集合中 unique_set.add(num) # 将唯一元素放置到 nums 的当前位置 nums[index] = num # 移动索引指针 index += 1 # 返回唯一元素的个数 return index # 示例 nums = [1, 1, 2, 2, 3, 4, 4, 5] length = remove_duplicates(nums) # 输出结果 print(&quot;新长度:&quot;, length) # 输出: 新长度: 5 print(&quot;修改后的数组:&quot;, nums[:length]) # 输出: 修改后的数组: [1, 2, 3, 4, 5] 常用数据类型对比列表（List） vs 元组（Tuple） vs 集合（Set）vs字典（DICT） 特性 列表（List） 元组（Tuple） 集合（Set） 字典（Dict） 定义 用方括号 [] 定义 用圆括号 () 定义 用花括号 {} 定义 用花括号 {} 定义，键值对 key: value 可变性 可变（可以修改、添加、删除元素） 不可变（不能修改、添加、删除元素） 可变（可以修改、添加、删除元素） 可变（可以修改、添加、删除键值对） 有序性 有序（元素顺序保持） 有序（元素顺序保持） 无序（不保证元素顺序） 无序（保持插入顺序） 支持重复 支持重复元素 支持重复元素 不支持重复元素 键不支持重复，值支持重复 操作 支持丰富的操作：append(), extend(), remove(), pop(), 切片等 支持基本操作：切片, 解包等 支持集合运算：并集 ` ，交集 &amp; ，差集 -` 性能 对于大规模数据，性能较低（因为可变性） 性能较高（由于不可变性，通常比列表快） 由于哈希表实现，查找操作非常高效 查找、插入和删除操作平均时间复杂度为 O(1) 用途 适用于需要频繁修改、添加、删除的场景 适用于数据不需要修改的场景，例如作为字典的键 适用于去重操作和集合运算 适用于键值对映射，如存储对象的属性等 示例 my_list = [1, 2, 3, 4] my_tuple = (1, 2, 3, 4) my_set = {1, 2, 3} my_dict = {“name”: “Alice”, “age”: 25} 2.常用的数据结构顺序表 是一种线性数据结构，其元素顺序存储在一块连续的内存空间中。在 Python 中，顺序表通常由列表（ list ）来实现。 顺序表的基本操作包括插入、删除、查找和遍历。顺序表的插入和删除操作包括在表的头部、尾部和任意位置进行插入和删除。查找操作包括遍历查找和二分查找。 1. 数组（Array）——顺序表 定义 : 数组是一种线性数据结构，由一组连续的内存位置组成，其中每个元素可以通过索引直接访问。 特点 : 元素按顺序存储，支持随机访问。 大小固定（静态数组）。 用法示例 : 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 初始化一个顺序表（列表）seq_list = [2, 4, 6, 8]# 头插seq_list.insert(0, 1) # 在索引0位置插入元素1# 尾插seq_list.append(10) # 在尾部添加元素10# 任意位置插入seq_list.insert(2, 3) # 在索引2位置插入元素3# 头删seq_list.pop(0) # 删除索引0位置的元素# 尾删seq_list.pop() # 删除尾部元素# 任意位置删除seq_list.pop(2) # 删除索引2位置的元素# 遍历查找def linear_search(seq_list, target): for i, value in enumerate(seq_list): if value == target: return i # 返回目标元素的索引 return -1 # 如果未找到，返回-1index = linear_search(seq_list, 6)# 二分查找def binary_search(seq_list, target): left, right = 0, len(seq_list) - 1 while left &lt;= right: mid = (left + right) // 2 if seq_list[mid] == target: return mid # 返回目标元素的索引 elif seq_list[mid] &lt; target: left = mid + 1 else: right = mid - 1 return -1 # 如果未找到，返回-1index = binary_search(seq_list, 6)# 打印结果print(&quot;头插:&quot;, seq_list)print(&quot;尾插:&quot;, seq_list)print(&quot;任意位置插入:&quot;, seq_list)print(&quot;头删:&quot;, seq_list)print(&quot;尾删:&quot;, seq_list)print(&quot;任意位置删除:&quot;, seq_list)print(&quot;遍历查找:&quot;, index)print(&quot;二分查找:&quot;, index) 2. 链表（Linked List）——顺序表 链表是一种常用的动态数据结构，适合频繁的插入和删除操作。链表的每个元素称为一个节点，节点包含数据和指向下一个节点的指针。在链表中，元素的插入和删除不需要移动其他元素，但需要调整指针的指向。 可以使用链表来实现与顺序表（列表）类似的操作，如头插、尾插、任意位置插入、头删、尾删、任意位置删除、遍历查找等。 定义 : 链表是一种线性数据结构，其中每个元素称为节点，节点包含数据和指向下一个节点的指针。 特点 : 支持动态内存分配。 插入和删除操作在链表中更高效。 用法示例 : 123456789class ListNode: def __init__(self, value=0, next=None): self.value = value self.next = next# 初始化链表为空head = None 这里遇到最多的算法题反转链表，力扣面试150-92反转链表， + #### 步骤解析（这个题与普通反转列表的区别就是普通的全部都反转了无需考虑前后点，这个也只是多了一步确认从哪开始反转） + **定位反转的起点** ：通过遍历链表，找到位置 `left` 的节点，即反转部分的起点。这个节点的前一个节点我们称为 `prev` 。 + **反转指定区间的节点** ：使用三个指针进行反转操作，分别是 `prev` （当前节点的前一个节点）， `current` （当前节点），以及 `next` （当前节点的下一个节点）。 + **连接反转后的部分** ：将反转后的部分连接回原链表。 `prev` 指向反转后的头节点，反转区间的末尾节点连接到 `right` 位置后的第一个节点。 + ``` class ListNode: def __init__(self, value=0, next=None): self.value = value self.next = next def reverse_between(head, left, right): if not head or left == right: return head # 创建一个x，方便处理头节点为left的情况 x = ListNode(0) x.next = head prev = x #移动 prev 到 left 的前一个节点 for _ in range(left - 1): prev = prev.next #开始反转 y = prev.next next = None for _ in range(right - left): next = y.next #将 y 节点的下一个节点存储在 next 中。next 变量保存了即将被反转的节点。 y.next = next.next #设置为 next.next，即跳过 next 节点， #使 y 节点直接连接到 next 节点的下一个节点。将 next 从当前链表中拆出来。 next.next = prev.next #将 next 节点的下一个指针 next.next 指向 prev.next，即链表中反转部分的当前头部。 #这样，next 节点就成为了反转部分的新头部。 prev.next = next #更新 prev.next 指向新的头部 next，即反转后的链表部分。 #prev 节点的 next 就连接到了新的头节点上，完成了反转。 return dummy.next 12345678910111213141516171819### 3\\. **栈（Stack）*** **定义** : 栈是一种线性数据结构，遵循后进先出（LIFO）的原则。* **特点** : + 插入和删除操作只能在栈顶进行。 + 用于处理逆序问题（如括号匹配、逆波兰表达式计算）。* **用法示例** :力扣面试150\\-20有效的括号（注意这道题出现在两次面试中，不得不说力扣这个刷题的确实很管用） ![](https://i-blog.csdnimg.cn/direct/0e4224744a6149098ecbf3f61ffe4471.png) class Solution: def isValid(self, s: str) -&gt; bool: dic &#x3D; {‘{‘: ‘}’, ‘[‘: ‘]’, ‘(‘: ‘)’, ‘?’: ‘?’} #这里用字典匹配后续的括号，前后括号 stack &#x3D; [‘?’] #到时候如果最后只剩下？了证明就是对滴 for c in s: if c in dic: stack.append(c) elif dic[stack.pop()] !&#x3D; c: return False return len(stack) &#x3D;&#x3D; 1 123456789101112131415161718192021### 4\\. **队列（Queue）*** **定义** : 队列是一种线性数据结构，遵循先进先出（FIFO）的原则。* **特点** : + 插入操作发生在队尾，删除操作发生在队首。 + 常用于任务调度和缓冲区管理。* **用法示例** :* ```队列先进先出的原则可以使用在归并排序中：例如面试150-23题目 本题目可以使用最小堆（优先级队列）来合并链表。（这个题目是个困难题，印象中好像某个大厂出现过原题，我当时写的啥也不是，这题目我需要再斟酌，有大佬有更好的思路拜托大佬们评论区指导我一下谢谢谢谢！） 5. 树（Tree） 定义 : 树是一种非线性数据结构，由节点组成，通常具有层次结构。每个节点包含数据，并可以有零个或多个子节点。 特点 : 特殊的树结构如二叉树、二叉搜索树、AVL 树等。 常用于表示层次关系（如文件系统、组织结构）。 用法示例 :树真的超级常用递归，完全可以在树的这里好好学习一番递归函数，因为他的性质很合适 123456class Solution: def maxDepth(self, root: Optional[TreeNode]) -&gt; int: if not root: return 0 return max(self.maxDepth(root.left), self.maxDepth(root.right)) + 1 还有就是二叉树的各种遍历，需要跟排序算法一样了如指掌，实在不会背过都可以（开玩笑的哈哈），以这个为延申可以做很多题目。 12345678910111213141516171819202122232425262728293031323334353637383940class TreeNode: def __init__(self, val=0, left=None, right=None): self.val = val self.left = left self.right = rightdef preorder_traversal(root):#前序遍历 if root is None: return [] # 访问根节点 result = [root.val] # 递归遍历左子树 result += preorder_traversal(root.left) # 递归遍历右子树 result += preorder_traversal(root.right) return resultdef inorder_traversal(root): #中序遍历 if root is None: return [] # 递归遍历左子树 result = inorder_traversal(root.left) # 访问根节点 result.append(root.val) # 递归遍历右子树 result += inorder_traversal(root.right) return resultdef postorder_traversal(root):#后续遍历 if root is None: return [] # 递归遍历左子树 result = postorder_traversal(root.left) # 递归遍历右子树 result += postorder_traversal(root.right) # 访问根节点 result.append(root.val) return result 6. 图（Graph） 定义 : 图是一种非线性数据结构，由顶点和边组成。顶点通过边相互连接。 特点 : 图可以是有向图或无向图，有权图或无权图。 常用于网络路由、社交网络分析等。 用法示例 : 123 7. 堆（Heap） 定义 : 堆是一种特殊的树形数据结构，满足堆属性。最大堆中每个节点的值都大于或等于其子节点的值，最小堆中每个节点的值都小于或等于其子节点的值。 特点 : 常用于实现优先队列。 支持快速获取最大值或最小值。 用法示例 :堆排序谁懂！超级容易出现的面试题目，冲冲冲！ 123 8. 散列表（Hash Table） 定义 : 散列表是一种数据结构，通过将键映射到值来实现键值对的存储。它使用哈希函数将键映射到数组中的特定位置。 特点 : 支持快速的查找、插入和删除操作。 Python 中的字典（ dict ）实现了散列表。 3.常用的关键字集锦 关键字 作用 使用场景 补充说明 and 逻辑与 用于两个条件都为真时，结果为真 可用于条件组合 or 逻辑或 两个条件中只要一个为真，结果为真 可用于条件组合 not 逻辑非 将布尔值取反 可用于布尔值判断 if 条件判断 条件为真时执行代码块 常与 else 、 elif 结合 elif 额外条件 if 条件不满足时的额外判断 通常跟在 if 之后 else 备用方案 当 if 和 elif 条件不满足时执行 用于条件语句和异常处理 for 循环 遍历序列或可迭代对象 常与 range() 结合使用 while 循环 当条件为真时，重复执行代码块 适合未知循环次数的场景 continue 跳过当前循环 跳过当前循环的剩余部分，进入下一次循环 主要用于循环控制 break 终止循环 立即终止当前循环 适合提前结束循环 pass 占位符 什么也不做，用于占位 用于未实现的代码块 try 异常捕获 用于捕获可能出现的异常 常与 except 、 finally 结合 except 异常处理 在捕获异常后执行的代码块 用于处理异常 finally 异常清理 无论是否有异常，都会执行 常用于资源清理 raise 抛出异常 主动引发一个异常 可自定义异常类型 import 导入模块 导入外部模块或库 常与 from 结合 def 定义函数 定义一个函数或方法 函数体内使用 return 返回值 return 返回值 从函数中返回一个值并结束函数 用于结束函数的执行 class 定义类 定义一个类 类定义中包含属性和方法 lambda 匿名函数 定义一个简短的匿名函数 通常用于简单函数表达式 global 声明全局变量 声明一个变量为全局变量 用于函数内部修改全局变量 nonlocal 声明非局部变量 用于嵌套函数中，声明外层函数的变量 修改闭包作用域中的变量 in 成员判断 检查一个值是否在序列或集合中 常用于 if 语句 is 身份判断 判断两个对象是否为同一个对象 比较对象的内存地址 None 空值 表示什么都没有 NoneType 的实例 assert 断言 用于调试 条件为 False 时触发异常 with 上下文管理 简化资源管理 常与 open 一起使用 yield 生成器 定义一个生成器函数 函数会暂停并返回值 4.常用的函数列举 函数 作用 使用场景 补充说明 next() 获取迭代器的下一个元素 手动遍历迭代器 如果没有更多元素，会抛出 StopIteration 异常 len() 返回对象的长度 计算序列或集合的元素数量 常用于 str 、 list 、 tuple 等 range() 返回一个范围对象 生成一个数字序列 通常用于循环 enumerate() 返回枚举对象 在循环中获取索引和值 常用于 for 循环 zip() 打包可迭代对象 将多个序列打包成元组的迭代器 用于并行遍历 map() 函数映射 将函数应用于可迭代对象的每个元素 返回一个迭代器 filter() 过滤元素 根据条件筛选可迭代对象的元素 返回满足条件的元素 sorted() 返回排序后的列表 对序列进行排序 不修改原序列，返回新的列表 min() 返回最小值 查找序列或集合中的最小元素 适用于 str 、 list 等 max() 返回最大值 查找序列或集合中的最大元素 适用于 str 、 list 等 sum() 计算总和 求序列或集合中元素的总和 仅适用于数字类型 abs() 绝对值 计算数字的绝对值 返回非负数 all() 全真判断 判断可迭代对象的所有元素是否为真 如果全为真，返回 True any() 存在真判断 判断可迭代对象中是否有任意一个元素为真 如果有一个为真，返回 True type() 返回对象类型 获取对象的类型 常用于检查对象类型 isinstance() 检查实例 判断对象是否为特定类的实例 用于类型判断 id() 返回对象唯一标识 获取对象的内存地址 常用于比较对象身份 注意其中next面试的时候被问到，我回答的不太靠谱，这里涉及到迭代器和生成器的原理，我在这里补充一下： 5.迭代器和生成器1. 迭代器原理 迭代器（Iterator）是一种用于遍历数据结构中元素的设计模式，能够按照顺序访问容器中的每个元素，而不需要了解容器的底层实现。迭代器提供了一种统一的接口来访问容器中的元素。 迭代器的核心特征 ： 封装 ：迭代器封装了数据的遍历过程，不需要暴露容器的内部结构。 状态保存 ：迭代器能够在遍历过程中保存状态，以便在需要时恢复。 实现原理 ： 迭代器协议 ：在 Python 中，迭代器需要实现两个关键方法： __iter__() ：返回迭代器对象自身，通常可以直接返回 self 。 __next__() ：返回容器中的下一个元素。如果没有更多元素可以返回，则抛出 StopIteration 异常，表示迭代已结束。 2. Python 中的迭代器与可迭代对象 在 Python 中，有两种重要的概念： 可迭代对象（Iterable） ：指那些可以返回迭代器的对象，如列表、元组、字典、集合、字符串等。可迭代对象实现了 __iter__() 方法，但不一定实现 __next__() 方法。 迭代器（Iterator） ：指那些实现了 __iter__() 和 __next__() 方法的对象。迭代器不仅是可迭代对象，还可以通过 next() 函数获取下一个元素。 迭代器(iterator)一定是可迭代对象(iterable)，可迭代对象不一定是迭代器 可迭代对象 ： 列表 ( list ) 元组 ( tuple ) 字典 ( dict ) 集合 ( set ) 字符串 ( str ) 迭代器 ： 生成器 （使用 yield 的函数） 自定义的迭代器类 （如上述示例） 特性 列表 (List) 迭代器 (Iterator) 生成器 (Generator) 定义 有序的元素集合，可变的容器类型 实现了 __iter__() 和 __next__() 方法的对象 使用 yield 关键字创建的特殊迭代器 创建 使用方括号创建，如 [1, 2, 3] 使用 iter() 函数创建，如 iter(lst) 使用生成器函数创建，如 def gen(): yield … 存储方式 存储所有元素在内存中 按需生成元素，不需一次性存储所有元素 按需生成元素，不需一次性存储所有元素 访问方式 支持随机访问，如 lst[0] 只能逐个访问，通过 next() 函数 只能逐个访问，通过 next() 函数 内存使用 存储所有元素，占用更多内存 节省内存，按需生成元素 节省内存，按需生成元素 迭代功能 可以被 for 循环遍历 可以被 for 循环遍历 可以被 for 循环遍历 重复使用 可以多次遍历 遍历一次后不能重用，需要重新创建 遍历一次后不能重用，需要重新创建 性能 遍历速度快，但内存消耗大 性能较好，但每次调用 next() 可能略慢 性能较好，特别适合处理大数据 示例 lst = [1, 2, 3] it = iter([1, 2, 3]) def gen(): yield 1; yield 2; yield 3 3. 生成器原理 生成器（Generator）是 Python 中一种特殊的迭代器，用于按需生成值。与一次性创建所有元素的数据结构（如列表或元组）不同，生成器每次迭代时只生成下一个值，因此更节省内存并支持无限序列或大量数据流的操作。 2. 生成器的关键特性 延迟计算 ：生成器不会一次性计算所有结果，而是每次需要时逐步计算并生成值。这种按需计算的方式可以节省内存。 状态保持 ：生成器在每次 yield 语句执行后会保存其执行状态，下一次调用时会从上次暂停的地方继续执行。 内存效率 ：生成器在生成值时只需存储当前状态，相比于列表或元组，内存消耗较小。 3.yield 关键字 定义生成器函数 ： yield 关键字用于定义生成器函数。生成器函数是一种特殊的函数，当执行到 yield 语句时，函数的执行会暂停，并返回 yield 后面的值。函数的状态会被保存，每次迭代时从上次暂停的地方继续执行。 以下代码创造一个生成器 1234567891011def count_up_to(max): count = 1 while count &lt;= max: yield count count += 1counter = count_up_to(5)for num in counter: print(num)","categories":[{"name":"萱仔求职系列","slug":"萱仔求职系列","permalink":"http://example.com/categories/%E8%90%B1%E4%BB%94%E6%B1%82%E8%81%8C%E7%B3%BB%E5%88%97/"}],"tags":[{"name":"技术, 教程","slug":"技术-教程","permalink":"http://example.com/tags/%E6%8A%80%E6%9C%AF-%E6%95%99%E7%A8%8B/"}]},{"title":"萱仔求职系列——3.1_力扣面试150题目——数组&字符串第一弹","slug":"萱仔求职系列——3.1_力扣面试150题目——数组&字符串第一弹","date":"2024-09-05T06:00:00.000Z","updated":"2024-09-09T03:00:59.960Z","comments":true,"path":"2024/09/05/萱仔求职系列——3.1_力扣面试150题目——数组&字符串第一弹/","permalink":"http://example.com/2024/09/05/%E8%90%B1%E4%BB%94%E6%B1%82%E8%81%8C%E7%B3%BB%E5%88%97%E2%80%94%E2%80%943.1_%E5%8A%9B%E6%89%A3%E9%9D%A2%E8%AF%95150%E9%A2%98%E7%9B%AE%E2%80%94%E2%80%94%E6%95%B0%E7%BB%84&%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%AC%AC%E4%B8%80%E5%BC%B9/","excerpt":"","text":"由于最近求职又涉及到很多面试时候敲代码，我又不得不捡起我的老力扣，只能说刷题这个东西必须越耍越上瘾，我将稳定更新，每天做3-10题 面试经典 150 题 - 学习计划 - 力扣（LeetCode）全球极客挚爱的技术成长平台 ------------------------------------------------------------------------------------------------------- 88. 合并两个有序数组 给你两个按 非递减顺序 排列的整数数组 nums1 和 nums2 ，另有两个整数 m 和 n ，分别表示 nums1 和 nums2 中的元素数目。 请你 合并nums2 到 nums1 中，使合并后的数组同样按 非递减顺序 排列。 注意： 最终，合并后数组不应由函数返回，而是存储在数组 nums1 中。为了应对这种情况， nums1 的初始长度为 m + n ，其中前 m 个元素表示应合并的元素，后 n 个元素为 0 ，应忽略。 nums2 的长度为 n 。 这个题目吧，非常简单，要把n2合到n1里，不能重新搞一个新的表出来，由于它本身就是两个非递减的整数数组，我直接用3个指针就好啦，重点在于我要从后往前里面填入数据，因为他那个从前往后会盖住后面的数，没有空位。 对比两个蓝色的，然后大的往绿色那填，就这么easy~ 1234567891011121314151617class Solution: def merge(self, nums1: List[int], m: int, nums2: List[int], n: int) -&gt; None: # 初始化指针 n1 = m - 1 # nums1 的有效部分的最后一个元素 n2 = n - 1 # nums2 的最后一个元素 j = m + n - 1 # 合并后数组的最后一个位置 for i in range(m + n): if n2 &lt; 0:#没有可以合并的啦 break elif n1 &gt;= 0 and nums1[n1] &gt; nums2[n2]: nums1[j] = nums1[n1] n1=n1-1 else: nums1[j] = nums2[n2] n2=n2-1 j=j-1 27. 移除元素 给你一个数组 nums 和一个值 val ，你需要 原地 移除所有数值等于 val 的元素。元素的顺序可能发生改变。然后返回 nums 中与 val 不同的元素的数量。 假设 nums 中不等于 val 的元素数量为 k ，要通过此题，您需要执行以下操作： 更改 nums 数组，使 nums 的前 k 个元素包含不等于 val 的元素。 nums 的其余元素和 nums 的大小并不重要。 返回 k 。 这个有一个题解写的很好，简单来说就是把所有不等于val的填到nums里 12345678910class Solution: def removeElement(self, nums: List[int], val: int) -&gt; int: k=0 for i in range(0,len(nums)): if nums[i]!=val: nums[k]=nums[i] k=k+1 #print(k) return k 26. 删除有序数组中的重复项 这个题我用字典记录出现过的数字，然后遍历即可 12345678910111213class Solution: def removeDuplicates(self, nums: List[int]) -&gt; int: dic = &#123;&#125; k = 0 # 个数 for i in range(len(nums)): if nums[i] not in dic: dic[nums[i]] = True # 记录该元素已经出现过 nums[k] = nums[i] # 将非重复元素放在前面 k += 1 # 计数+1 return k 80. 删除有序数组中的重复项 II 给你一个有序数组 nums ，请你 原地 删除重复出现的元素，使得出现次数超过两次的元素 只出现两次 ，返回删除后数组的新长度。 不要使用额外的数组空间，你必须在 原地 修改输入数组 并在使用 O(1) 额外空间的条件下完成。 123456789101112131415class Solution: def removeDuplicates(self, nums: List[int]) -&gt; int: # 如果数组长度小于等于 2，直接返回长度 if len(nums) &lt;= 2: return len(nums) # k 指向要插入的下一个位置 k = 2 # 从索引 2 开始遍历数组 for i in range(2, len(nums)): # 只要当前元素与 k-2 位置的元素不相同，说明没有超过两次 if nums[i] != nums[k - 2]: nums[k] = nums[i] k += 1 return k 169. 多数元素 给定一个大小为 n 的数组 nums ，返回其中的多数元素。多数元素是指在数组中出现次数 大于⌊ n/2 ⌋ 的元素。 你可以假设数组是非空的，并且给定的数组总是存在多数元素。 接着用字典解决熬，记住每个词出现的次数，我这个解法虽然很拉，但是个人认为思路简单，无脑解决问题 123456789101112131415class Solution: def majorityElement(self, nums: List[int]) -&gt; int: dic=&#123;&#125; k=0 for i in range(0,len(nums)): if nums[i] not in dic: dic[nums[i]]=1 elif nums[i] in dic: dic[nums[i]]=1+dic[nums[i]] #每个数字出现的次数，遇见了就加一个 for key, value in dic.items(): if value&gt;(len(nums)//2) : print(key,value) k=key #输出符合要求的key return k 189. 轮转数组 给定一个整数数组 nums ，将数组中的元素向右轮转 k 个位置，其中 k 是非负数。 真的很无语以下是一个使用了切片的错误代码，求有没有大佬可以帮看一下这个错误的为什么不对，明明print是对的。 1234567891011121314class Solution: def rotate(self, nums: List[int], k: int) -&gt; None: &quot;&quot;&quot; Do not return anything, modify nums in-place instead. &quot;&quot;&quot; n=len(nums) n1=nums[0:k+1] n2=nums[k+1:n] nums2=n2 for i in range(0,len(n1)): nums2.append(n1[i]) print(nums2) return nums2 这是结果：","categories":[{"name":"萱仔求职系列","slug":"萱仔求职系列","permalink":"http://example.com/categories/%E8%90%B1%E4%BB%94%E6%B1%82%E8%81%8C%E7%B3%BB%E5%88%97/"}],"tags":[{"name":"技术, 教程","slug":"技术-教程","permalink":"http://example.com/tags/%E6%8A%80%E6%9C%AF-%E6%95%99%E7%A8%8B/"}]},{"title":"旧项目新学习-天池-零基础入门NLP_-_新闻文本分类_-_BERT算法处理","slug":"旧项目新学习-天池-零基础入门NLP_-_新闻文本分类_-_BERT算法处理","date":"2024-09-04T06:00:00.000Z","updated":"2024-09-08T15:05:14.703Z","comments":true,"path":"2024/09/04/旧项目新学习-天池-零基础入门NLP_-_新闻文本分类_-_BERT算法处理/","permalink":"http://example.com/2024/09/04/%E6%97%A7%E9%A1%B9%E7%9B%AE%E6%96%B0%E5%AD%A6%E4%B9%A0-%E5%A4%A9%E6%B1%A0-%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP_-_%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB_-_BERT%E7%AE%97%E6%B3%95%E5%A4%84%E7%90%86/","excerpt":"","text":"当时代码使用了机器学习的方法进行处理，结果仅有80%多的准确率，最近整理代码发现这个项目已经有了更好的处理方式，记录一下代码的训练过程， bert算法，可以用于nlp任务，由Google AI于2018年推出。在多种语言理解任务中取得了显著的性能提升，包括问答系统、语言推断、情感分析等。核心创新在于利用双向Transformer网络预训练深度双向表征，这些表征随后可以被微调用来改善各种特定的NLP任务。 1、bert理论（待补充）（已经补充至萱仔大模型学习记录） bert算法是自然语言处理（NLP）领域的一项重要技术。这种算法帮助计算机理解和使用人类语言。这个算法特色是其“双向”处理能力。不同于传统语言模型只从一个方向读取文本，bert能同时从左到右和从右到左考虑上下文，这样能更全面地理解每个词的含义。Transformers结构适合处理大量数据。通过在大量文本上的训练，例如书籍、文章和网页。 bert训练方式 bert有两种训练方式在实际应用中，BERT能显著提升翻译、问题回答、内容总结等任务的表现。总的来说，bert算法通过模仿阅读和理解的过程，提高计算机处理语言任务的能力。 1. 掩码语言模型（MLM） 在这个训练任务中，算法从输入的文本中随机选择并隐藏一定比例的词汇。例如，句子“我喜欢吃苹果”可能被转换为“我喜欢吃 [MASK]”。然后，BERT需要预测出被掩盖的词是什么。这种方法迫使模型不仅考虑掩码词前的语境，也考虑后面的语境，从而实现双向理解。 2. 下一句预测（NSP） 在这个任务中，模型接收一对句子，并需要预测第二个句子是否是第一个句子的逻辑后续。这种类型的训练帮助模型学习理解句子之间的关系，问答系统和自然语言推理可以使用到这个模型。 bert微调 微调（Fine-tuning）是将预训练的模型应用到特定任务的过程。这一步骤是在预训练完成后进行的，目的是调整模型以适应具体的应用场景，如情感分析、文本分类、问答系统等。下面是进行微调的典型步骤： 1. 选择适当的预训练模型 首先，需要选取一个已经在大规模文本数据上进行预训练的模型。 2. 准备任务特定的数据集 根据需要解决的问题，准备相应的数据集。这个数据集应包含任务相关的输入文本以及对应的标签或答案。例如，对于情感分析任务，数据集会包含文本和其对应的情感标签（如正面、负面）。还有我这个任务中的文本数据数据集包括了14个类别的标签。 3. 设置任务特定的模型结构 一般来说bert的核心架构不变，为了完成我具体的任务，我得在其顶部添加特定的层来适应具体的任务。例如，对于分类任务，可以在BERT的输出上添加一个全连接层，这个层负责将BERT的输出映射到类别标签上。 4. 微调超参数调参 要调整一些超参数，如学习率lr，轮数epoch，批量batch size。这些参数会影响训练的速度和最终的模型效果。 5. 训练模型 使用特定任务的数据集来训练模型。这个过程中，模型参数会根据任务数据进行调整。通常，微调不需要像预训练那样的大量迭代，因为模型已经具备了一定的语言处理能力。 6. 评估模型性能 在独立的验证数据集上评估模型的表现，确保模型没有过拟合。可能需要反复调整模型结构和超参数，以达到最佳效果。 7. 应用模型 一旦模型在验证集上表现良好，就可以在实际的应用场景中部署使用了。需要持续监控模型的表现，并根据实际使用中遇到的问题进行调整。 这个微调过程使得模型可以从通用的语言理解转变为针对特定任务的优化，提高了模型在特定领域的应用效果和准确度。 2、bert算法微调实践demo 由于本地内存实在有限，选择先调试通代码然后去服务器运行，以下是具体步骤。 将bert用于本新闻文本分类算法的数据集，本代码的结构是一个tool.py,train.py,test.py,eva.py: 首先是tool的代码，其中包括了数据集的处理，和bert模型的引入，后面补充加了一个全连接层进行训练。主要是为了适应我这个工作输出新闻文本类型有14个类型。 12345678910111213141516171819202122232425262728293031323334353637383940#xuan_tools.pyimport torchfrom torch.utils.data import Datasetfrom transformers import BertTokenizer, BertModelimport pandas as pdimport torch.nn as nnclass XuanDataset(Dataset): def __init__(self, sentences, labels, max_length=128): self.tokenizer = BertTokenizer.from_pretrained(&#x27;D:/pytorch/nlp_new&#x27;)#这里放模型架构文件地址 self.sentences = sentences self.labels = torch.tensor(labels, dtype=torch.long) self.max_length = max_length def __len__(self): return len(self.sentences) def __getitem__(self, index): sent = self.sentences[index] label = self.labels[index] encoded_pair = self.tokenizer(sent, padding=&#x27;max_length&#x27;, truncation=True, max_length=self.max_length, return_tensors=&#x27;pt&#x27;) token_ids = encoded_pair[&#x27;input_ids&#x27;].squeeze(0) attn_masks = encoded_pair[&#x27;attention_mask&#x27;].squeeze(0) token_type_ids = encoded_pair[&#x27;token_type_ids&#x27;].squeeze(0) return token_ids, attn_masks, token_type_ids, labelclass Bert_c(nn.Module): def __init__(self, num_classes=14):#这里是需要分类的类别 super(Bert_c, self).__init__() self.bert = BertModel.from_pretrained(&#x27;D:/pytorch/nlp_new&#x27;) #这里放模型架构文件https://huggingface.co/models这里下载 self.linear = nn.Linear(768, num_classes) self.dropout = nn.Dropout(0.5) def forward(self, token_ids, attn_masks, token_type_ids): outputs = self.bert(input_ids=token_ids, attention_mask=attn_masks, token_type_ids=token_type_ids) logits = self.linear(self.dropout(outputs.pooler_output)) return logits 然后是训练的过程如下： 可以看出来训练还是选择读入了以前那个新闻文本分类的数据集，然后对bert的算法进行微调 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# train.pyimport torchimport torch.nn as nnimport torch.optim as optimfrom torch.utils.data import DataLoaderfrom sklearn.model_selection import train_test_splitimport pandas as pdimport matplotlib.pyplot as pltfrom xuan_tools import XuanDataset, Bert_cdef main(): device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;) df = pd.read_csv(&#x27;D:/pytorch/nlp_new/train.csv&#x27;, sep=&#x27;\\t&#x27;) X_train, X_test, y_train, y_test = train_test_split(df[&#x27;text&#x27;], df[&#x27;label&#x27;], test_size=0.2, random_state=42) train_dataset = XuanDataset(X_train.tolist(), y_train.tolist()) train_loader = DataLoader(dataset=train_dataset, batch_size=2, shuffle=True) model = Bert_c(num_classes=14).to(device) optimizer = optim.Adam(model.parameters(), lr=1e-3) loss_fn = nn.CrossEntropyLoss() losses = [] model.train() for epoch in range(100):#这里是训练轮数 total_loss = 0 for token_ids, attn_masks, token_type_ids, labels in train_loader: token_ids, attn_masks, token_type_ids, labels = token_ids.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device) optimizer.zero_grad() outputs = model(token_ids, attn_masks, token_type_ids) loss = loss_fn(outputs, labels) loss.backward() optimizer.step() total_loss += loss.item() losses.append(total_loss) print(f&#x27;Epoch &#123;epoch + 1&#125;, Loss: &#123;total_loss&#125;&#x27;) torch.save(model.state_dict(), &#x27;bert_model.pth&#x27;) plt.plot(losses) plt.title(&#x27;Training Loss&#x27;) plt.xlabel(&#x27;Epoch&#x27;) plt.ylabel(&#x27;Loss&#x27;) plt.savefig(&#x27;training_loss.png&#x27;)if __name__ == &#x27;__main__&#x27;: main() 验证的代码如下： 1234567891011121314151617181920212223242526272829303132333435import torchimport torch.nn as nnimport torch.optim as optimfrom torch.utils.data import DataLoaderfrom sklearn.model_selection import train_test_splitimport pandas as pdimport matplotlib.pyplot as pltfrom xuan_tools import XuanDataset, Bert_cdef main(): device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;) model = BertClassify().to(device) model.load_state_dict(torch.load(&#x27;bert_model.pth&#x27;)) model.eval() df = pd.read_csv(&#x27;D:/pytorch/nlp_new/eva.csv&#x27;, delimiter=&#x27;\\t&#x27;) X_val, y_val = df[&#x27;text&#x27;].tolist(), df[&#x27;label&#x27;].tolist() validate_dataset = MyDataset(X_val, y_val) validate_loader = Data.DataLoader(dataset=validate_dataset, batch_size=2, shuffle=False) correct = 0 total = 0 with torch.no_grad(): for token_ids, attn_masks, token_type_ids, labels in validate_loader: token_ids, attn_masks, token_type_ids, labels = token_ids.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device) outputs = model(token_ids, attn_masks, token_type_ids) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print(f&#x27;Accuracy of the network on the validation texts: &#123;100 * correct / total&#125;%&#x27;)if __name__ == &#x27;__main__&#x27;: main()","categories":[{"name":"大模型学习记录","slug":"大模型学习记录","permalink":"http://example.com/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"技术, nlp, bert, 理论, 大模型","slug":"技术-nlp-bert-理论-大模型","permalink":"http://example.com/tags/%E6%8A%80%E6%9C%AF-nlp-bert-%E7%90%86%E8%AE%BA-%E5%A4%A7%E6%A8%A1%E5%9E%8B/"}]},{"title":"萱仔大模型学习记录1-了解常用大模型","slug":"萱仔大模型学习记录1-了解常用大模型","date":"2024-09-04T06:00:00.000Z","updated":"2024-09-08T15:12:03.534Z","comments":true,"path":"2024/09/04/萱仔大模型学习记录1-了解常用大模型/","permalink":"http://example.com/2024/09/04/%E8%90%B1%E4%BB%94%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%951-%E4%BA%86%E8%A7%A3%E5%B8%B8%E7%94%A8%E5%A4%A7%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"由于前段时间拿到了一个某个nlp大模型offer，最近正在入职之前猛猛学习大模型的相关知识，避免入职之后露怯，学习了一下相关知识。我个人准备恶补的路线如下， 由于我本人已经掌握了pytorch和TensorFlow库，不做赘述，如果还有uu们想要学习一下pytorch，我我真的强烈推荐某粉色交友软件一个叫小甲鱼的阿婆主的视频，超级适合入门选手（真的无广，很适合学习使用） 以下则开始第一部分的总结，大模型的基础知识，相关方法，评价指标总结 ------------------------------------------------------------------------------------------------------------- 大语言模型基础知识和方法1. 大语言模型简介 大语言模型（LLM）常常具有数十亿到数千亿个参数，能够做nlp任务。常用的大语言模型包括 GPT、BERT 和 T5和 LLaMA 等。 1. OpenAI 的 GPT 系列 gpt是一种自回归语言模型，即通过预测给定上下文的下一个词来进行训练。gpt常常用于文本生成、对话系统、文本补全等。GPT是基于一种叫做Transformer的模型架构。这个架构由Vaswani等人在2017年提出，核心在于它的“注意力机制”，使得模型能够更好地理解和处理一段文字或句子。传统的模型在处理长文本时会遇到很多麻烦，但Transformer能够解决这些问题，并且计算速度更快。它首先在大量的文本数据上进行训练，不需要特定任务的标签。在这个阶段，GPT学会了理解文本中的语法、语义和上下文。这就像是让模型读了很多书，掌握了丰富的语言知识和使用模式。在预训练期间，它被训练为根据给定的上下文预测下一个词。这种自回归的训练方式使得模型能够理解并学到长期依赖关系，类似于写作时根据前面的句子继续写后面的内容。GPT模型通常由多个Transformer层堆叠而成。每一层包含多头自注意力机制和前馈神经网络。这些层次结构使得模型能够对输入进行更深层次的理解，从而更好地捕捉文本中的复杂语义和结构。为了让模型知道每个词在句子中的位置，GPT引入了位置嵌入。它帮助模型区分不同位置的词，从而正确理解句子的顺序。在预训练完成后，可以对GPT模型进行微调，以适应特定的任务，比如文本生成、问答、语言翻译等。微调时，使用有标签的数据来进一步调整模型的参数，让它在特定任务上表现更好。 12345678910111213141516import openai # 导入 OpenAI 库# 设置 API 密钥openai.api_key = &#x27;&#x27; # 这个密钥可以在官网获得，具体获得可以看我下面推荐的那个网址（无广# 使用 GPT-3 生成文本response = openai.Completion.create( engine=&quot;text-davinci-003&quot;, prompt=&quot;你好&quot;, max_tokens=100 # 生成的最大词数)# 输出生成的文本print(response.choices[0].text.strip()) 此处插播一个推荐获得openai api的方法（不是广告不是广告不是广告）： 如何获取 OpenAI API 密钥：分步指南 - 哔哩哔哩 2. Google 的 BERTBERT 是一种双向编码器表示模型，通过掩码语言建模（Masked Language Modeling, MLM）和下一句预测（Next Sentence Prediction, NSP）进行预训练。我们常常在文本分类、问答系统、命名实体识别等任务种用到BERT（后续补充BERT论文内容在下一章）。 4. Meta 的 LLaMALLaMA 是 Meta 开发的大规模语言模型，常用于文本生成、对话系统、文本补全等。 2. 大语言模型的训练方法 大语言模型的训练分为两个主要阶段：预训练和微调。 1）大模型的预训练 ：一些大公司使用较强的设备在大量无监督文本数据上进行训练，以捕捉语言的广泛模式和结构，使得产生的模型学习了语言的基本规律。 2）大模型微调 ：在自己需要的任务的数据集上进行后期的训练训练，以提高模型在该任务上的性能。例如自己的任务：情感分析、文本分类、问答等。 3. 预训练方法 常见的预训练方法包括自回归模型 GPT和自编码模型 BERT。 1）自回归模型 ：在训练过程中，模型通过预测给定上下文的下一个词来学习。 2）自编码模型 ：通过掩盖输入序列中的某些部分并要求模型预测这些掩码位置的词来进行训练。 4. 微调方法 微调方法通过在预训练模型的基础上，使用较小的数据集进行进一步训练，优化模型在特定任务上的表现。 任务特定微调 ：例如文本分类、命名实体识别、问答等。 提示词调优（Prompt Tuning） ：我们可以在输入数据前后添加提示，让想利用的模型更好地理解自己的任务。 参数高效微调（如 LoRA） ：通过减少参数调整的数量，提高微调效率。 混合优化（Hybrid Optimization） ：结合多种优化策略提升模型性能。 对抗训练（Adversarial Training） ：通过加入对抗样本，提高模型鲁棒性。 评价指标 评价语言模型性能的指标有多种，常见的包括精确率（Precision）、召回率（Recall）、F1 分数（F1 Score）、困惑度（Perplexity）、BLEU 分数等。 1. 精确率（Precision） 精确率是模型预测为正类的样本中，实际为正类的比例。 2. 召回率（Recall） 召回率是实际为正类的样本中，模型预测为正类的比例。 3. F1 分数（F1 Score） F1 分数是精确率和召回率的调和平均数，用于综合评价模型性能。 1234567891011121314151617181920import numpy as np # 导入 numpy 库from sklearn.metrics import precision_score, recall_score, f1_score # 从 sklearn.metrics 导入评价指标# 假设是我们已经预测完了，这个是预测完了之后的标签，和真实标签，我们使用sklearn库的函数来计算各种评价指标。true_labels = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0]) # 定义实际标签predicted_labels = np.array([1, 0, 1, 0, 0, 1, 1, 0, 1, 0]) # 定义模型预测标签# 精确率precision = precision_score(true_labels, predicted_labels) print(f&quot;Precision: &#123;precision&#125;&quot;) # 召回率recall = recall_score(true_labels, predicted_labels) print(f&quot;Recall: &#123;recall&#125;&quot;) # F1 分数f1 = f1_score(true_labels, predicted_labels) print(f&quot;F1 Score: &#123;f1&#125;&quot;) 4. 困惑度（Perplexity） 困惑度表示模型对给定数据集的预测能力。困惑度越低，模型性能越好。 123456789101112131415161718import torchimport torch.nn.functional as F# 假设有一个模型的输出标签和真实标签logits = torch.tensor([[0.2, 0.4, 0.1, 0.3], [0.1, 0.5, 0.2, 0.2]])labels = torch.tensor([1, 2])# 交叉熵损失loss = F.cross_entropy(logits, labels)# 困惑度是交叉熵损失计算而来的perplexity = torch.exp(loss).item()print(f&#x27;Cross Entropy Loss: &#123;loss.item()&#125;&#x27;)print(f&#x27;Perplexity: &#123;perplexity&#125;&#x27;)print(f&quot;Perplexity: &#123;perplexity.item()&#125;&quot;) 5. BLEU 分数（BLEU Score） BLEU 分数用于评价生成文本与参考文本的相似度。 12345678from nltk.translate.bleu_score import sentence_bleureference = [[&#x27;this&#x27;, &#x27;is&#x27;, &#x27;a&#x27;, &#x27;test&#x27;], [&#x27;this&#x27;, &#x27;is&#x27; &#x27;test&#x27;]]candidate = [&#x27;this&#x27;, &#x27;is&#x27;, &#x27;a&#x27;, &#x27;test&#x27;]score = sentence_bleu(reference, candidate)print(f&#x27;BLEU score: &#123;score&#125;&#x27;)","categories":[{"name":"大模型学习记录","slug":"大模型学习记录","permalink":"http://example.com/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"技术, 教程","slug":"技术-教程","permalink":"http://example.com/tags/%E6%8A%80%E6%9C%AF-%E6%95%99%E7%A8%8B/"}]},{"title":"萱仔大模型学习记录2.1-BERT算法论文和实践","slug":"萱仔大模型学习记录2.1-BERT算法论文和实践","date":"2024-09-04T06:00:00.000Z","updated":"2024-09-08T15:12:12.526Z","comments":true,"path":"2024/09/04/萱仔大模型学习记录2.1-BERT算法论文和实践/","permalink":"http://example.com/2024/09/04/%E8%90%B1%E4%BB%94%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%952.1-BERT%E7%AE%97%E6%B3%95%E8%AE%BA%E6%96%87%E5%92%8C%E5%AE%9E%E8%B7%B5/","excerpt":"","text":"第二章的内容承接上一章的内容，主要对nlp的模型bert论文进行学习和分析，虽然以前的项目中已经使用过bert进行新闻文本分类，但我个人认为我自己还是对理论和论文分析的不够到尾，需要对论文的理论进行仔细的学习，这样才能应对未来的魔改操作。这里我学习bert算法以及对应的transform结构 1.bert原论文—— BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 论文网址： https://arxiv.org/pdf/1810.04805 代码网址： https://github.com/google\\-research/bert（论文内代码网址） 2.transform原论文—— Attention Is All You Need 论文网址： https://arxiv.org/pdf/1706.03762 代码网址： --------------------------------------------------------------------------------------------------------------- BERT：1.摘要 我们介绍了一种新的语言表示模型，名为BERT，全称是双向编码器表示的Transformer。与最近的语言表示模型（如Peters等人，2018；Radford等人，2018）不同，BERT被设计为从无标签文本中预训练深度双向表示，它通过在所有层中同时考虑左右上下文来实现这一点。因此，预训练后的BERT模型只需在其基础上添加一个额外的输出层，就可以针对各种任务（如问答和语言推理）创建最先进的模型，而无需进行大幅度的任务特定架构修改。 BERT在概念上简单但在实验中却非常强大。它在十一项自然语言处理任务上获得了新的最先进结果，包括将GLUE基准得分提高到80.5%（绝对提升7.7个百分点）、MultiNLI准确率提高到86.7%（绝对提升4.6个百分点）、SQuAD v1.1问答测试的F1得分提高到93.2（绝对提升1.5个百分点）以及SQuAD v2.0测试的F1得分提高到83.1（绝对提升5.1个百分点）。 2.引言 在本文中，我们通过提出BERT（双向编码器表示的Transformer）改进了基于微调的方法。BERT通过使用“掩码语言模型”（MLM）预训练目标缓解了之前提到的单向性约束，这个目标受到了Cloze任务（Taylor，1953）的启发。掩码语言模型随机掩盖输入中的一些词，目标是仅基于其上下文预测被掩盖词的原始词汇ID。与从左到右的语言模型预训练不同，MLM目标使表示能够融合左右上下文，这使我们能够预训练一个深度双向的Transformer。除了掩码语言模型外，我们还使用了“下一个句子预测”任务，该任务共同预训练文本对的表示。 与训练和微调都差不太多，除了输出层以外。相同的预训练模型参数用于初始化不同下游任务的模型。在微调期间，所有参数都将进行微调。[CLS]是在每个输入示例前添加的特殊符号，而[SEP]是特殊的分隔符（例如，分隔问题&#x2F;答案）。 可以看出来，前面已经解释了重点的贡献，和两种预训练微调的方法 预训练（Pre-training） ： 输入数据 ：大量的未标记文本。 架构 ：Transformer架构，使用注意力机制来处理文本序列。 预训练 ： 掩码语言模型（Masked Language Model, MLM） ：随机掩盖输入文本中的一些单词，模型的任务是根据上下文预测被掩盖的单词。 下一句预测（Next Sentence Prediction, NSP） ：训练的模型需要判断两个句子是否是连续的。 输出 ：预训练好的模型参数。 微调（Fine-tuning） ： 输入数据 ：包含特定任务的标记数据（如问答数据、情感分析数据等）。 架构 ：与预训练时相同的Transformer架构。 微调过程 ： 初始化 ：使用预训练好的模型参数初始化模型。 训练 ：在特定任务数据上进行训练，调整所有模型参数。 输出 ：适应特定任务的模型。 12345678910111213141516171819202122232425262728293031from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArgumentsfrom datasets import load_datasettokenizer = BertTokenizer.from_pretrained(&#x27;bert-base-uncased&#x27;) #随便选一个已经与训练好的模型dataset = load_dataset(&#x27;glue&#x27;, &#x27;mrpc&#x27;)def preprocess_function(examples): return tokenizer(examples[&#x27;sentence1&#x27;], examples[&#x27;sentence2&#x27;], truncation=True, padding=&#x27;max_length&#x27;, max_length=128)encoded_dataset = dataset.map(preprocess_function, batched=True)model = BertForSequenceClassification.from_pretrained(&#x27;bert-base-uncased&#x27;)training_args = TrainingArguments( output_dir=&#x27;./results&#x27;, evaluation_strategy=&quot;epoch&quot;, per_device_train_batch_size=8, per_device_eval_batch_size=8, num_train_epochs=3, weight_decay=0.01, )trainer = Trainer( model=model, args=training_args, train_dataset=encoded_dataset[&#x27;train&#x27;], eval_dataset=encoded_dataset[&#x27;validation&#x27;], )trainer.train() 导入transformers库中的BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments模块，以及datasets库。 定义训练参数，包括输出目录、评估策略、每个设备的训练和评估批次大小、训练轮数和权重衰减。 BERT的模型架构是基于Vaswani等人（2017年）提出的原始实现和tensor2tensor库发布的多层双向Transformer编码器。bert几乎和原始版本相同，因此我们不再详细介绍模型架构的背景，建议读者参考Vaswani等人的论文和一些优秀的指南，比如“注释版Transformer”。 代码地址如下（论文中的）： https://github.com/tensorflow/tensor2tensor 其中层数用L表示，隐藏层大小用H表示，自注意力头的数量用A表示。论文里主要展示了两种模型大小的结果：BERTBASE（L=12, H=768, A=12，总参数=110M）和BERTLARGE（L=24, H=1024, A=16，总参数=340M）。选择BERTBASE是为了与OpenAI GPT在模型规模上进行对比。BERT Transformer使用的是双向自注意力，GPT Transformer使用的是单向自注意力，每个词元只能关注其左侧的上下文。 bert的输入表示方法，能清晰地表示单个句子和成对的句子（例如问题和答案）。在论文的研究中，“句子”可以是任意一段连续的文字，而不一定是一个完整的语言句子。“序列”指的是输入给BERT的词序列，可以是一个句子或两个句子连在一起。 论文里使用WordPiece嵌入方法，这种方法将单词拆分成更小的词片，使用一个包含30,000个词片的词汇表。每个序列的第一个词片是一个特殊的分类词片（[CLS]）。这个词片的最终隐藏状态用于分类任务中，作为整个序列的代表。两个句子被打包在一个序列中。我们用两种方式区分这两个句子：首先，用一个特殊的分隔词片（[SEP]）将它们分开；其次，为每个词片添加一个标识，表明它属于句子A还是句子B。如图1所示，我们将输入嵌入表示为E，特殊[CLS]词片的最终隐藏向量表示为C ∈ RH，i个输入词片的最终隐藏向量表示为Ti ∈ RH。 对于每个词片，它的输入表示通过将对应的词片、段和位置嵌入相加得到。这个过程的可视化见图2。 3.1 预训练 BERT（原文翻译）引言 不同于Peters等人（2018a）和Radford等人（2018），我们不使用传统的从左到右或从右到左的语言模型来预训练BERT。相反，我们使用两种无监督任务来预训练BERT。本节详细描述了这两种任务。这一步如图1的左半部分所示。 任务1：遮蔽语言模型 (Masked LM) 直观上来看，一个深度的双向模型要比单向模型（无论是从左到右还是从右到左）或左右双向模型的浅层拼接更加强大。然而，标准的条件语言模型只能从左到右或从右到左进行训练，因为双向条件会导致每个词可以间接地“看到自己”，这样模型可以在多层上下文中轻松地预测目标词。 为了训练一个深度双向表示，我们简单地随机遮蔽输入词元中的一部分，然后预测那些被遮蔽的词元。我们将这个过程称为“遮蔽语言模型”（Masked LM, MLM），尽管在文献中它通常被称为Cloze任务（Taylor, 1953）。在这种情况下，与被遮蔽词元对应的最终隐藏向量会被输入到词汇表上的输出softmax中，就像标准的语言模型一样。在我们的所有实验中，我们随机遮蔽每个序列中15%的WordPiece词元。不同于降噪自编码器（Vincent等，2008），我们只预测被遮蔽的词而不是重建整个输入。 虽然这让我们获得了一个双向的预训练模型，但有一个缺点是，这在预训练和微调之间产生了不匹配，因为[MASK]词元在微调过程中不会出现。为了缓解这一问题，我们并不总是用实际的[MASK]词元替换“被遮蔽”的词。训练数据生成器随机选择15%的词元位置进行预测。如果选择了第i个词元， 我们会有80%的时间将其替换为[MASK]词元，10%的时间替换为一个随机词元，10%的时间保持第i个词元不变。然后，使用Ti来通过交叉熵损失预测原始词元。我们在附录C.2中比较了这种程序的变体。 这里个人理解意思是 80％：用[MASK]词块替换单词：我是人——我是[MASK] 10％：用随机词替换遮蔽词：我是人——我是狗 10％：保持单词不变：我是人——我是人。#偏向于实际观察到的单词。 任务2：下一句预测 (Next Sentence Prediction, NSP) 许多重要的下游任务，如问答（QA）和自然语言推理（NLI），都基于理解两个句子之间的关系，而这种能力并不是直接通过语言建模捕捉到的。为了训练一个能够理解句子关系的模型，我们预训练了一个二元化的下一句预测任务，这个任务可以从任何单语语料库中轻松生成。具体来说，当为每个预训练样本选择句子A和B时，50%的时间B是实际紧随A之后的句子（标记为IsNext），50%的时间B是语料库中的一个随机句子（标记为NotNext）。如图1所示，我们使用C来进行下一句预测（NSP）。尽管这个任务很简单，但我们在第5.1节中展示了预训练这个任务对于QA和NLI任务是非常有益的。NSP任务与Jernite等人（2017）和Logeswaran与Lee（2018）使用的表示学习目标密切相关。然而，在之前的工作中，只有句子嵌入被转移到下游任务中，而在BERT中，所有参数都会被转移来初始化最终任务模型的参数。 很多重要的下游任务，例如问答(QA)和自然语言推理(NLI)，都是基于对两个文本句子间关系的理解，而这种关系并非通过 语言建模 直接获得。为了训练一个理解句子关系的模型，我们预训练了一个 二值化下一句预测任务 ，该任务可以从任何单语语料库中轻松生成。具体来说，选择句子A和B作为预训练样本：B有50%的可能是A的下一句，也有50%的可能是来自语料库的随机句子。例如： 50％： 输入=[CLS]我是[MASK][SEP]我喜欢吃[MASK][SEP] 其中MASK是“人”，“苹果”，这种情况，标记为IsNext 50％： 输入=[CLS]我是[MASK][SEP]白色的[MASK]出去玩##ing[SEP] 其中MASK是“人”，“小狗”，这种情况，标记为NotNext 预训练数据 预训练过程在很大程度上遵循了现有的语言模型预训练文献。我们使用BooksCorpus（800M词）和英文维基百科（2,500M词）作为预训练语料库。对于维基百科，我们只提取文本段落，忽略列表、表格和标题。使用文档级语料库而不是句子级语料库（如Billion Word Benchmark，Chelba等，2013）来提取长的连续序列是至关重要的。 3.2 微调 微调过程相对简单，因为Transformer中的自注意力机制允许BERT通过更换合适的输入和输出来处理多种下游任务，无论这些任务涉及单个文本还是文本对。对于涉及文本对的应用，一个常见的模式是先独立编码文本对，然后应用双向交叉注意力（例如Parikh等，2016；Seo等，2017）。而BERT则使用自注意力机制将这两个阶段统一起来，因为通过自注意力编码连接的文本对可以有效地在两个句子之间包含双向交叉注意力。 在微调BERT时，论文里将预训练好的模型应用到具体的任务上。BERT的自注意力机制让它能够处理多种任务，无论是单个文本还是文本对。对于文本对任务，传统的方法是先分别处理每个文本，再让它们相互注意；而BERT则直接将两个文本连接起来一起处理。 微调步骤 ： 任务输入和输出 ：根据具体任务，将输入和输出插入到BERT中。例如，在问答任务中，我们的输入是问题和相关段落，输出是问题的答案；在文本分类任务中，输入是文本，输出是分类结果。 模型微调 ：微调所有模型参数，使其适应具体任务。这个过程相对简单且快速。 然后就是实验部分 4.实验 （略过实验部分）","categories":[{"name":"大模型学习记录","slug":"大模型学习记录","permalink":"http://example.com/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"技术, 教程","slug":"技术-教程","permalink":"http://example.com/tags/%E6%8A%80%E6%9C%AF-%E6%95%99%E7%A8%8B/"}]},{"title":"萱仔大模型学习记录3.1-BERT算法微调理论和实践","slug":"萱仔大模型学习记录3.1-BERT算法微调理论和实践","date":"2024-09-04T06:00:00.000Z","updated":"2024-09-09T03:02:00.856Z","comments":true,"path":"2024/09/04/萱仔大模型学习记录3.1-BERT算法微调理论和实践/","permalink":"http://example.com/2024/09/04/%E8%90%B1%E4%BB%94%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%953.1-BERT%E7%AE%97%E6%B3%95%E5%BE%AE%E8%B0%83%E7%90%86%E8%AE%BA%E5%92%8C%E5%AE%9E%E8%B7%B5/","excerpt":"","text":"涉及bert的参数微调，还是得返回去学习原本transform的模块理论，不同的微调可以在不同的模块上进行改进： 2.transform原论文—— Attention Is All You Need 论文网址： https://arxiv.org/pdf/1706.03762 代码网址： Transformer模型是一种用于处理序列数据的神经网络架构，尤其在自然语言处理任务中表现优异。Transformer由多个编码器（Encoder）和解码器（Decoder）堆叠而成。以下是Transformer模型各个模块的作用： 1. 输入嵌入（Input Embedding） 将输入序列中的每个词转换为固定长度的向量表示。这一步通常使用词嵌入（如Word2Vec或GloVe）或直接训练的嵌入矩阵。 2. 位置编码（Positional Encoding） 由于Transformer模型没有循环结构（不像RNN），因此需要显式地注入序列的位置信息。位置编码向量被加到输入嵌入向量上，使得模型能够感知词的相对和绝对位置。位置编码通常使用正弦和余弦函数生成。 3. 多头自注意力机制（Multi-Head Self-Attention） 这是Transformer的核心模块之一，用于捕捉输入序列中不同词之间的依赖关系。通过多头注意力，模型能够并行计算多个不同的注意力表示，从而提高模型的表示能力。每个注意力头计算如下： 其中，Q,K,VQ, K, VQ,K,V 分别表示查询（Query）、键（Key）和值（Value）矩阵，dkd_kdk​ 是键向量的维度。 4. 前馈神经网络（Feed-Forward Neural Network） 每个注意力头的输出经过前馈神经网络，通常包括两个线性变换和一个ReLU激活函数： 前馈神经网络增加了模型的非线性表达能力。 5. 残差连接和层归一化（Residual Connection and Layer Normalization） 在每个子层（注意力机制和前馈神经网络）之后，都会有残差连接和层归一化。这有助于防止梯度消失和梯度爆炸，提高模型训练的稳定性和收敛速度。 6. 编码器（Encoder） 编码器由多个相同结构的层堆叠而成，每层包括一个多头自注意力机制和一个前馈神经网络。编码器的输入是带有位置编码的词嵌入，输出是输入序列的编码表示。 7. 解码器（Decoder） 解码器也由多个相同结构的层堆叠而成，但与编码器不同的是，每层解码器包含三个子层：一个多头自注意力机制、一个编码器-解码器注意力机制和一个前馈神经网络。编码器-解码器注意力机制允许解码器在生成输出序列时关注编码器的输出。 8. 输出层（Output Layer） 解码器的输出通过一个线性层和softmax层，生成目标序列中每个词的概率分布。该概率分布用于选择最终的输出词。 9. 掩码（Masking） 在自注意力机制中，为了防止模型在训练时看到未来的信息（尤其在解码器中），会使用掩码来屏蔽不相关的位置。掩码确保解码器只能关注已生成的词和当前词的位置。 Transformer模型通过并行处理序列数据，显著提高了计算效率和性能。多头自注意力机制和残差连接是其核心创新，使得Transformer在处理长序列数据和捕捉远距离依赖关系上表现出色。通过堆叠多个编码器和解码器层，Transformer能够学习复杂的语义和句法结构，从而在机器翻译、文本生成和问答系统等任务中取得优异的表现。 2.调优方法集锦——理论加代码demo 我前面完成了一个bert的实践，使用Bert算法在一个新闻文本分类的数据集上，这个数据集是天池官方的学习数据集，其中包含16类文本，且做了脱敏，可以用于实践。 然后调优的方法可以分为7种：以下为图片介绍（ 此图是我学习的时候偶然截图得到，不知道原版作者是谁，如果侵权请联系我，我马上删掉，谢谢谢谢） 1.LORA新增低秩矩阵到原权重矩阵 LoRA在大规模语言模型扮演着至关重要的角色，LoRA使用在微调一些复杂模型的参数。 原版论文： https://arxiv.org/pdf/2106.09685.pdf 原版代码：https://github.com/microsoft/LoRA 背景介绍 传统的神经网络微调方法通常需要调整所有层的权重，以适应新的任务。这种方法虽然有效，但需要大量计算资源和存储空间。研究表明，许多过参数化的模型实际上存在于一个低维空间中，模型在适应新任务时，权重变化的“内在秩”较低。这一发现启发了LoRA的提出。 LoRA的核心思想 LoRA的核心思想是通过优化低秩矩阵来间接训练神经网络中的某些密集层，从而在保持预训练权重不变的情况下，实现模型的微调。具体来说，LoRA通过引入两个低秩矩阵A和B来表示权重变化： LoRA是一种通过增加低秩矩阵来微调预训练模型的方法。其核心思想是通过增加较小的矩阵来表示权重更新，从而减少计算复杂度和存储需求。 假设预训练模型的权重矩阵是W。LoRA在训练过程中将其表示为两个低秩矩阵的乘积，即： 其中，A和B是较小的低秩矩阵（秩为r），这样可以减少训练过程中需要调整的参数数量。降低计算复杂度，减少存储需求，保留预训练模型的原始能力，这个我个人感觉可以类比到图像算法中的通道注意力机制上，类似看那个通道更重要，就把这个通道更加的计算（个人理解不一定对） LoRA在模型微调中具有多个显著优势： 存储和计算效率 ： 使用低秩矩阵A和B来表示权重变化，可以显著减少需要训练的参数数量。例如，在GPT-3 175B模型中，尽管全秩（full rank）为12,288，但使用低秩（r）为1或2的矩阵就足够了。这使得LoRA在存储和计算方面都非常高效。 模块化和灵活性 ： 预训练模型可以被共享，并用于构建多个小的LoRA模块，以适应不同的任务。通过冻结共享模型并替换矩阵A和B，可以高效地切换任务，显著减少存储需求和任务切换开销。 降低训练成本 ： LoRA通过只优化注入的低秩矩阵无需计算大部分参数的梯度或维护优化器状态，使得训练更加高效。使用自适应优化器时，硬件需求可以降低至原来的三分之一。 无推理延迟 ： LoRA的简单线性设计允许在部署时将可训练矩阵与冻结权重合并，从而在推理时不会引入额外的延迟。这使得LoRA在推理阶段与完全微调的模型相比没有性能差异。 与其他方法的兼容性 ： LoRA可以与许多现有的方法（如前缀调优）结合使用，以进一步提高模型的性能和适应性。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100import torchfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, TrainerCallbackimport pandas as pdfrom sklearn.metrics import accuracy_score, precision_recall_fscore_supportfrom torch.utils.data import Datasetimport loralib as loraimport matplotlib.pyplot as pltimport os# 将数据转换为PyTorch的Dataset格式class XuanDataset(Dataset): def __init__(self, texts, labels, tokenizer, max_len): self.texts = texts self.labels = labels self.tokenizer = tokenizer self.max_len = max_len def __len__(self): return len(self.texts) def __getitem__(self, idx): text = self.texts[idx] label = self.labels[idx] encoding = self.tokenizer( text, add_special_tokens=True, max_length=self.max_len, return_token_type_ids=False, padding=&#x27;max_length&#x27;, truncation=True, return_attention_mask=True, return_tensors=&#x27;pt&#x27;, ) return &#123; &#x27;input_ids&#x27;: encoding[&#x27;input_ids&#x27;].flatten(), &#x27;attention_mask&#x27;: encoding[&#x27;attention_mask&#x27;].flatten(), &#x27;labels&#x27;: torch.tensor(label, dtype=torch.long) &#125;df_train = pd.read_csv(&#x27;train.csv&#x27;) df_test = pd.read_csv(&#x27;test.csv&#x27;) train_texts = df_train[&#x27;text&#x27;].tolist()train_labels = df_train[&#x27;label&#x27;].tolist()test_texts = df_test[&#x27;text&#x27;].tolist()test_labels = df_test[&#x27;label&#x27;].tolist()tokenizer = BertTokenizer.from_pretrained(&#x27;bert-base-uncased&#x27;)train_dataset = XuanDataset( texts=train_texts, labels=train_labels, tokenizer=tokenizer, max_len=128)test_dataset = XuanDataset( texts=test_texts, labels=test_labels, tokenizer=tokenizer, max_len=128)model = BertForSequenceClassification.from_pretrained(&#x27;bert-base-uncased&#x27;, num_labels=16) # 加载预训练的BERT模型，并设置分类任务的类别数为16lora.lora(model, r=4) # 应用LoRA方法，将低秩矩阵的秩设置为4class MetricsCallback(TrainerCallback): def __init__(self): self.metrics = &#123;&#x27;epoch&#x27;: [], &#x27;loss&#x27;: [], &#x27;accuracy&#x27;: [], &#x27;precision&#x27;: [], &#x27;recall&#x27;: [], &#x27;f1&#x27;: []&#125; def on_epoch_end(self, args, state, control, **kwargs): metrics = kwargs[&#x27;metrics&#x27;] self.metrics[&#x27;epoch&#x27;].append(state.epoch) self.metrics[&#x27;loss&#x27;].append(metrics[&#x27;eval_loss&#x27;]) self.metrics[&#x27;accuracy&#x27;].append(metrics[&#x27;eval_accuracy&#x27;]) self.metrics[&#x27;precision&#x27;].append(metrics[&#x27;eval_precision&#x27;]) self.metrics[&#x27;recall&#x27;].append(metrics[&#x27;eval_recall&#x27;]) self.metrics[&#x27;f1&#x27;].append(metrics[&#x27;eval_f1&#x27;]) # 保存模型权重 model.save_pretrained(f&#x27;./model_weights/epoch_&#123;int(state.epoch)&#125;&#x27;)def compute_metrics(pred): labels = pred.label_ids preds = pred.predictions.argmax(-1) precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=&#x27;weighted&#x27;) acc = accuracy_score(labels, preds) return &#123; &#x27;accuracy&#x27;: acc, &#x27;precision&#x27;: precision, &#x27;recall&#x27;: recall, &#x27;f1&#x27;: f1, &#125;training_args = TrainingArguments( output_dir=&#x27;./results_lora&#x27;, evaluation_strategy) 2.QLORA量化权重到4bit+LORA 原版论文： 论文提出了QLoRA，一种高效的微调方法，该方法通过显著降低内存使用，使得在单个48GB GPU上微调包含650亿参数的模型成为可能，同时保持了全16位微调任务性能。QLoRA通过在一个冻结的、4位量化的预训练语言模型中反向传播梯度到低秩适配器（LoRA）来实现。最佳模型系列，命名为Guanaco，在Vicuna基准测试中超越了所有之前公开发布的模型，达到了ChatGPT性能的99.3%，而仅需在单个GPU上微调24小时。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128import torchfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, TrainerCallbackfrom torch.utils.data import Dataset, DataLoader, random_splitimport pandas as pdfrom sklearn.metrics import accuracy_score, precision_recall_fscore_supportimport matplotlib.pyplot as pltimport osimport loralib as lorafrom quantization import quantize_model class CustomDataset(Dataset): def __init__(self, texts, labels, tokenizer, max_len): self.texts = texts self.labels = labels self.tokenizer = tokenizer self.max_len = max_len def __len__(self): return len(self.texts) def __getitem__(self, idx): text = self.texts[idx] label = self.labels[idx] encoding = self.tokenizer( text, add_special_tokens=True, max_length=self.max_len, return_token_type_ids=False, padding=&#x27;max_length&#x27;, truncation=True, return_attention_mask=True, return_tensors=&#x27;pt&#x27;, ) return &#123; &#x27;input_ids&#x27;: encoding[&#x27;input_ids&#x27;].flatten(), &#x27;attention_mask&#x27;: encoding[&#x27;attention_mask&#x27;].flatten(), &#x27;labels&#x27;: torch.tensor(label, dtype=torch.long) &#125;df = pd.read_csv(&#x27;train.csv&#x27;) texts = df[&#x27;text&#x27;].tolist()labels = df[&#x27;label&#x27;].tolist()tokenizer = BertTokenizer.from_pretrained(&#x27;bert-base-uncased&#x27;)dataset = CustomDataset( texts=texts, labels=labels, tokenizer=tokenizer, max_len=128)train_size = int(0.8 * len(dataset))val_size = len(dataset) - train_sizetrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])model = BertForSequenceClassification.from_pretrained(&#x27;bert-base-uncased&#x27;, num_labels=16) # 加载预训练的BERT模型，并设置分类任务的类别数为16lora.lora(model, r=4) # 应用LoRA方法，将低秩矩阵的秩设置为4quantize_model(model) # 对模型进行量化处理class MetricsCallback(TrainerCallback): def __init__(self): self.metrics = &#123;&#x27;epoch&#x27;: [], &#x27;loss&#x27;: [], &#x27;accuracy&#x27;: [], &#x27;precision&#x27;: [], &#x27;recall&#x27;: [], &#x27;f1&#x27;: []&#125; def on_epoch_end(self, args, state, control, **kwargs): metrics = kwargs[&#x27;metrics&#x27;] self.metrics[&#x27;epoch&#x27;].append(state.epoch) self.metrics[&#x27;loss&#x27;].append(metrics[&#x27;eval_loss&#x27;]) self.metrics[&#x27;accuracy&#x27;].append(metrics[&#x27;eval_accuracy&#x27;]) self.metrics[&#x27;precision&#x27;].append(metrics[&#x27;eval_precision&#x27;]) self.metrics[&#x27;recall&#x27;].append(metrics[&#x27;eval_recall&#x27;]) self.metrics[&#x27;f1&#x27;].append(metrics[&#x27;eval_f1&#x27;]) # 保存模型权重 model.save_pretrained(f&#x27;./model_weights/epoch_&#123;int(state.epoch)&#125;&#x27;)def compute_metrics(pred): labels = pred.label_ids preds = pred.predictions.argmax(-1) precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=&#x27;weighted&#x27;) acc = accuracy_score(labels, preds) return &#123; &#x27;accuracy&#x27;: acc, &#x27;precision&#x27;: precision, &#x27;recall&#x27;: recall, &#x27;f1&#x27;: f1, &#125;training_args = TrainingArguments( output_dir=&#x27;./results_qlora&#x27;, evaluation_strategy=&#x27;epoch&#x27;, learning_rate=2e-5, per_device_train_batch_size=8, per_device_eval_batch_size=8, num_train_epochs=3, weight_decay=0.01, )metrics_callback = MetricsCallback()trainer = Trainer( model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset, compute_metrics=compute_metrics, callbacks=[metrics_callback] )trainer.train()plt.figure(figsize=(12, 8))#huatuplt.plot(metrics_callback.metrics[&#x27;epoch&#x27;], metrics_callback.metrics[&#x27;loss&#x27;], label=&#x27;Loss&#x27;)plt.plot(metrics_callback.metrics[&#x27;epoch&#x27;], metrics_callback.metrics[&#x27;accuracy&#x27;], label=&#x27;Accuracy&#x27;)plt.plot(metrics_callback.metrics[&#x27;epoch&#x27;], metrics_callback.metrics[&#x27;precision&#x27;], label=&#x27;Precision&#x27;)plt.plot(metrics_callback.metrics[&#x27;epoch&#x27;], metrics_callback.metrics[&#x27;recall&#x27;], label=&#x27;Recall&#x27;)plt.plot(metrics_callback.metrics[&#x27;epoch&#x27;], metrics_callback.metrics[&#x27;f1&#x27;], label=&#x27;F1 Score&#x27;)plt.xlabel(&#x27;Epoch&#x27;)plt.ylabel(&#x27;Score&#x27;)plt.legend()plt.title(&#x27;Training Metrics Over Epochs&#x27;)plt.savefig(&#x27;training_metrics.png&#x27;)plt.show() -------------------------------------------------------------------------------剩下的下一章再更新3.Adapter Tuning 原版论文： https://arxiv.org/pdf/1902.00751.pdf 原版代码： GitHub - google-research&#x2F;adapter-bert 4.Prefix Tuning输入增加可训练的上下文前缀5.Prompt Tuning输入增加可训练的嵌入向量提示6.P-Tuning使用可训练的LSTM模型生成嵌入向量到输入7.P-Tuning V2P-Tuning+多个N中输入","categories":[{"name":"大模型学习记录","slug":"大模型学习记录","permalink":"http://example.com/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"技术, 教程","slug":"技术-教程","permalink":"http://example.com/tags/%E6%8A%80%E6%9C%AF-%E6%95%99%E7%A8%8B/"}]},{"title":"萱仔大模型学习记录3.2-BERT算法微调理论和实践Adapter_Tuning、Prefix_Tuning等","slug":"萱仔大模型学习记录3.2-BERT算法微调理论和实践Adapter_Tuning、Prefix_Tuning等","date":"2024-09-04T06:00:00.000Z","updated":"2024-09-09T03:01:51.551Z","comments":true,"path":"2024/09/04/萱仔大模型学习记录3.2-BERT算法微调理论和实践Adapter_Tuning、Prefix_Tuning等/","permalink":"http://example.com/2024/09/04/%E8%90%B1%E4%BB%94%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%953.2-BERT%E7%AE%97%E6%B3%95%E5%BE%AE%E8%B0%83%E7%90%86%E8%AE%BA%E5%92%8C%E5%AE%9E%E8%B7%B5Adapter_Tuning%E3%80%81Prefix_Tuning%E7%AD%89/","excerpt":"","text":"3.Adapter Tuning 原版论文：https://arxiv.org/pdf/1902\\.00751\\.pdf 原版代码：GitHub - google-research&#x2F;adapter-bert Adapter Tuning 是一种在预训练模型的基础上进行高效微调的方法。通过插入adapter modules，只需增加少量可训练参数，就能使模型在新任务上取得优异的性能。（我个人感觉这个在使用上和lora有点类似，lora是引入了两个矩阵，在外面训练微调，较低成本实现训练任务。这个是加入一个适配器的模块。 Adapter Modules 是插入到现有预训练模型中的小型网络层。由少量可训练参数组成，通常包括一个下采样层、一个ReLU之类的非线性激活层，以及一个上采样层。（这一点又和计算机视觉中有一点点像，上采样和下采样各有作用，然后加上激活函数的非线性函数，加入到神经网络中可以让网格拟合非线性映射。 Adapter Modules 插入位置通常在Transformer层之间。） 在适配器微调过程中，预训练模型的参数保持不变。只训练适配器模块的参数，从而实现参数高效的迁移学习。相比于传统的微调，适配器微调只需增加很少的新参数，但能够取得接近或甚至超越全量微调的效果。适配器模块可以独立于任务进行训练，适应多任务学习的需求。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788import torchfrom transformers import BertModel, BertTokenizer, BertForSequenceClassification, AdapterConfig# 加载预训练BERT模型和Tokenizermodel_name = &quot;bert-base-uncased&quot;tokenizer = BertTokenizer.from_pretrained(model_name)model = BertForSequenceClassification.from_pretrained(model_name)# 冻结BERT主体模型的所有参数for param in model.bert.parameters(): param.requires_grad = False# 添加适配器模块配置adapter_config = AdapterConfig( hidden_size=64, # 适配器模块的隐藏层大小 adapter_non_linearity=&quot;relu&quot;, # 非线性激活函数 reduction_factor=16 # 下采样比例)# 为分类任务添加适配器model.add_adapter(&quot;text_classification_adapter&quot;, config=adapter_config)model.train_adapter(&quot;text_classification_adapter&quot;)# 示例输入inputs = tokenizer(&quot;This is a test sentence.&quot;, return_tensors=&quot;pt&quot;)# 模型预测前向传播outputs = model(**inputs)# 打印模型输出print(outputs.logits)# 准备训练数据（以GLUE任务中的MRPC数据集为例）from datasets import load_datasetdataset = load_dataset(&quot;glue&quot;, &quot;mrpc&quot;)train_dataset = dataset[&#x27;train&#x27;]validation_dataset = dataset[&#x27;validation&#x27;]# 数据处理函数def preprocess_function(examples): return tokenizer(examples[&#x27;sentence1&#x27;], examples[&#x27;sentence2&#x27;], truncation=True, padding=True)# 预处理数据train_dataset = train_dataset.map(preprocess_function, batched=True)validation_dataset = validation_dataset.map(preprocess_function, batched=True)# 数据加载器from torch.utils.data import DataLoadertrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)validation_loader = DataLoader(validation_dataset, batch_size=16)# 定义优化器和损失函数from transformers import AdamWoptimizer = AdamW(model.parameters(), lr=5e-5)loss_fn = torch.nn.CrossEntropyLoss()# 训练适配器model.train()for epoch in range(3): # 训练3个epoch for batch in train_loader: inputs = &#123;k: v.to(model.device) for k, v in batch.items() if k in tokenizer.model_input_names&#125; labels = batch[&#x27;label&#x27;].to(model.device) optimizer.zero_grad() outputs = model(**inputs) loss = loss_fn(outputs.logits, labels) loss.backward() optimizer.step() print(f&quot;Epoch &#123;epoch + 1&#125;: Loss &#123;loss.item()&#125;&quot;)# 验证模型model.eval()correct_predictions = 0total_predictions = 0with torch.no_grad(): for batch in validation_loader: inputs = &#123;k: v.to(model.device) for k, v in batch.items() if k in tokenizer.model_input_names&#125; labels = batch[&#x27;label&#x27;].to(model.device) outputs = model(**inputs) predictions = torch.argmax(outputs.logits, dim=-1) correct_predictions += (predictions == labels).sum().item() total_predictions += labels.size(0)accuracy = correct_predictions / total_predictionsprint(f&quot;Validation Accuracy: &#123;accuracy&#125;&quot;)# 保存适配器model.save_adapter(&quot;text_classification_adapter&quot;, &quot;output_adapter_directory&quot;) 4.Prefix Tuning输入增加可训练的上下文前缀 原版论文： https://arxiv.org/pdf/2101.00190.pdf 原版代码： Fine-tuning是利用大型预训练语言模型来执行下游任务的默认方式。然而，微调会修改所有语言模型的参数，因此需要为每个任务存储一份完整的副本。本文提出了一种轻量级的微调替代方法——前缀调优prefix-tuning，专用于自然语言生成任务。前缀调优保持语言模型参数不变，仅优化一个小的任务特定的连续向量（称为前缀）。前缀调优受提示（prompting）的启发，允许后续的token关注这个前缀，仿佛它们是“虚拟token”。这篇论文将前缀调优应用于GPT-2进行表格到文本生成，以及BART进行摘要生成。结果表明，通过仅学习0.1%的参数，前缀调优在完整数据集环境中获得了与全量微调相当的性能，在低数据环境中表现优于微调，并且在训练时未见过主题的例子上具有更好的外推能力。 Prefix Tuning 是一种在预训练语言模型（如 BERT 或 GPT-2）基础上进行高效微调的方法。与传统的微调方式不同，Prefix Tuning 只调整少量的前缀参数，而不修改原始模型的参数，从而在多个任务之间实现高效迁移。 前缀参数（Prefix Parameters） ： * 前缀参数是在每层 Transformer 网络的输入前增加的一组可训练参数。 * 这些参数在模型进行任务特定的微调时可以独立更新，而无需调整原始模型的参数。 冻结预训练模型 ： * 在 Prefix Tuning 过程中，预训练模型的参数保持不变。 * 只训练前缀参数，从而实现高效的参数更新和迁移学习。 高效微调 ： * 相比于传统的微调，Prefix Tuning 只需调整少量的前缀参数，但能够取得接近或甚至超越全量微调的效果。 * 前缀参数可以独立于任务进行训练，适应多任务学习的需求。 总结 传统微调的局限性 ： 修改所有语言模型参数，需要为每个任务存储一个完整的模型副本。 前缀调优的优势 ： 保持语言模型参数不变，仅优化前缀参数。 通过仅学习0.1%的参数，前缀调优在全数据集设置中获得了可比的性能，在低数据设置中表现优于微调，并在训练中未见的主题上表现更好。 应用场景 ： 前缀调优适用于自然语言生成任务，如表格到文本生成和摘要生成。 所有P-tuning都是基于优化连续的prefix或者prompt的思想。调整软提示优于先前对离散提示优化的工作。Prompt-tuning的工作表明全量微调和P*-tuning之间的性能差距随着模型的大小增大而消失。 5.Prompt Tuning输入增加可训练的嵌入向量提示","categories":[{"name":"大模型学习记录","slug":"大模型学习记录","permalink":"http://example.com/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"技术, 教程","slug":"技术-教程","permalink":"http://example.com/tags/%E6%8A%80%E6%9C%AF-%E6%95%99%E7%A8%8B/"}]},{"title":"萱仔大模型学习记录4-BERT_lora实战","slug":"萱仔大模型学习记录4-BERT_lora实战","date":"2024-09-04T06:00:00.000Z","updated":"2024-09-09T03:01:42.767Z","comments":true,"path":"2024/09/04/萱仔大模型学习记录4-BERT_lora实战/","permalink":"http://example.com/2024/09/04/%E8%90%B1%E4%BB%94%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%954-BERT_lora%E5%AE%9E%E6%88%98/","excerpt":"","text":"这两天把 bert+lorade 代码已经改顺了，成功将这个lora应用到那个天池官方的新闻文本分类项目中，训练的结果较好，例如F1能达到0.90以上。（我第一个尝试做lora实践的原因还是我比较缺少资源，虽然有一些薅羊毛来的资源，但还是远远不够的，希望我入职之后能够快乐用卡嘿嘿嘿） 话又说回来，Lora的优点在高效的模型微调和参数量减少上。由于在大模型中，微调所有参数的成本非常高，特别是在内存和计算资源方面。Lora通过添加低秩的矩阵，使得只需要微调少量参数，而不是整个模型。由于Lora只引入了一些低秩的矩阵进行训练，相较于微调整个模型，它大大减少了训练时间和计算资源。 Lora可以应用于各种不同类型的模型，包括Transformer模型、卷积神经网络等，具有很高的适应性。Lora在保留预训练模型的原始参数的同时，只对特定任务引入改动，有助于保留模型的泛化能力，从而避免过拟合。 Lora方法允许在微调新任务时保留预训练模型的原始参数不变，这意味着我们可以利用预训练模型的知识，而不需要从头开始训练。这样就可以用比较小的代价去处理我自己的任务，虽然bert本身不算一个特别巨大的模型，但是作为实践，还是可以尝试吧lora引入进去当作一个实践，从简单到困难需要循序渐进嘛。 由于我已经理顺了一次lora的原理，对于个人简单来说就是外接一个新的A和B矩阵，来训练新的小小矩阵去×原本的大大模型，这样只训练小矩阵的成本就能大大降低，而且由于训练关注的也是更加值得关注的部分，模型的精度常常不降反升，用在自己的任务上非常的方便。 接下来是代码部分： --------------------------------------------------------------------------------------------------------------- 第一种代码（通用demo，方便后续自己修改的小小demo）： 定义低秩矩阵 A 和 B。 替换原始的全连接层权重矩阵 W。 仅微调 A 和 B 矩阵。 123456789101112131415161718192021222324252627282930313233343536373839404142434445import torchimport torch.nn as nnfrom transformers import BertModel, BertTokenizerclass BertWithLoRA(nn.Module): def __init__(self, num_classes, r=8): # r为低秩矩阵的秩 super(BertWithLoRA, self).__init__() self.bert = BertModel.from_pretrained(&#x27;bert-base-uncased&#x27;) self.num_classes = num_classes hidden_size = self.bert.config.hidden_size # 定义低秩矩阵 self.A = nn.Parameter(torch.randn(hidden_size, r)) self.B = nn.Parameter(torch.randn(r, hidden_size)) self.classifier = nn.Linear(hidden_size, num_classes) def forward(self, input_ids, attention_mask, token_type_ids): outputs = self.bert(input_ids, attention_mask, token_type_ids) pooled_output = outputs[1] # 应用低秩矩阵变换 low_rank_output = torch.matmul(pooled_output, self.A) low_rank_output = torch.matmul(low_rank_output, self.B) logits = self.classifier(low_rank_output) return logits# 加载模型和分词器tokenizer = BertTokenizer.from_pretrained(&#x27;bert-base-uncased&#x27;)model = BertWithLoRA(num_classes=2) # 假设我需要完成的是二分类任务optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)criterion = nn.CrossEntropyLoss()inputs = tokenizer(&quot;Example sentence&quot;, return_tensors=&#x27;pt&#x27;)labels = torch.tensor([1]) # 示例标签model.train()optimizer.zero_grad()outputs = model(**inputs)loss = criterion(outputs, labels)loss.backward()optimizer.step() 第二个代码（我自己完成前面章节提到的新闻文本分类任务的代码，供大家参考和自己学习记录使用）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122import torchfrom torch.utils.data import DataLoaderfrom transformers import AdamW, BertTokenizerimport pandas as pdimport osfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import precision_recall_fscore_support, accuracy_scoreimport openpyxlfrom openpyxl import Workbookdef evaluate_model(model, dataloader, device): model.eval() preds = [] true_labels = [] for batch in dataloader: batch = tuple(t.to(device) for t in batch) token_ids, attn_masks, token_type_ids, labels = batch with torch.no_grad(): logits = model(token_ids, attn_masks, token_type_ids) preds.extend(torch.argmax(logits, axis=1).cpu().numpy()) true_labels.extend(labels.cpu().numpy()) precision, recall, f1, _ = precision_recall_fscore_support(true_labels, preds, average=&#x27;weighted&#x27;) acc = accuracy_score(true_labels, preds) #记录一下我的验证的结果 return &#123; &#x27;accuracy&#x27;: acc, &#x27;precision&#x27;: precision, &#x27;recall&#x27;: recall, &#x27;f1&#x27;: f1, &#x27;preds&#x27;: preds &#125;# 保存模型和分词器def save_model_and_tokenizer(model, tokenizer, dir_path): os.makedirs(dir_path, exist_ok=True) torch.save(model.state_dict(), os.path.join(dir_path, &#x27;model.pth&#x27;)) tokenizer.save_pretrained(dir_path)# 保存指标和模型的函数def save_metrics(epoch, avg_train_loss, metrics, best_f1, model, val_dataloader, device, tokenizer): # 每隔五轮就保存一次训练的模型 if epoch % 5 == 0: save_model_and_tokenizer(model, tokenizer, f&#x27;./model_weights/epoch_&#123;epoch&#125;&#x27;) if metrics[&#x27;f1&#x27;] &gt; best_f1: #这里保存了我训练完的模型，最好的那个模型 best_f1 = metrics[&#x27;f1&#x27;] save_model_and_tokenizer(model, tokenizer, &#x27;./model_weights/best_model&#x27;) val_metrics = evaluate_model(model, val_dataloader, device) predictions = val_metrics[&#x27;preds&#x27;] val_results = pd.DataFrame(&#123;&#x27;text&#x27;: val_texts, &#x27;label&#x27;: val_labels, &#x27;prediction&#x27;: predictions&#125;) val_results.to_csv(f&#x27;./val_results/epoch_&#123;epoch&#125;.csv&#x27;, index=False) # 更新指标 print(f&quot;Epoch: &#123;epoch&#125;, Loss: &#123;avg_train_loss:.4f&#125;, Accuracy: &#123;metrics[&#x27;accuracy&#x27;]:.4f&#125;, Precision: &#123;metrics[&#x27;precision&#x27;]:.4f&#125;, Recall: &#123;metrics[&#x27;recall&#x27;]:.4f&#125;, F1: &#123;metrics[&#x27;f1&#x27;]:.4f&#125;&quot;)df = pd.read_csv(&#x27;./data_news/train_set.csv&#x27;, sep=&#x27;\\t&#x27;)train_texts, val_texts, train_labels, val_labels = train_test_split(df[&#x27;text&#x27;].tolist(), df[&#x27;label&#x27;].astype(int).tolist(), test_size=0.2)train_dataset = XuanDataset(train_texts, train_labels, max_length=128)val_dataset = XuanDataset(val_texts, val_labels, max_length=128)train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)val_dataloader = DataLoader(val_dataset, batch_size=8)model = BertWithLoRA(num_classes=16)device = &#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;model.to(device)tokenizer = BertTokenizer.from_pretrained(&#x27;bert-base-uncased&#x27;)os.makedirs(&#x27;./model_weights&#x27;, exist_ok=True)os.makedirs(&#x27;./val_results&#x27;, exist_ok=True)excel_file = &#x27;metrics1.xlsx&#x27;if not os.path.exists(excel_file): wb = Workbook() ws = wb.active ws.title = &#x27;Metrics&#x27; ws.append([&#x27;Epoch&#x27;, &#x27;Loss&#x27;, &#x27;Accuracy&#x27;, &#x27;Precision&#x27;, &#x27;Recall&#x27;, &#x27;F1&#x27;]) wb.save(excel_file)optimizer = AdamW(model.parameters(), lr=2e-5)epochs = 100 # 训练轮数best_f1 = 0.0for epoch in range(1, epochs + 1): model.train() total_loss = 0 for batch in train_dataloader: batch = tuple(t.to(device) for t in batch) token_ids, attn_masks, token_type_ids, labels = batch optimizer.zero_grad() logits = model(token_ids, attn_masks, token_type_ids) loss = nn.CrossEntropyLoss()(logits, labels) loss.backward() optimizer.step() total_loss += loss.item() avg_train_loss = total_loss / len(train_dataloader) # 验证模型 metrics = evaluate_model(model, val_dataloader, device) wb = openpyxl.load_workbook(excel_file) ws = wb[&#x27;Metrics&#x27;] ws.append([epoch, avg_train_loss, metrics[&#x27;accuracy&#x27;], metrics[&#x27;precision&#x27;], metrics[&#x27;recall&#x27;], metrics[&#x27;f1&#x27;]]) wb.save(excel_file) # 保存模型和打印评价指标 save_metrics(epoch, avg_train_loss, metrics, best_f1, model, val_dataloader, device, tokenizer) 结果数据就如下啦：","categories":[{"name":"大模型学习记录","slug":"大模型学习记录","permalink":"http://example.com/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"技术, 教程","slug":"技术-教程","permalink":"http://example.com/tags/%E6%8A%80%E6%9C%AF-%E6%95%99%E7%A8%8B/"}]},{"title":"萱仔大模型学习记录5-langchain实战","slug":"萱仔大模型学习记录5-langchain实战","date":"2024-09-04T06:00:00.000Z","updated":"2024-09-09T03:01:34.488Z","comments":true,"path":"2024/09/04/萱仔大模型学习记录5-langchain实战/","permalink":"http://example.com/2024/09/04/%E8%90%B1%E4%BB%94%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%955-langchain%E5%AE%9E%E6%88%98/","excerpt":"","text":"前面我的bert+lora微调已经跑出了不错的结果，我也学会了如何在bert上使用Lora进行微调，我后续会补充一个医疗意图识别的项目于这个系列，现在这个医疗意图识别代码还暂时不准备公开。我就继续按照我的计划学习一番LangChain。 LangChain是一个用于构建语言模型应用程序的框架。 LangChain提供了多个模块来简化开发和集成语言模型的过程。以下是 LangChain 的七个主要模块及其详细介绍： 1. LLMs (Language Models) 功能 : 这个模块处理与各种语言模型的交互。你可以使用它来加载预训练的语言模型，或者自定义和训练自己的模型。 核心组件 : LLM : 用于封装语言模型的基类。 OpenAI : 对接 OpenAI 的模型（如 GPT-3, GPT-4）。 2. Prompts 功能 : 用于生成和管理提示（prompts），以便有效地与语言模型进行交互。提供了构建和优化提示的工具。 核心组件 : PromptTemplate : 用于定义和格式化提示模板。 PromptChain : 用于管理多个提示的链式处理。 3. Agents 功能 : 允许创建智能代理（agents），这些代理可以处理复杂的任务和对话，并决定如何调用不同的工具或模型。 核心组件 : Agent : 处理任务和对话的智能代理。 AgentExecutor : 执行代理任务的类。 4. Chains 功能 : 用于将多个组件（如提示、模型、数据）连接在一起，以实现更复杂的工作流和数据处理管道。 核心组件 : Chain : 基类，用于创建链式工作流。 SequentialChain : 按顺序执行多个步骤的链。 5. Memory 功能 : 用于管理和存储会话中的信息，以便在对话中保持上下文和状态。 核心组件 : Memory : 用于存储和检索会话信息。 ConversationBufferMemory : 专门用于对话的缓冲记忆。 6. Tools 功能 : 提供了一些工具和功能，用于扩展语言模型的能力，如数据检索、处理和转换。 核心组件 : Tool : 基类，用于定义各种工具。 APITool : 对接外部API的工具。 7. Callbacks 功能 : 提供了回调功能，用于在模型运行过程中进行日志记录、监控和其他自定义操作。 核心组件 : CallbackHandler : 用于处理和管理回调操作。 Logger : 记录模型运行的日志信息。 LangChain的demo使用方法： 123456789101112131415161718192021222324252627from langchain_openai import ChatOpenAIfrom langchain_core.messages import HumanMessage, SystemMessage&quot;&quot;&quot;api_key = &quot;&quot;api_base = &quot;&quot;model_name = &quot;&quot;&quot;&quot;&quot;chat = ChatOpenAI( model=&quot;yi-spark&quot;, temperature=0.3, max_tokens=200, api_key=&#x27;&#x27;, base_url=&quot;&quot;)messages = [ SystemMessage(content=&quot;你是一名精通 python 的专家&quot;), HumanMessage(content=&quot;写一个 python 的 hello world 程序&quot;),]response = chat.invoke(messages)print(response.content) 这是运行的结果：","categories":[{"name":"大模型学习记录","slug":"大模型学习记录","permalink":"http://example.com/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"技术, 教程","slug":"技术-教程","permalink":"http://example.com/tags/%E6%8A%80%E6%9C%AF-%E6%95%99%E7%A8%8B/"}]},{"title":"anaconda环境配置，以及python常用库下载语句（包含镜像）","slug":"anaconda环境配置，以及python常用库下载语句（包含镜像）","date":"2024-09-02T12:00:00.000Z","updated":"2024-09-03T09:28:46.184Z","comments":true,"path":"2024/09/02/anaconda环境配置，以及python常用库下载语句（包含镜像）/","permalink":"http://example.com/2024/09/02/anaconda%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%EF%BC%8C%E4%BB%A5%E5%8F%8Apython%E5%B8%B8%E7%94%A8%E5%BA%93%E4%B8%8B%E8%BD%BD%E8%AF%AD%E5%8F%A5%EF%BC%88%E5%8C%85%E5%90%AB%E9%95%9C%E5%83%8F%EF%BC%89/","excerpt":"前言Anaconda 是一个非常流行的 Python 和 R 语言的集成开发环境，特别适合数据科学、机器学习和人工智能领域的开发工作。本文将介绍一些在 Anaconda 环境中常用的 Python 库，并详细说明这些库的安装与引入方法。","text":"前言Anaconda 是一个非常流行的 Python 和 R 语言的集成开发环境，特别适合数据科学、机器学习和人工智能领域的开发工作。本文将介绍一些在 Anaconda 环境中常用的 Python 库，并详细说明这些库的安装与引入方法。 1. 常用 Python 库介绍在使用 Anaconda 时，有一些 Python 库非常常用，几乎每个数据科学家、机器学习工程师都会用到。以下是一些常用库的简单介绍： 1.1 NumPyNumPy 是一个强大的数学库，提供了多维数组对象以及用于操作数组的函数。它是许多数据科学和机器学习库（如 pandas 和 scikit-learn）的基础。 示例NumPy 数组相关操作123456789101112131415161718192021222324252627282930313233import numpy as np# 创建一维数组array_1d = np.array([1, 2, 3, 4])print(&quot;一维数组:&quot;, array_1d)# 创建二维数组array_2d = np.array([[1, 2], [3, 4]])print(&quot;二维数组:\\n&quot;, array_2d)# 数组相加array_sum = array_1d + array_1dprint(&quot;数组相加:&quot;, array_sum)# 数组的广播机制array_broadcast = array_1d * 2print(&quot;数组广播:&quot;, array_broadcast)# 获取数组的第二个元素print(&quot;第二个元素:&quot;, array_1d[1])# 获取二维数组的第一列print(&quot;第一列:&quot;, array_2d[:, 0])# 生成全零数组zeros_array = np.zeros((3, 3))print(&quot;全零数组:\\n&quot;, zeros_array)# 生成单位矩阵identity_matrix = np.eye(3)print(&quot;单位矩阵:\\n&quot;, identity_matrix) 1.2 PandasPandas 是一个数据分析库，提供了易于使用的数据结构和数据分析工具。Pandas 主要用于操作和分析结构化数据（如表格数据）。 1234567891011121314151617181920212223242526272829303132import pandas as pd# 创建字典data = &#123;&#x27;Name&#x27;: [&#x27;Tom&#x27;, &#x27;Jerry&#x27;, &#x27;Spike&#x27;], &#x27;Age&#x27;: [20, 21, 19], &#x27;Grade&#x27;: [&#x27;A&#x27;, &#x27;B&#x27;, &#x27;A&#x27;]&#125;# 从字典创建 DataFramedf = pd.DataFrame(data)print(&quot;DataFrame:\\n&quot;, df)# 筛选年龄大于20的行filtered_df = df[df[&#x27;Age&#x27;] &gt; 20]print(&quot;筛选结果:\\n&quot;, filtered_df)# 添加新列df[&#x27;Passed&#x27;] = df[&#x27;Grade&#x27;] == &#x27;A&#x27;print(&quot;添加新列后的 DataFrame:\\n&quot;, df)# 按照 Grade 分组并统计各组的平均年龄grouped_df = df.groupby(&#x27;Grade&#x27;)[&#x27;Age&#x27;].mean()print(&quot;按 Grade 分组后的平均年龄:\\n&quot;, grouped_df)# 创建包含缺失值的 DataFramedata_with_nan = &#123;&#x27;Name&#x27;: [&#x27;Tom&#x27;, &#x27;Jerry&#x27;, None], &#x27;Age&#x27;: [20, None, 19]&#125;df_with_nan = pd.DataFrame(data_with_nan)# 填充缺失值df_filled = df_with_nan.fillna(&#x27;Unknown&#x27;)print(&quot;填充缺失值后的 DataFrame:\\n&quot;, df_filled) 1.3 MatplotlibMatplotlib 是一个用于创建静态、动态和交互式可视化图表的绘图库。它与 NumPy 配合良好，可用于生成各种类型的图表，如折线图、柱状图、散点图等。自用库 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import matplotlib.pyplot as plt# 数据x = [1, 2, 3, 4, 5]y = [1, 4, 9, 16, 25]# 绘制折线图plt.plot(x, y, marker=&#x27;o&#x27;)plt.title(&#x27;折线图示例&#x27;)plt.xlabel(&#x27;X 轴&#x27;)plt.ylabel(&#x27;Y 轴&#x27;)plt.show()# 数据categories = [&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;]values = [5, 7, 3]# 绘制柱状图plt.bar(categories, values, color=&#x27;green&#x27;)plt.title(&#x27;柱状图示例&#x27;)plt.xlabel(&#x27;类别&#x27;)plt.ylabel(&#x27;值&#x27;)plt.show()# 数据x = [1, 2, 3, 4, 5]y = [2, 3, 5, 7, 11]# 绘制散点图plt.scatter(x, y, color=&#x27;red&#x27;)plt.title(&#x27;散点图示例&#x27;)plt.xlabel(&#x27;X 轴&#x27;)plt.ylabel(&#x27;Y 轴&#x27;)plt.show()# 数据data = [1, 2, 2, 3, 3, 3, 4, 4, 5, 5, 5, 5, 6, 7]# 绘制直方图plt.hist(data, bins=7, color=&#x27;blue&#x27;, edgecolor=&#x27;black&#x27;)plt.title(&#x27;直方图示例&#x27;)plt.xlabel(&#x27;值&#x27;)plt.ylabel(&#x27;频率&#x27;)plt.show()# 数据x = [1, 2, 3, 4, 5]y1 = [1, 4, 9, 16, 25]y2 = [2, 3, 5, 7, 11]# 创建两个子图fig, axs = plt.subplots(2)# 第一个子图axs[0].plot(x, y1, marker=&#x27;o&#x27;)axs[0].set_title(&#x27;子图 1&#x27;)# 第二个子图axs[1].scatter(x, y2, color=&#x27;red&#x27;)axs[1].set_title(&#x27;子图 2&#x27;)# 调整布局plt.tight_layout()plt.show() 1.4 Scikit-learnScikit-learn 是一个机器学习库，提供了简单而高效的数据挖掘和数据分析工具，支持各种分类、回归、聚类算法。它建立在 NumPy、SciPy 和 Matplotlib 之上。 123456789101112131415161718192021222324252627282930313233343536from sklearn.datasets import load_iris# 加载 Iris 数据集iris = load_iris()print(&quot;特征名称:&quot;, iris.feature_names)print(&quot;标签名称:&quot;, iris.target_names)from sklearn.model_selection import train_test_split# 数据集划分为训练集和测试集X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)print(&quot;训练集大小:&quot;, X_train.shape)print(&quot;测试集大小:&quot;, X_test.shape)from sklearn.neighbors import KNeighborsClassifier# 创建 KNN 分类器并训练knn = KNeighborsClassifier(n_neighbors=3)knn.fit(X_train, y_train)# 预测predictions = knn.predict(X_test)print(&quot;预测结果:&quot;, predictions)from sklearn.metrics import accuracy_score# 计算模型的准确率accuracy = accuracy_score(y_test, predictions)print(&quot;模型准确率:&quot;, accuracy)from sklearn.preprocessing import StandardScaler# 特征标准化scaler = StandardScaler()X 1.5 TensorFlow&#x2F;PyTorchTensorFlow 和 PyTorch 是两个非常流行的深度学习框架，分别由 Google 和 Facebook 开发。它们都提供了强大的工具和函数，用于构建和训练神经网络模型。 1.6 Jupyter NotebookJupyter Notebook 是一个交互式的开发环境，特别适合数据分析和机器学习任务。它允许用户将代码、文本、图表和公式组合在一个文档中，非常适合演示和分享工作成果。我在实习的时候常用jupyterlab，比pycharm好用在 2. 常用库列表以下是常用库的列表，供大家参考： 库名称 功能描述 官方文档链接 NumPy 数学计算，支持多维数组 NumPy 官方文档 Pandas 数据分析和操作 Pandas 官方文档 Matplotlib 数据可视化 Matplotlib 官方文档 Scikit-learn 机器学习算法 Scikit-learn 官方文档 TensorFlow 深度学习框架 TensorFlow 官方文档 PyTorch 深度学习框架 PyTorch 官方文档 Jupyter Notebook 交互式计算环境 Jupyter 官方文档 3. 引入与安装方法3.1 使用 Conda 安装Anaconda 自带了 Conda 包管理工具，使用 Conda 可以方便地安装和管理环境中的库。 安装库的命令如下： conda install numpy pandas matplotlib scikit-learn tensorflow pytorch jupyter 清华镜像：https://pypi.tuna.tsinghua.edu.cn/simple 科大镜像：https://pypi.mirrors.ustc.edu.cn/simple 豆瓣镜像：http://pypi.douban.com/simple/ 阿里镜像：https://mirrors.aliyun.com/pypi/simple/ 百度镜像：https://mirror.baidu.com/pypi/simple #安装pytorch（自用 pip3 install torch==2.0.1 torchaudio torchvision==0.15.2 --index-url https://download.pytorch.org/whl/cu118","categories":[{"name":"环境","slug":"环境","permalink":"http://example.com/categories/%E7%8E%AF%E5%A2%83/"},{"name":"Python","slug":"环境/Python","permalink":"http://example.com/categories/%E7%8E%AF%E5%A2%83/Python/"}],"tags":[{"name":"Anaconda","slug":"Anaconda","permalink":"http://example.com/tags/Anaconda/"},{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"常用库","slug":"常用库","permalink":"http://example.com/tags/%E5%B8%B8%E7%94%A8%E5%BA%93/"}]},{"title":"Hexo 博客中如何插入多种内容","slug":"leecode150-萱仔算法复习记录1-数组","date":"2024-09-02T06:00:00.000Z","updated":"2024-09-02T17:25:03.313Z","comments":true,"path":"2024/09/02/leecode150-萱仔算法复习记录1-数组/","permalink":"http://example.com/2024/09/02/leecode150-%E8%90%B1%E4%BB%94%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%E8%AE%B0%E5%BD%951-%E6%95%B0%E7%BB%84/","excerpt":"前言在这篇文章中，我将演示如何在 Hexo 博客中插入各种内容，包括图片、表格、代码块，以及使用不同级别的标题来组织文章结构。","text":"前言在这篇文章中，我将演示如何在 Hexo 博客中插入各种内容，包括图片、表格、代码块，以及使用不同级别的标题来组织文章结构。 插入图片在 Hexo 博客中插入图片非常简单。你可以使用本地图片或网络图片。下面是一个本地图片的示例： 插入表格Markdown 还支持创建简单的表格。下面是一个插入表格的示例，表格可以用来展示数据或进行比较。 功能 描述 支持情况 插入图片 可以插入本地或网络图片 支持 插入表格 可以用 Markdown 创建简单表格 支持 插入代码块 支持多种编程语言的语法高亮 支持 插入代码块Hexo 支持使用 Markdown 插入代码块，并且可以对多种编程语言进行语法高亮。下面是一个插入 Python 代码块的示例： 示例代码块（Python）def hello_world(): print(&quot;Hello, World!&quot;) hello_world()","categories":[{"name":"技术","slug":"技术","permalink":"http://example.com/categories/%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://example.com/tags/Hexo/"},{"name":"教程","slug":"教程","permalink":"http://example.com/tags/%E6%95%99%E7%A8%8B/"}]},{"title":"推荐算法学习记录1——常用基础概念和常用推荐算法","slug":"推荐算法学习记录1——常用基础概念和常用推荐算法","date":"2024-09-02T06:00:00.000Z","updated":"2024-09-08T15:06:58.887Z","comments":true,"path":"2024/09/02/推荐算法学习记录1——常用基础概念和常用推荐算法/","permalink":"http://example.com/2024/09/02/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%951%E2%80%94%E2%80%94%E5%B8%B8%E7%94%A8%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E5%92%8C%E5%B8%B8%E7%94%A8%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/","excerpt":"","text":"最近买了一本异步图书的推荐算法书，对推荐算法产生了兴趣，记录一下自己学习的过程。本人学习的第一步是，了解常用的推荐算法。 推荐系统的常用基础概念详解：1. 数据收集与处理数据类型 用户行为数据：记录用户的操作和行为，如点击、浏览、购买、评分等。这些数据用于理解用户的兴趣和偏好。 用户特征数据：包括用户的个人信息，如年龄、性别、地理位置、职业等。这些信息可以帮助理解用户的背景和兴趣。 物品特征数据：描述物品的属性，如电影的导演、演员、类型，商品的品牌、价格、类别等。这些特征用于对物品进行分类和推荐。 数据清洗 数据清洗是准备数据的重要步骤，包括： 去除重复数据：确保每条记录唯一。 处理缺失值：填补或删除缺失的数据。 数据标准化：将数据转换为统一的格式，如将日期格式统一，数值范围归一化等。 特征提取 特征提取从原始数据中提取有用的信息，用于建模和分析。常见的特征提取方法包括： TF-IDF：用于文本数据，将词汇转换为特征。 特征工程：从用户行为数据中提取用户的兴趣特征，如用户对某一类物品的偏好。 2. 用户建模与物品建模用户建模 用户建模是为了理解用户的兴趣和需求。常见的用户建模方法包括： 用户画像：构建用户的详细画像，包括用户的基本信息、历史行为、兴趣爱好等。 兴趣建模：基于用户的历史行为预测用户对未来物品的兴趣。 物品建模 物品建模是为了理解物品的特征和类别。常见的物品建模方法包括： 物品特征建模：利用物品的属性和特征来描述和分类物品。 物品嵌入：将物品映射到低维空间中，用于捕捉物品之间的潜在关系。 3. 推荐系统架构数据存储与管理 推荐系统需要处理大量的用户和物品数据，常用的数据存储技术包括： 关系型数据库：如MySQL、PostgreSQL，用于存储结构化数据。 NoSQL数据库：如MongoDB、Cassandra，用于处理非结构化数据和大规模数据存储。 分布式文件系统：如Hadoop HDFS，用于处理大规模数据集。 实时推荐与离线推荐 实时推荐：在用户进行操作时，系统即时生成推荐结果，通常需要高效的数据处理和计算能力。 离线推荐：在非实时的情况下对用户和物品进行分析和推荐，适合用于大规模的数据分析和模型训练。 系统架构 推荐系统的架构包括前端用户接口、后端推荐引擎和数据存储。系统架构设计需要考虑高并发、数据一致性、系统扩展性等因素。 4. 推荐系统的性能评估评估指标 推荐系统的性能评估包括多个方面，常用的评估指标有： 准确率（Precision）：推荐结果中实际相关物品的比例。 召回率（Recall）：推荐系统找到的相关物品占所有相关物品的比例。 F1值：准确率和召回率的调和平均值。 均方根误差（RMSE）：预测评分与实际评分的差异的平方根，用于评价预测的准确性。 覆盖率（Coverage）：推荐系统能够覆盖的物品比例。 新颖性（Novelty）：推荐结果的新颖程度，即推荐物品的独特性和新颖性。 A&#x2F;B测试 A&#x2F;B测试是一种常用的实验方法，用于评估不同推荐算法或策略的效果。通过将用户分为两个组，分别使用不同的推荐策略，比较它们的效果来选择最佳方案。 5. 用户体验与伦理问题用户体验 用户体验是推荐系统设计的重要方面，包括： 推荐的相关性：推荐结果是否符合用户的兴趣和需求。 推荐的多样性：推荐结果是否具有足够的多样性，避免推荐结果过于单一。 推荐的透明性：用户是否能够理解推荐的理由和过程。 伦理问题 推荐系统在处理用户数据时需要考虑伦理问题，包括： 隐私保护：保护用户的个人隐私，确保数据的安全性和机密性。 公平性：避免算法偏见，确保推荐结果对所有用户公平。 透明性：让用户了解数据的使用和推荐算法的工作原理。 6. 未来发展趋势 推荐系统技术不断进步，未来的发展趋势包括： 深度学习和人工智能：利用更先进的深度学习技术，提高推荐系统的准确性和个性化水平。 跨域推荐：在不同领域之间进行推荐，如从电商推荐到音乐推荐。 个性化推荐：根据用户的实时行为和上下文进行更个性化的推荐。 总结 推荐系统不仅涉及复杂的算法和模型，还包括数据处理、系统架构、性能评估、用户体验和伦理问题等多个方面。了解这些基础概念有助于全面理解推荐系统的设计和实现，提升系统的效果和用户满意度。 ----------------------------------------------------------------------------------------------------------------------- 推荐系统可以帮助我们发现可能感兴趣的产品或信息方面。如果我们在网购的时候，推荐系统会根据浏览和购买记录，向我们推荐可能会喜欢的其他商品。以下是一些常用的推荐算法，以及它们的工作原理和优缺点： 常用的推荐算法详解：1. 协同过滤（Collaborative Filtering）用户基于协同过滤（User-Based Collaborative Filtering） 用户基于协同过滤算法会基于用户之间的相似性为用户推荐物品。具体来说，这种算法会找出与目标用户兴趣相似的其他用户，然后推荐那些用户喜欢的物品。例如，如果用户A和用户B有相似的购物记录，且用户B买了一本书但用户A还没有买，这个算法可能会推荐这本书给用户A。 优点： 简单易实现。 能够推荐用户未曾接触过的物品。 缺点： 对新用户或新物品不友好（冷启动问题）。 当数据稀疏时，推荐效果可能不佳。 步骤如下： 计算用户之间的相似度（常用余弦相似度）。 找到与目标用户最相似的用户。 根据相似用户的偏好推荐物品。 代码demo1234567891011121314151617181920212223242526import numpy as npimport pandas as pdfrom sklearn.metrics.pairwise import cosine_similarity# 用户-物品评分矩阵data = &#123;&#x27;user&#x27;: [1, 1, 1, 2, 2, 3, 3, 4, 4, 4], &#x27;item&#x27;: [&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;A&#x27;, &#x27;C&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;], &#x27;rating&#x27;: [5, 4, 3, 4, 2, 5, 3, 2, 4, 5]&#125;df = pd.DataFrame(data)# 创建用户-物品矩阵user_item_matrix = df.pivot(index=&#x27;user&#x27;, columns=&#x27;item&#x27;, values=&#x27;rating&#x27;).fillna(0)# 计算用户相似度user_similarity = cosine_similarity(user_item_matrix)user_similarity_df = pd.DataFrame(user_similarity, index=user_item_matrix.index, columns=user_item_matrix.index)# 为用户1推荐物品user_id = 1similar_users = user_similarity_df[user_id].sort_values(ascending=False).index[1:]# 获取相似用户评分recommendations = user_item_matrix.loc[similar_users].mean().sort_values(ascending=False)print(recommendations) 物品基于协同过滤（Item-Based Collaborative Filtering） 物品基于协同过滤算法会基于物品之间的相似性为用户推荐物品。具体来说，这种算法会分析用户之前喜欢的物品，然后推荐与这些物品相似的新物品。例如，如果你买了几本关于机器学习的书，系统可能会推荐更多关于机器学习的书给你。 优点： 稳定性较高，推荐结果不会因为单个用户的行为变化而有太大波动。 能处理大量用户的情况下仍然有效。 缺点： 需要计算大量物品之间的相似度，计算量大。 对于冷启动问题仍然存在一定影响。 步骤如下： 计算物品之间的相似度（常用余弦相似度）。 找到与用户喜欢的物品相似的物品。 推荐这些相似物品。 代码demo： 1234567891011121314151617181920212223242526import numpy as npimport pandas as pdfrom sklearn.metrics.pairwise import cosine_similarity# 用户-物品评分矩阵data = &#123;&#x27;user&#x27;: [1, 1, 1, 2, 2, 3, 3, 4, 4, 4], &#x27;item&#x27;: [&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;A&#x27;, &#x27;C&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;], &#x27;rating&#x27;: [5, 4, 3, 4, 2, 5, 3, 2, 4, 5]&#125;df = pd.DataFrame(data)# 创建用户-物品矩阵user_item_matrix = df.pivot(index=&#x27;user&#x27;, columns=&#x27;item&#x27;, values=&#x27;rating&#x27;).fillna(0)# 计算物品相似度item_similarity = cosine_similarity(user_item_matrix.T)item_similarity_df = pd.DataFrame(item_similarity, index=user_item_matrix.columns, columns=user_item_matrix.columns)# 为用户1推荐物品user_id = 1user_ratings = user_item_matrix.loc[user_id]# 计算推荐得分recommendations = user_ratings.dot(item_similarity_df).sort_values(ascending=False)print(recommendations) 2. 基于内容的推荐（Content-Based Filtering） 基于内容的推荐算法会根据用户之前喜欢的内容的特征和属性，推荐具有相似特征的新内容。这种方法依赖于物品的元数据，如电影的导演、演员列表或文章的关键词。例如，如果你喜欢某一类型的电影，系统会推荐更多同类型的电影。 优点： 能够处理冷启动问题，特别是新物品。 推荐结果可以解释，因为可以展示推荐理由（例如“你喜欢A，所以我们推荐B，因为它们有相似的特征”）。 缺点： 推荐的多样性可能不足，因为只推荐与用户已有兴趣相似的物品。 需要大量的物品特征信息。 步骤如下： 将用户-物品交互矩阵分解为用户矩阵和物品矩阵。 利用分解后的矩阵预测用户对未评分物品的偏好。 推荐得分最高的物品。 代码demo： 1234567891011121314151617181920212223from sklearn.feature_extraction.text import TfidfVectorizer# 示例数据：电影和它们的特征movies = &#123;&#x27;title&#x27;: [&#x27;Movie1&#x27;, &#x27;Movie2&#x27;, &#x27;Movie3&#x27;], &#x27;features&#x27;: [&#x27;action adventure&#x27;, &#x27;romance drama&#x27;, &#x27;action thriller&#x27;]&#125;movies_df = pd.DataFrame(movies)# 用户的历史偏好user_history = &#x27;action adventure&#x27;# 计算TF-IDF矩阵vectorizer = TfidfVectorizer()tfidf_matrix = vectorizer.fit_transform(movies_df[&#x27;features&#x27;])# 计算用户历史偏好和电影特征的相似度user_tfidf = vectorizer.transform([user_history])cosine_similarities = cosine_similarity(user_tfidf, tfidf_matrix).flatten()# 排序并推荐recommendations = movies_df.iloc[np.argsort(cosine_similarities)[::-1]]print(recommendations) 3. 矩阵分解（Matrix Factorization） 矩阵分解算法如奇异值分解（SVD）和交替最小二乘法（ALS）通过分解用户-物品交互矩阵，找到潜在的因子来预测用户对未评分物品的偏好。这个方法能够揭示用户和物品之间的隐含关系，从而提高推荐的准确性。 优点： 能够处理大规模数据。 可以发现用户和物品之间的隐含关系，推荐效果好。 缺点： 对于冷启动问题仍然存在一定影响。 需要一定的计算资源和时间来训练模型。 步骤如下： 将用户-物品交互矩阵分解为用户矩阵和物品矩阵。 利用分解后的矩阵预测用户对未评分物品的偏好。 推荐得分最高的物品。 代码demo： 123456789101112131415161718192021222324from surprise import SVD, Dataset, Readerfrom surprise.model_selection import train_test_splitfrom surprise import accuracy# 示例数据data = Dataset.load_from_df(df[[&#x27;user&#x27;, &#x27;item&#x27;, &#x27;rating&#x27;]], Reader(rating_scale=(1, 5)))# 训练SVD模型trainset, testset = train_test_split(data, test_size=0.25)algo = SVD()algo.fit(trainset)# 预测并计算准确度predictions = algo.test(testset)print(&#x27;RMSE:&#x27;, accuracy.rmse(predictions))# 为用户1推荐物品user_id = 1items = df[&#x27;item&#x27;].unique()user_ratings = &#123;item: algo.predict(user_id, item).est for item in items&#125;recommendations = pd.Series(user_ratings).sort_values(ascending=False)print(recommendations) 4. 深度学习方法 深度学习方法利用神经网络进行特征学习和推荐，如使用自编码器、卷积神经网络（CNNs）、循环神经网络（RNNs）和最近的注意力机制和Transformer模型。深度学习方法可以从复杂的数据中学习到深层的特征表示，提高推荐的准确性和个性化水平。 优点： 能够处理复杂和大规模的数据。 推荐效果好，能够学习到深层次的特征。 缺点： 需要大量的计算资源和数据。 训练和调参较为复杂。、 步骤如下： 构建神经网络模型。 利用历史数据训练模型。 使用训练好的模型进行推荐。 代码demo： 1234567891011121314151617181920212223242526272829303132333435import tensorflow as tffrom tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenatefrom tensorflow.keras.models import Model# 构建示例数据num_users = 10num_items = 20ratings = np.random.randint(1, 6, size=(100, 3))# 构建模型user_input = Input(shape=(1,))item_input = Input(shape=(1,))user_embedding = Embedding(input_dim=num_users, output_dim=10)(user_input)item_embedding = Embedding(input_dim=num_items, output_dim=10)(item_input)user_vector = Flatten()(user_embedding)item_vector = Flatten()(item_embedding)concatenated = Concatenate()([user_vector, item_vector])dense = Dense(128, activation=&#x27;relu&#x27;)(concatenated)output = Dense(1)(dense)model = Model([user_input, item_input], output)model.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;mean_squared_error&#x27;)# 训练模型model.fit([ratings[:, 0], ratings[:, 1]], ratings[:, 2], epochs=10, batch_size=32)# 为用户1推荐物品user_id = 1user_vector = np.array([user_id] * num_items)item_vector = np.array(range(num_items))predictions = model.predict([user_vector, item_vector])recommendations = pd.Series(predictions.flatten(), index=item_vector).sort_values(ascending=False)print(recommendations) 5. 混合推荐系统（Hybrid Recommender Systems） 混合推荐系统结合了以上一个或多个推荐技术的方法，比如将内容推荐和协同过滤结合起来，以利用各自的优势并克服单一方法的限制。混合方法可以提高推荐系统的准确性和覆盖面。 优点： 综合了多种方法的优点，推荐效果更好。 能够处理单一方法无法解决的问题，如冷启动和数据稀疏问题。 缺点： 实现复杂度较高。 需要综合考虑多种方法的优缺点，调参难度较大。 这些算法可以根据具体的应用场景和需求单独使用，也可以结合使用来构建更复杂的推荐系统。选择合适的推荐算法取决于可用的数据类型、系统的目标以及用户的期望等因素。 步骤如下： 选择合适的单一推荐算法并训练模型。 将多个模型的推荐结果结合起来。 利用综合推荐结果进行推荐。 代码demo： 123456789# 结合协同过滤和基于内容的推荐user_based_recommendations = user_item_matrix.loc[similar_users].mean()content_based_recommendations = pd.Series(cosine_similarities, index=movies_df[&#x27;title&#x27;])# 混合推荐（简单加权平均）combined_recommendations = (user_based_recommendations + content_based_recommendations) / 2print(combined_recommendations.sort_values(ascending=False)) 总结 推荐系统在现代互联网应用中发挥着至关重要的作用，从在线购物到音乐推荐，再到新闻推送，几乎无处不在。通过了解不同推荐算法的工作原理和特点，我们可以更好地设计和优化推荐系统，为用户提供更精准和个性化的推荐服务。","categories":[{"name":"推荐算法学习记录","slug":"推荐算法学习记录","permalink":"http://example.com/categories/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"推荐算法, 理论, python","slug":"推荐算法-理论-python","permalink":"http://example.com/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95-%E7%90%86%E8%AE%BA-python/"}]},{"title":"推荐算法学习记录2.1——kaggle数据集的动漫电影数据集推荐算法实践——数据集介绍","slug":"推荐算法学习记录2.1——kaggle数据集的动漫电影数据集推荐算法实践——数据集介绍","date":"2024-09-02T06:00:00.000Z","updated":"2024-09-08T15:11:42.423Z","comments":true,"path":"2024/09/02/推荐算法学习记录2.1——kaggle数据集的动漫电影数据集推荐算法实践——数据集介绍/","permalink":"http://example.com/2024/09/02/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%952.1%E2%80%94%E2%80%94kaggle%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%8A%A8%E6%BC%AB%E7%94%B5%E5%BD%B1%E6%95%B0%E6%8D%AE%E9%9B%86%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"最近已经学习的差不多了那个推荐算法，开启我的实践部分，本次实践采取了一个kaggle数据集的动漫电影数据集，写一个推荐系统的算法，数据集如下所示： 数据集介绍，单独版本1、anime_with_synopsis.csv123456import pandas as pdanime_with_synopsis = pd. read_csv(&#x27;./data/archive_2/anime_with_synopsis.csv&#x27;)anime_with_synopsis.head() 对这个关键的数据集进行最基本的观察和分析： 123456789101112131415161718import pandas as pd# 加载数据anime_with_synopsis = pd.read_csv(&#x27;./data/archive_2/anime_with_synopsis.csv&#x27;)# 打印前几行数据print(anime_with_synopsis.head())# 去除重复项anime_with_synopsis = anime_with_synopsis.drop_duplicates()# 填补缺失值anime_with_synopsis[&#x27;Synopsis&#x27;] = anime_with_synopsis[&#x27;Synopsis&#x27;].fillna(&#x27;No synopsis available&#x27;)# 统计每种类型的动漫数量anime_with_synopsis[&#x27;Genres&#x27;] = anime_with_synopsis[&#x27;Genres&#x27;].str.split(&#x27;, &#x27;)genres_exploded = anime_with_synopsis.explode(&#x27;Genres&#x27;)genre_counts = genres_exploded[&#x27;Genres&#x27;].value_counts()print(genre_counts)# 按评分降序排列sorted_anime = anime_with_synopsis.sort_values(by=&#x27;Score&#x27;, ascending=False)print(sorted_anime.head()) synopsis: 动漫的简要描述或剧情简介。 Genres: 动漫的类型（例如：动作、冒险、喜剧等）。 Score: 动漫的评分（通常是从 1 到 10）。 Name: 动漫的名称。 MAL_ID: 动漫的唯一标识符。 2、anime.csv123anime = pd. read_csv(&#x27;./data/archive_2/anime.csv&#x27;)anime.head() MAL_ID: 动漫的唯一标识符（与 anime_with_synopsis.csv 中的 MAL_ID 对应）。 Name: 动漫的名称。 Score: 动漫的评分。 Genres: 动漫的类型。 English name: 动漫的英文名称（如果有）。 Japanese name: 动漫的日文名称。 Type: 动漫的类型（例如：TV、电影、OVA）。 Episodes: 动漫的集数。 Aired: 播出时间段。 Premiered: 首播季节。 其他字段：有关评分的详细数据（例如，评分的分布）。 3、 animelist.csv123animelist = pd. read_csv(&#x27;./data/archive_2/animelist.csv&#x27;)animelist.head() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import pandas as pdimport matplotlib.pyplot as plt# 加载数据animelist = pd.read_csv(&#x27;./data/archive_2/animelist.csv&#x27;)# 打印前几行数据print(animelist.head())# 显示 DataFrame 的基本信息print(animelist.info())# 显示数据的描述统计信息print(animelist.describe())# 检查缺失值print(animelist.isna().sum())# 填补缺失值animelist = animelist.fillna(0)# 筛选评分高于 8 的记录high_ratings = animelist[animelist[&#x27;rating&#x27;] &gt; 8]print(high_ratings)# 筛选已观看的动漫watched_anime = animelist[animelist[&#x27;watching_status&#x27;] == 2] # 假设 2 表示已观看print(watched_anime)# 筛选已观看集数超过 10 集的记录more_than_10_episodes = animelist[animelist[&#x27;watched_episodes&#x27;] &gt; 10]print(more_than_10_episodes)# 计算每个用户的平均评分user_avg_rating = animelist.groupby(&#x27;user_id&#x27;)[&#x27;rating&#x27;].mean()print(user_avg_rating)# 计算每个用户观看的总集数user_total_watched_episodes = animelist.groupby(&#x27;user_id&#x27;)[&#x27;watched_episodes&#x27;].sum()print(user_total_watched_episodes)# 绘制评分分布图plt.figure(figsize=(10, 6))plt.hist(animelist[&#x27;rating&#x27;].dropna(), bins=10, edgecolor=&#x27;k&#x27;, alpha=0.7)plt.title(&#x27;Rating Distribution&#x27;)plt.xlabel(&#x27;Rating&#x27;)plt.ylabel(&#x27;Frequency&#x27;)plt.show() user_id: 用户的唯一标识符。 anime_id: 动漫的唯一标识符（与 anime.csv 中的 MAL_ID 对应）。 rating: 用户对动漫的评分。 watching_status: 用户对动漫的观看状态。 watched_episodes: 用户观看的集数。 4、rating_complete.csv 字段: user_id: 用户的唯一标识符。 anime_id: 动漫的唯一标识符。 rating: 用户对动漫的评分。 5、watching_status.csv 字段: status: 状态的唯一标识符（与 animelist.csv 中的 watching_status 对应）。 description: 状态的描述（例如：正在观看、已完成、计划观看等）。 数据集之间的关系 anime_with_synopsis.csv 和 anime.csv 关系: 这两个数据集都包含动漫的基本信息，例如 MAL_ID、Name、Score 和 Genres。 区别: anime_with_synopsis.csv 还包括每部动漫的简要描述（synopsis）。 anime.csv 包含更详细的播出信息和评分分布数据。 animelist.csv 和 rating_complete.csv 关系: 这两个数据集都包含用户对动漫的评分信息。 区别: animelist.csv 包含了用户的观看状态和观看的集数。 rating_complete.csv 只包含用户对动漫的评分，没有观看状态和观看集数信息。 watching_status.csv 和 status_description.csv 关系: 这两个数据集都涉及用户的观看状态。 区别: watching_status.csv 包含了用户对动漫的观看状态和状态的标识符。 status_description.csv 提供了这些状态标识符的描述。 animelist.csv 和 watching_status.csv 关系: animelist.csv 中的 watching_status 列与 watching_status.csv 中的 status 列相对应，表示用户的观看状态。 --------------------------------------------------------------------------------------------------- 官网的网址： https://www.kaggle.com/datasets/hernan4444/anime-recommendation-database-2020&#x2F;data kaggle官网介绍： MyAnimeList 数据库 2020 推荐数据来自 MyAnimeList 网站的 320,000 用户和 16,000 动漫。 这个数据集包含关于 17,562 部动漫和 325,772 不同用户的偏好信息。具体来说，这个数据集包含： 每个用户的动漫列表，包括已弃置、已完成、计划观看、正在观看和搁置的动漫。 用户对其完全观看过的动漫所给出的评分。 动漫的相关信息，如类型、统计数据、制作公司等。 包含动漫信息的 HTML 文件进行数据抓取。这些文件包含诸如评论、剧情简介、工作人员信息、动漫统计、类型等信息。 内容 数据抓取时间为 2 月 26 日至 3 月 20 日。 “html” 文件夹包含每个动漫的 1 个 zip 文件（共 17,562 个动漫）。每个 zip 文件包含从 MyAnimeList 抓取的不同 HTML 页面，包括： 主页 评论 推荐 统计数据 角色和工作人员 animelist.csv 包含用户登记的所有动漫列表及其相应评分、观看状态和观看的集数。这个数据集包含 1.09 亿行、17,562 部不同的动漫和 325,772 名不同的用户。文件包含以下列： user_id: 不可识别的随机生成用户 ID。 anime_id: MyAnimeList 动漫 ID。（例如：1） score: 用户给出的 1 到 10 分的评分。如果用户未评分，则为 0。（例如：10） watching_status: 用户动漫列表中的该动漫的状态 ID。（例如：2） watched_episodes: 用户观看的集数。（例如：24） watching_status.csv 描述 animelist.csv 中 “watching_status” 列的所有可能状态。 rating_complete.csv 是 animelist.csv 的子集。这个数据集只考虑用户完全观看（watching_status==2）并评分（score!=0）的动漫。这个数据集包含 5,700 万个评分，涉及 16,872 部动漫和 310,059 名用户。文件包含以下列： user_id: 不可识别的随机生成用户 ID。 anime_id: 用户评分的 MyAnimeList 动漫 ID。 rating: 用户给出的评分。 anime.csv 包含每部动漫（共 17,562 部）的常规信息，如类型、统计数据、制作公司等。文件包含以下列： MAL_ID: MyAnimeList 动漫 ID。（例如：1） Name: 动漫全名。（例如：Cowboy Bebop） Score: MyAnimeList 数据库中所有用户给出的平均评分。（例如：8.78） Genres: 动漫类型的逗号分隔列表。（例如：Action, Adventure, Comedy, Drama, Sci-Fi, Space） English name: 动漫的英文全名。（例如：Cowboy Bebop） Japanese name: 动漫的日文全名。（例如：カウボーイビバップ） Type: 动漫类型，如 TV、电影、OVA 等。（例如：TV） Episodes: 动漫的集数。（例如：26） Aired: 播出日期。（例如：Apr 3, 1998 to Apr 24, 1999） Premiered: 首播季节。（例如：Spring 1998） Producers: 制作公司的逗号分隔列表。（例如：Bandai Visual） Licensors: 发行商的逗号分隔列表。（例如：Funimation, Bandai Entertainment） Studios: 制作公司的逗号分隔列表。（例如：Sunrise） Source: 动漫的来源，如漫画、轻小说、书籍等。（例如：Original） Duration: 每集的时长。（例如：24 min. per ep.） Rating: 年龄分级。（例如：R - 17+ (violence &amp; profanity)） Ranked: 根据评分的排名。（例如：28） Popularity: 根据添加到用户列表的用户数量的排名。（例如：39） Members: 动漫的社区成员数量。（例如：1251960） Favorites: 将该动漫标为最爱的用户数量。（例如：61,971） Watching: 正在观看该动漫的用户数量。（例如：105808） Completed: 已完成观看该动漫的用户数量。（例如：718161） On-Hold: 将该动漫搁置的用户数量。（例如：71513） Dropped: 弃置该动漫的用户数量。（例如：26678） Plan to Watch: 计划观看该动漫的用户数量。（例如：329800） Score-10: 评分为 10 的用户数量。（例如：229170） Score-9: 评分为 9 的用户数量。（例如：182126） Score-8: 评分为 8 的用户数量。（例如：131625） Score-7: 评分为 7 的用户数量。（例如：62330） Score-6: 评分为 6 的用户数量。（例如：20688） Score-5: 评分为 5 的用户数量。（例如：8904） Score-4: 评分为 4 的用户数量。（例如：3184） Score-3: 评分为 3 的用户数量。（例如：1357） Score-2: 评分为 2 的用户数量。（例如：741） Score-1: 评分为 1 的用户数量。（例如：1580）","categories":[{"name":"推荐算法学习记录","slug":"推荐算法学习记录","permalink":"http://example.com/categories/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"推荐算法, 理论, python","slug":"推荐算法-理论-python","permalink":"http://example.com/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95-%E7%90%86%E8%AE%BA-python/"}]},{"title":"推荐算法学习记录2.2——kaggle数据集的动漫电影数据集推荐算法实践——基于内容的推荐算法、协同过滤推荐","slug":"推荐算法学习记录2.2——kaggle数据集的动漫电影数据集推荐算法实践——基于内容的推荐算法、协同过滤推荐","date":"2024-09-02T06:00:00.000Z","updated":"2024-09-08T15:11:31.061Z","comments":true,"path":"2024/09/02/推荐算法学习记录2.2——kaggle数据集的动漫电影数据集推荐算法实践——基于内容的推荐算法、协同过滤推荐/","permalink":"http://example.com/2024/09/02/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%952.2%E2%80%94%E2%80%94kaggle%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%8A%A8%E6%BC%AB%E7%94%B5%E5%BD%B1%E6%95%B0%E6%8D%AE%E9%9B%86%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AE%B9%E7%9A%84%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E3%80%81%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%8E%A8%E8%8D%90/","excerpt":"","text":"1、基于内容的推荐 ： 这种方法根据项的相关信息（如描述信息、标签等）和用户对项的操作行为（如评论、收藏、点赞等）来构建推荐算法模型。它可以直接利用物品的内容特征进行推荐，适用于内容较为丰富的场景。‌ 1234567891011121314151617181920212223242526272829303132#1. 基于内容的推荐算法from sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.metrics.pairwise import linear_kernelimport pandas as pdanime = pd.read_csv(&#x27;./data/archive_2/anime.csv&#x27;)# 使用TF-IDF向量化tfidf = TfidfVectorizer(stop_words=&#x27;english&#x27;)tfidf_matrix = tfidf.fit_transform(anime[&#x27;Genres&#x27;].fillna(&#x27;&#x27;))# 计算每个动漫之间的余弦相似度cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)# 创建一个动漫名字到索引的映射indices = pd.Series(anime.index, index=anime[&#x27;Name&#x27;]).drop_duplicates()# 推荐函数def content_based_recommendations(title, cosine_sim=cosine_sim): idx = indices[title] sim_scores = list(enumerate(cosine_sim[idx])) sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) sim_scores = sim_scores[1:11] anime_indices = [i[0] for i in sim_scores] return anime[&#x27;Name&#x27;].iloc[anime_indices]# 示例：推荐与&quot;Cowboy Bebop&quot;相似的动漫print(content_based_recommendations(&#x27;Cowboy Bebop&#x27;)) 结果如下： 2、协同过滤推荐 ： 这种方法通过分析用户的历史行为和偏好，找出具有相似兴趣的用户群体，然后推荐他们喜欢的物品或内容。协同过滤是一种常见的推荐系统技术，它基于用户的历史行为（例如评分、点击、购买等）来预测用户可能感兴趣的物品。协同过滤主要有两种类型：基于用户的协同过滤和基于物品的协同过滤。 基于用户的协同过滤推荐通过寻找具有相似兴趣的用户来推荐， 基于物品的协同过滤推荐通过寻找具有相似特征的物品来推荐。‌ 基于用户的协同过滤 计算用户相似度 ： 找出与目标用户相似的用户。 常用的相似度度量方法包括余弦相似度、皮尔逊相关系数等。 余弦相似度公式： 其中 u 和 v 是两个用户的评分向量。 推荐物品： 找出相似用户喜欢但目标用户未评分的物品。 根据相似用户的评分来预测目标用户对这些物品的评分。 推荐评分最高的物品给目标用户. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import pandas as pdfrom sklearn.metrics.pairwise import cosine_similarityfrom scipy.sparse import csr_matriximport numpy as npbatch_size = 10000 # 由于内存不够，一次处理10000条信息num_recommendations = 10 # 推荐10个电影def process_batch(batch_df, user_item_matrix): # 将当前批的数据合并到全局用户-项目矩阵 batch_pivot = batch_df.pivot(index=&#x27;user_id&#x27;, columns=&#x27;anime_id&#x27;, values=&#x27;rating&#x27;).fillna(0) # 合并到全局矩阵 user_item_matrix = user_item_matrix.add(batch_pivot, fill_value=0) return user_item_matrix# 计算用户之间的相似度def calculate_similarity(user_item_matrix): # 转换为稀疏矩阵 sparse_user_item_matrix = csr_matrix(user_item_matrix) # 计算相似度 user_similarity = cosine_similarity(sparse_user_item_matrix) user_similarity_df = pd.DataFrame(user_similarity, index=user_item_matrix.index, columns=user_item_matrix.index) return user_similarity_df# 生成推荐def user_based_recommendations(user_id, user_item_matrix, user_similarity_df, anime_df, num_recommendations=10): if user_id not in user_item_matrix.index: raise ValueError(f&quot;用户ID &#123;user_id&#125; 不在数据中&quot;) # 获取相似用户 similar_users = user_similarity_df[user_id].sort_values(ascending=False).index[1:] if len(similar_users) == 0: return [] # 获取相似用户的评分数据 similar_users_ratings = user_item_matrix.loc[similar_users] # 计算推荐得分 scores = similar_users_ratings.mean(axis=0).sort_values(ascending=False) # 获取当前用户已经评分的动漫 user_watched_animes = user_item_matrix.loc[user_id] user_watched_animes = user_watched_animes[user_watched_animes &gt; 0].index # 排除当前用户已经看过的动漫 recommendations = scores.drop(user_watched_animes, errors=&#x27;ignore&#x27;) # 获取前num_recommendations个推荐 top_recommendations = recommendations.head(num_recommendations).index # 确保推荐的动漫在anime数据集中 top_recommendations = top_recommendations[top_recommendations.isin(anime_df[&#x27;MAL_ID&#x27;])] # 获取推荐的动漫名称 recommended_animes = anime_df[anime_df[&#x27;MAL_ID&#x27;].isin(top_recommendations)][&#x27;Name&#x27;].values return recommended_animesdef main(): anime_df = pd.read_csv(&#x27;./data/archive_2/anime.csv&#x27;) user_item_matrix = pd.DataFrame() for chunk in pd.read_csv(&#x27;./data/archive_2/rating_complete.csv&#x27;, chunksize=batch_size): user_item_matrix = process_batch(chunk, user_item_matrix) # 计算用户之间的相似度 user_similarity_df = calculate_similarity(user_item_matrix) # 为用户0推荐动漫 recommended_animes = user_based_recommendations(0, user_item_matrix, user_similarity_df, anime_df, num_recommendations) print(&quot;推荐动漫：&quot;, recommended_animes)if __name__ == &quot;__main__&quot;: main() 基于物品的协同过滤 计算物品相似度 ： 找出与目标物品相似的物品。 常用的相似度度量方法同样包括余弦相似度、皮尔逊相关系数等。 余弦相似度公式： 其中 i 和 j 是两个物品的评分向量。 推荐物品 ： 根据用户对相似物品的评分来预测用户对目标物品的评分。 推荐评分最高的物品给用户。 协同过滤算法的优缺点优点 无需领域知识 ：协同过滤仅依赖于用户的历史行为数据，不需要对物品本身有深入的了解。 个性化推荐 ：能够根据用户的兴趣和行为进行个性化推荐，效果较好。 简单直观 ：算法相对简单，易于实现和解释。 缺点 数据稀疏性 ：在实际应用中，用户对大多数物品没有评分，这会导致评分矩阵非常稀疏，影响推荐效果。 冷启动问题 ：对于新用户或新物品，由于缺乏历史数据，协同过滤很难做出准确的推荐。 扩展性差 ：当用户和物品数量增加时，计算相似度的开销会显著增加，影响系统性能。 冷启动问题 冷启动问题是指在推荐系统中，无法为新用户或新物品提供高质量的推荐。这是协同过滤算法面临的主要挑战之一。解决冷启动问题的方法包括： 新用户冷启动 ： 问卷调查 ：在用户注册时，通过问卷调查获取用户的兴趣偏好。 使用社交网络数据 ：利用用户的社交网络数据，推测用户的兴趣。 混合推荐 ：结合内容过滤或基于知识的推荐方法，使用用户的属性和特征进行推荐。 新物品冷启动 ： 内容过滤 ：利用物品的内容信息（如标签、描述等）进行推荐。 使用上下文数据 ：利用物品的上下文数据，如发布日期、类型等，进行推荐。 混合推荐 ：结合协同过滤与内容过滤，通过物品的内容特征和已有的少量用户反馈进行推荐。","categories":[{"name":"推荐算法学习记录","slug":"推荐算法学习记录","permalink":"http://example.com/categories/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"推荐算法, python, kaggle","slug":"推荐算法-python-kaggle","permalink":"http://example.com/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95-python-kaggle/"}]},{"title":"Hello World","slug":"hello-world","date":"2024-08-31T17:09:31.473Z","updated":"2024-08-31T17:09:31.473Z","comments":true,"path":"2024/09/01/hello-world/","permalink":"http://example.com/2024/09/01/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"新闻文本分类项目","slug":"新闻文本分类项目","permalink":"http://example.com/categories/%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E9%A1%B9%E7%9B%AE/"},{"name":"萱仔求职系列","slug":"萱仔求职系列","permalink":"http://example.com/categories/%E8%90%B1%E4%BB%94%E6%B1%82%E8%81%8C%E7%B3%BB%E5%88%97/"},{"name":"大模型学习记录","slug":"大模型学习记录","permalink":"http://example.com/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"},{"name":"环境","slug":"环境","permalink":"http://example.com/categories/%E7%8E%AF%E5%A2%83/"},{"name":"Python","slug":"环境/Python","permalink":"http://example.com/categories/%E7%8E%AF%E5%A2%83/Python/"},{"name":"技术","slug":"技术","permalink":"http://example.com/categories/%E6%8A%80%E6%9C%AF/"},{"name":"推荐算法学习记录","slug":"推荐算法学习记录","permalink":"http://example.com/categories/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"}],"tags":[{"name":"技术, nlp, python, bert, transform","slug":"技术-nlp-python-bert-transform","permalink":"http://example.com/tags/%E6%8A%80%E6%9C%AF-nlp-python-bert-transform/"},{"name":"技术, 教程","slug":"技术-教程","permalink":"http://example.com/tags/%E6%8A%80%E6%9C%AF-%E6%95%99%E7%A8%8B/"},{"name":"技术, nlp, bert, 理论, 大模型","slug":"技术-nlp-bert-理论-大模型","permalink":"http://example.com/tags/%E6%8A%80%E6%9C%AF-nlp-bert-%E7%90%86%E8%AE%BA-%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"name":"Anaconda","slug":"Anaconda","permalink":"http://example.com/tags/Anaconda/"},{"name":"Python","slug":"Python","permalink":"http://example.com/tags/Python/"},{"name":"常用库","slug":"常用库","permalink":"http://example.com/tags/%E5%B8%B8%E7%94%A8%E5%BA%93/"},{"name":"Hexo","slug":"Hexo","permalink":"http://example.com/tags/Hexo/"},{"name":"教程","slug":"教程","permalink":"http://example.com/tags/%E6%95%99%E7%A8%8B/"},{"name":"推荐算法, 理论, python","slug":"推荐算法-理论-python","permalink":"http://example.com/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95-%E7%90%86%E8%AE%BA-python/"},{"name":"推荐算法, python, kaggle","slug":"推荐算法-python-kaggle","permalink":"http://example.com/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95-python-kaggle/"}]}