<!DOCTYPE html>
<html lang=zh>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>萱仔大模型学习记录4-BERT_lora实战 | 萱仔的学习小屋</title>
  <meta name="description" content="进行大模型的学习记录，包括了大模型原理，nlp常用的bert算法和transform学习，一些项目的代码和langchain等框架的入门学习过程">
<meta property="og:type" content="article">
<meta property="og:title" content="萱仔大模型学习记录4-BERT_lora实战">
<meta property="og:url" content="http://example.com/2024/09/04/%E8%90%B1%E4%BB%94%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%954-BERT_lora%E5%AE%9E%E6%88%98/index.html">
<meta property="og:site_name" content="萱仔的学习小屋">
<meta property="og:description" content="进行大模型的学习记录，包括了大模型原理，nlp常用的bert算法和transform学习，一些项目的代码和langchain等框架的入门学习过程">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i-blog.csdnimg.cn/direct/aaef44cfde0042d0be9f6556384d90a0.png">
<meta property="og:image" content="https://i-blog.csdnimg.cn/direct/7b20cd313bfe435eabe5aabf667427bc.png">
<meta property="article:published_time" content="2024-09-04T06:00:00.000Z">
<meta property="article:modified_time" content="2024-09-09T03:01:42.767Z">
<meta property="article:author" content="xuanzaismart">
<meta property="article:tag" content="技术, 教程">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i-blog.csdnimg.cn/direct/aaef44cfde0042d0be9f6556384d90a0.png">
  <!-- Canonical links -->
  <link rel="canonical" href="http://example.com/2024/09/04/%E8%90%B1%E4%BB%94%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%954-BERT_lora%E5%AE%9E%E6%88%98/index.html">
  
    <link rel="alternate" href="[object Object]" title="萱仔的学习小屋" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
  
  
<meta name="generator" content="Hexo 7.3.0"></head>


<body class="main-center theme-black" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="https://blog.csdn.net/qq_44117805?" target="_blank">
          <img class="img-circle img-rotate" src="/images/avatar.jpg" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">萱仔</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">萱仔的自我学习记录，冲冲冲！</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Hebei, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="搜索" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav menu-highlight">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">首页</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">归档</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">分类</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            
            <i class="icon icon-tags"></i>
            
            <span class="menu-title">标签</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-repository">
          <a href="/repository">
            
            <i class="icon icon-project"></i>
            
            <span class="menu-title">项目</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-books">
          <a href="/books">
            
            <i class="icon icon-book-fill"></i>
            
            <span class="menu-title">书单</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-links">
          <a href="/links">
            
            <i class="icon icon-friendship"></i>
            
            <span class="menu-title">友链</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-about">
          <a href="/about">
            
            <i class="icon icon-cup-fill"></i>
            
            <span class="menu-title">关于</span>
          </a>
        </li>
        
      </ul>
      
	
    <ul class="social-links">
    	
        <li><a href="https://blog.csdn.net/qq_44117805?" target="_blank" title="CSDN" data-toggle=tooltip data-placement=top><i class="icon icon-CSDN"></i></a></li>
        
        <li><a href="/atom.xml" target="_blank" title="Rss" data-toggle=tooltip data-placement=top><i class="icon icon-rss"></i></a></li>
        
    </ul>

    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">公告</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>不问收获，但问耕耘！冲冲冲，萱仔的自我学习记录博客，恭喜自己重新在github搭建个人博客，前面内容逐渐搬运到新博客之后和原CSDN博客同时更新，<br>CSDN网址：<a href="https://blog.csdn.net/qq_44117805?type=blog" target="_blank">https://blog.csdn.net/qq_44117805?type=blog</a></p>
            </div>
        </div>
    </div>
</div>

    
      
  <div class="widget">
    <h3 class="widget-title">分类</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/">大模型学习记录</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/">推荐算法学习记录</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E9%A1%B9%E7%9B%AE/">新闻文本分类项目</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%8E%AF%E5%A2%83/">环境</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%8E%AF%E5%A2%83/Python/">Python</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%90%B1%E4%BB%94%E6%B1%82%E8%81%8C%E7%B3%BB%E5%88%97/">萱仔求职系列</a><span class="category-list-count">4</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">标签</h3>
    <div class="widget-body">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Anaconda/" rel="tag">Anaconda</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/" rel="tag">Hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B8%B8%E7%94%A8%E5%BA%93/" rel="tag">常用库</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8A%80%E6%9C%AF-nlp-bert-%E7%90%86%E8%AE%BA-%E5%A4%A7%E6%A8%A1%E5%9E%8B/" rel="tag">技术, nlp, bert, 理论, 大模型</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8A%80%E6%9C%AF-nlp-python-bert-transform/" rel="tag">技术, nlp, python, bert, transform</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8A%80%E6%9C%AF-%E6%95%99%E7%A8%8B/" rel="tag">技术, 教程</a><span class="tag-list-count">10</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95-python-kaggle/" rel="tag">推荐算法, python, kaggle</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95-%E7%90%86%E8%AE%BA-python/" rel="tag">推荐算法, 理论, python</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%99%E7%A8%8B/" rel="tag">教程</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">标签云</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/Anaconda/" style="font-size: 13px;">Anaconda</a> <a href="/tags/Hexo/" style="font-size: 13px;">Hexo</a> <a href="/tags/Python/" style="font-size: 13px;">Python</a> <a href="/tags/%E5%B8%B8%E7%94%A8%E5%BA%93/" style="font-size: 13px;">常用库</a> <a href="/tags/%E6%8A%80%E6%9C%AF-nlp-bert-%E7%90%86%E8%AE%BA-%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="font-size: 13px;">技术, nlp, bert, 理论, 大模型</a> <a href="/tags/%E6%8A%80%E6%9C%AF-nlp-python-bert-transform/" style="font-size: 13px;">技术, nlp, python, bert, transform</a> <a href="/tags/%E6%8A%80%E6%9C%AF-%E6%95%99%E7%A8%8B/" style="font-size: 14px;">技术, 教程</a> <a href="/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95-python-kaggle/" style="font-size: 13px;">推荐算法, python, kaggle</a> <a href="/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95-%E7%90%86%E8%AE%BA-python/" style="font-size: 13.5px;">推荐算法, 理论, python</a> <a href="/tags/%E6%95%99%E7%A8%8B/" style="font-size: 13px;">教程</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">归档</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/09/">九月 2024</a><span class="archive-list-count">18</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled no-thumbnail">
        
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E9%A1%B9%E7%9B%AE/">新闻文本分类项目</a>
              </p>
              <p class="item-title">
                <a href="/2024/09/05/%E6%97%A7%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E4%B8%8A%E4%BC%A0%E8%AE%B0%E5%BD%95-%E5%A4%A9%E6%B1%A0-%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP_-_%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" class="title">旧代码学习上传记录-天池-零基础入门NLP_-_新闻文本分类</a>
              </p>
              <p class="item-date">
                <time datetime="2024-09-05T06:00:00.000Z" itemprop="datePublished">2024-09-05</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E8%90%B1%E4%BB%94%E6%B1%82%E8%81%8C%E7%B3%BB%E5%88%97/">萱仔求职系列</a>
              </p>
              <p class="item-title">
                <a href="/2024/09/05/%E8%90%B1%E4%BB%94%E6%B1%82%E8%81%8C%E7%B3%BB%E5%88%97%E2%80%94%E2%80%941.2_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%A4%8D%E4%B9%A0+%E9%83%A8%E5%88%86%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98/" class="title">萱仔求职系列——1.2_机器学习基础知识复习+部分代码实战</a>
              </p>
              <p class="item-date">
                <time datetime="2024-09-05T06:00:00.000Z" itemprop="datePublished">2024-09-05</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E8%90%B1%E4%BB%94%E6%B1%82%E8%81%8C%E7%B3%BB%E5%88%97/">萱仔求职系列</a>
              </p>
              <p class="item-title">
                <a href="/2024/09/05/%E8%90%B1%E4%BB%94%E6%B1%82%E8%81%8C%E7%B3%BB%E5%88%97%E2%80%94%E2%80%942.1_python%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%A4%8D%E4%B9%A0%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B+%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" class="title">萱仔求职系列——2.1_python基础知识复习——数据类型+数据结构</a>
              </p>
              <p class="item-date">
                <time datetime="2024-09-05T06:00:00.000Z" itemprop="datePublished">2024-09-05</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E8%90%B1%E4%BB%94%E6%B1%82%E8%81%8C%E7%B3%BB%E5%88%97/">萱仔求职系列</a>
              </p>
              <p class="item-title">
                <a href="/2024/09/05/%E8%90%B1%E4%BB%94%E6%B1%82%E8%81%8C%E7%B3%BB%E5%88%97%E2%80%94%E2%80%941.1_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%A4%8D%E4%B9%A0/" class="title">萱仔求职系列——1.1_机器学习基础知识复习</a>
              </p>
              <p class="item-date">
                <time datetime="2024-09-05T06:00:00.000Z" itemprop="datePublished">2024-09-05</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/%E8%90%B1%E4%BB%94%E6%B1%82%E8%81%8C%E7%B3%BB%E5%88%97/">萱仔求职系列</a>
              </p>
              <p class="item-title">
                <a href="/2024/09/05/%E8%90%B1%E4%BB%94%E6%B1%82%E8%81%8C%E7%B3%BB%E5%88%97%E2%80%94%E2%80%943.1_%E5%8A%9B%E6%89%A3%E9%9D%A2%E8%AF%95150%E9%A2%98%E7%9B%AE%E2%80%94%E2%80%94%E6%95%B0%E7%BB%84&%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%AC%AC%E4%B8%80%E5%BC%B9/" class="title">萱仔求职系列——3.1_力扣面试150题目——数组&amp;字符串第一弹</a>
              </p>
              <p class="item-date">
                <time datetime="2024-09-05T06:00:00.000Z" itemprop="datePublished">2024-09-05</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  
<main class="main" role="main">
  <div class="content">
  <article id="post-萱仔大模型学习记录4-BERT_lora实战" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      萱仔大模型学习记录4-BERT_lora实战
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2024/09/04/%E8%90%B1%E4%BB%94%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%954-BERT_lora%E5%AE%9E%E6%88%98/" class="article-date">
	  <time datetime="2024-09-04T06:00:00.000Z" itemprop="datePublished">2024-09-04</time>
	</a>
</span>
        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/">大模型学习记录</a>
  </span>

        
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link-link" href="/tags/%E6%8A%80%E6%9C%AF-%E6%95%99%E7%A8%8B/" rel="tag">技术, 教程</a>
  </span>


        

        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2024/09/04/%E8%90%B1%E4%BB%94%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%954-BERT_lora%E5%AE%9E%E6%88%98/#comments" class="article-comment-link">评论</a></span>
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <p> 这两天把 bert+lorade 代码已经改顺了，成功将这个lora应用到那个天池官方的新闻文本分类项目中，训练的结果较好，例如F1能达到0.90以上。（我第一个尝试做lora实践的原因还是我比较缺少资源，虽然有一些薅羊毛来的资源，但还是远远不够的，希望我入职之后能够快乐用卡嘿嘿嘿）</p>
<p> 话又说回来，Lora的优点在高效的模型微调和参数量减少上。由于在大模型中，微调所有参数的成本非常高，特别是在内存和计算资源方面。Lora通过添加低秩的矩阵，使得只需要微调少量参数，而不是整个模型。由于Lora只引入了一些低秩的矩阵进行训练，相较于微调整个模型，它大大减少了训练时间和计算资源。</p>
<p> Lora可以应用于各种不同类型的模型，包括Transformer模型、卷积神经网络等，具有很高的适应性。Lora在保留预训练模型的原始参数的同时，只对特定任务引入改动，有助于保留模型的泛化能力，从而避免过拟合。</p>
<p> Lora方法允许在微调新任务时保留预训练模型的原始参数不变，这意味着我们可以利用预训练模型的知识，而不需要从头开始训练。这样就可以用比较小的代价去处理我自己的任务，虽然bert本身不算一个特别巨大的模型，但是作为实践，还是可以尝试吧lora引入进去当作一个实践，从简单到困难需要循序渐进嘛。</p>
<p><img src="https://i-blog.csdnimg.cn/direct/aaef44cfde0042d0be9f6556384d90a0.png"></p>
<p> 由于我已经理顺了一次lora的原理，对于个人简单来说就是外接一个新的A和B矩阵，来训练新的小小矩阵去×原本的大大模型，这样只训练小矩阵的成本就能大大降低，而且由于训练关注的也是更加值得关注的部分，模型的精度常常不降反升，用在自己的任务上非常的方便。</p>
<p> 接下来是代码部分：</p>
<p> ---------------------------------------------------------------------------------------------------------------</p>
<p> 第一种代码（通用demo，方便后续自己修改的小小demo）：</p>
<ul>
<li>定义低秩矩阵 A 和 B。</li>
<li>替换原始的全连接层权重矩阵 W。</li>
<li>仅微调 A 和 B 矩阵。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">from transformers import BertModel, BertTokenizer</span><br><span class="line"></span><br><span class="line">class BertWithLoRA(nn.Module):</span><br><span class="line">    def __init__(self, num_classes, r=8):  # r为低秩矩阵的秩</span><br><span class="line">        super(BertWithLoRA, self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(&#x27;bert-base-uncased&#x27;)</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        hidden_size = self.bert.config.hidden_size</span><br><span class="line"></span><br><span class="line">        # 定义低秩矩阵</span><br><span class="line">        self.A = nn.Parameter(torch.randn(hidden_size, r))</span><br><span class="line">        self.B = nn.Parameter(torch.randn(r, hidden_size))</span><br><span class="line">        self.classifier = nn.Linear(hidden_size, num_classes)</span><br><span class="line"></span><br><span class="line">    def forward(self, input_ids, attention_mask, token_type_ids):</span><br><span class="line">        outputs = self.bert(input_ids, attention_mask, token_type_ids)</span><br><span class="line">        pooled_output = outputs[1]</span><br><span class="line"></span><br><span class="line">        # 应用低秩矩阵变换</span><br><span class="line">        low_rank_output = torch.matmul(pooled_output, self.A)</span><br><span class="line">        low_rank_output = torch.matmul(low_rank_output, self.B)</span><br><span class="line"></span><br><span class="line">        logits = self.classifier(low_rank_output)</span><br><span class="line">        return logits</span><br><span class="line"></span><br><span class="line"># 加载模型和分词器</span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(&#x27;bert-base-uncased&#x27;)</span><br><span class="line">model = BertWithLoRA(num_classes=2)  # 假设我需要完成的是二分类任务</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">inputs = tokenizer(&quot;Example sentence&quot;, return_tensors=&#x27;pt&#x27;)</span><br><span class="line">labels = torch.tensor([1])  # 示例标签</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">outputs = model(**inputs)</span><br><span class="line">loss = criterion(outputs, labels)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p> 第二个代码（我自己完成前面章节提到的新闻文本分类任务的代码，供大家参考和自己学习记录使用）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import torch</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from transformers import AdamW, BertTokenizer</span><br><span class="line">import pandas as pd</span><br><span class="line">import os</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.metrics import precision_recall_fscore_support, accuracy_score</span><br><span class="line">import openpyxl</span><br><span class="line">from openpyxl import Workbook</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def evaluate_model(model, dataloader, device):</span><br><span class="line">    model.eval()</span><br><span class="line">    preds = []</span><br><span class="line">    true_labels = []</span><br><span class="line"></span><br><span class="line">    for batch in dataloader:</span><br><span class="line">        batch = tuple(t.to(device) for t in batch)</span><br><span class="line">        token_ids, attn_masks, token_type_ids, labels = batch</span><br><span class="line"></span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            logits = model(token_ids, attn_masks, token_type_ids)</span><br><span class="line"></span><br><span class="line">        preds.extend(torch.argmax(logits, axis=1).cpu().numpy())</span><br><span class="line">        true_labels.extend(labels.cpu().numpy())</span><br><span class="line"></span><br><span class="line">    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, preds, average=&#x27;weighted&#x27;)</span><br><span class="line">    acc = accuracy_score(true_labels, preds)</span><br><span class="line">    #记录一下我的验证的结果</span><br><span class="line">    return &#123;</span><br><span class="line">        &#x27;accuracy&#x27;: acc,</span><br><span class="line">        &#x27;precision&#x27;: precision,</span><br><span class="line">        &#x27;recall&#x27;: recall,</span><br><span class="line">        &#x27;f1&#x27;: f1,</span><br><span class="line">        &#x27;preds&#x27;: preds  </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"># 保存模型和分词器</span><br><span class="line">def save_model_and_tokenizer(model, tokenizer, dir_path):</span><br><span class="line">    os.makedirs(dir_path, exist_ok=True)</span><br><span class="line">    torch.save(model.state_dict(), os.path.join(dir_path, &#x27;model.pth&#x27;))</span><br><span class="line">    tokenizer.save_pretrained(dir_path)</span><br><span class="line"></span><br><span class="line"># 保存指标和模型的函数</span><br><span class="line">def save_metrics(epoch, avg_train_loss, metrics, best_f1, model, val_dataloader, device, tokenizer):</span><br><span class="line">    # 每隔五轮就保存一次训练的模型</span><br><span class="line">    if epoch % 5 == 0:</span><br><span class="line">        save_model_and_tokenizer(model, tokenizer, f&#x27;./model_weights/epoch_&#123;epoch&#125;&#x27;)</span><br><span class="line"></span><br><span class="line">    if metrics[&#x27;f1&#x27;] &gt; best_f1:  #这里保存了我训练完的模型，最好的那个模型</span><br><span class="line">        best_f1 = metrics[&#x27;f1&#x27;]</span><br><span class="line">        save_model_and_tokenizer(model, tokenizer, &#x27;./model_weights/best_model&#x27;)</span><br><span class="line"></span><br><span class="line">    val_metrics = evaluate_model(model, val_dataloader, device)</span><br><span class="line">    predictions = val_metrics[&#x27;preds&#x27;]</span><br><span class="line">    val_results = pd.DataFrame(&#123;&#x27;text&#x27;: val_texts, &#x27;label&#x27;: val_labels, &#x27;prediction&#x27;: predictions&#125;)</span><br><span class="line">    val_results.to_csv(f&#x27;./val_results/epoch_&#123;epoch&#125;.csv&#x27;, index=False)</span><br><span class="line"></span><br><span class="line">    # 更新指标</span><br><span class="line">    print(f&quot;Epoch: &#123;epoch&#125;, Loss: &#123;avg_train_loss:.4f&#125;, Accuracy: &#123;metrics[&#x27;accuracy&#x27;]:.4f&#125;, Precision: &#123;metrics[&#x27;precision&#x27;]:.4f&#125;, Recall: &#123;metrics[&#x27;recall&#x27;]:.4f&#125;, F1: &#123;metrics[&#x27;f1&#x27;]:.4f&#125;&quot;)</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(&#x27;./data_news/train_set.csv&#x27;, sep=&#x27;\t&#x27;)</span><br><span class="line">train_texts, val_texts, train_labels, val_labels = train_test_split(df[&#x27;text&#x27;].tolist(), df[&#x27;label&#x27;].astype(int).tolist(), test_size=0.2)</span><br><span class="line"></span><br><span class="line">train_dataset = XuanDataset(train_texts, train_labels, max_length=128)</span><br><span class="line">val_dataset = XuanDataset(val_texts, val_labels, max_length=128)</span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)</span><br><span class="line">val_dataloader = DataLoader(val_dataset, batch_size=8)</span><br><span class="line"></span><br><span class="line">model = BertWithLoRA(num_classes=16)</span><br><span class="line">device = &#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(&#x27;bert-base-uncased&#x27;)</span><br><span class="line"></span><br><span class="line">os.makedirs(&#x27;./model_weights&#x27;, exist_ok=True)</span><br><span class="line">os.makedirs(&#x27;./val_results&#x27;, exist_ok=True)</span><br><span class="line"></span><br><span class="line">excel_file = &#x27;metrics1.xlsx&#x27;</span><br><span class="line">if not os.path.exists(excel_file):</span><br><span class="line">    wb = Workbook()</span><br><span class="line">    ws = wb.active</span><br><span class="line">    ws.title = &#x27;Metrics&#x27;</span><br><span class="line">    ws.append([&#x27;Epoch&#x27;, &#x27;Loss&#x27;, &#x27;Accuracy&#x27;, &#x27;Precision&#x27;, &#x27;Recall&#x27;, &#x27;F1&#x27;])</span><br><span class="line">    wb.save(excel_file)</span><br><span class="line"></span><br><span class="line">optimizer = AdamW(model.parameters(), lr=2e-5)</span><br><span class="line">epochs = 100  # 训练轮数</span><br><span class="line">best_f1 = 0.0</span><br><span class="line"></span><br><span class="line">for epoch in range(1, epochs + 1):</span><br><span class="line">    model.train()</span><br><span class="line">    total_loss = 0</span><br><span class="line"></span><br><span class="line">    for batch in train_dataloader:</span><br><span class="line">        batch = tuple(t.to(device) for t in batch)</span><br><span class="line">        token_ids, attn_masks, token_type_ids, labels = batch</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        logits = model(token_ids, attn_masks, token_type_ids)</span><br><span class="line">        loss = nn.CrossEntropyLoss()(logits, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    avg_train_loss = total_loss / len(train_dataloader)</span><br><span class="line">    </span><br><span class="line">    # 验证模型</span><br><span class="line">    metrics = evaluate_model(model, val_dataloader, device)</span><br><span class="line">    </span><br><span class="line">    wb = openpyxl.load_workbook(excel_file)</span><br><span class="line">    ws = wb[&#x27;Metrics&#x27;]</span><br><span class="line">    ws.append([epoch, avg_train_loss, metrics[&#x27;accuracy&#x27;], metrics[&#x27;precision&#x27;], metrics[&#x27;recall&#x27;], metrics[&#x27;f1&#x27;]])</span><br><span class="line">    wb.save(excel_file)</span><br><span class="line"></span><br><span class="line">    # 保存模型和打印评价指标</span><br><span class="line">    save_metrics(epoch, avg_train_loss, metrics, best_f1, model, val_dataloader, device, tokenizer)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p> 结果数据就如下啦：</p>
<p><img src="https://i-blog.csdnimg.cn/direct/7b20cd313bfe435eabe5aabf667427bc.png"></p>

      
    </div>
    <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="http://example.com/2024/09/04/%E8%90%B1%E4%BB%94%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%954-BERT_lora%E5%AE%9E%E6%88%98/" title="萱仔大模型学习记录4-BERT_lora实战" target="_blank" rel="external">http://example.com/2024/09/04/萱仔大模型学习记录4-BERT_lora实战/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="https://blog.csdn.net/qq_44117805?" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/avatar.jpg" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="https://blog.csdn.net/qq_44117805?" target="_blank"><span class="text-dark">萱仔</span><small class="ml-1x">萱仔的自我学习记录，冲冲冲！</small></a></h3>
        <div>个人简介</div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    
  <section id="comments">
  	
      <div id="vcomments"></div>
    
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    <li class="prev">
      <a href="/2024/09/04/%E6%97%A7%E9%A1%B9%E7%9B%AE%E6%96%B0%E5%AD%A6%E4%B9%A0-%E5%A4%A9%E6%B1%A0-%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8NLP_-_%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB_-_BERT%E7%AE%97%E6%B3%95%E5%A4%84%E7%90%86/" title="旧项目新学习-天池-零基础入门NLP_-_新闻文本分类_-_BERT算法处理"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a>
    </li>
    
    
    <li class="next">
      <a href="/2024/09/04/%E8%90%B1%E4%BB%94%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%955-langchain%E5%AE%9E%E6%88%98/" title="萱仔大模型学习记录5-langchain实战"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
  </ul>
  
  
  
  <div class="bar-right">
    
    <div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div>
    
  </div>
  </div>
</nav>
  


</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://blog.csdn.net/qq_44117805?" target="_blank" title="CSDN" data-toggle=tooltip data-placement=top><i class="icon icon-CSDN"></i></a></li>
        
        <li><a href="/atom.xml" target="_blank" title="Rss" data-toggle=tooltip data-placement=top><i class="icon icon-rss"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        <div class="publishby">
        	Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>






   




   
    
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/valine"></script>
  <script type="text/javascript">
  var GUEST = ['nick', 'mail', 'link'];
  var meta = 'nick,mail,link';
  meta = meta.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#vcomments',
    verify: false,
    notify: false,
    appId: '',
    appKey: '',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: meta,
    pageSize: '10' || 10,
    visitor: false
  });
  </script>

     







</body>
</html>